{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "######  NB! This,InteractiveLoginAuthentication, is only needed to run 1st time, then when ws_config is written, use later CELL in notebook, that just reads that file\r\n",
    "import repackage\r\n",
    "repackage.add(\"../azure-enterprise-scale-ml/esml/common/\")\r\n",
    "from azureml.core import Workspace\r\n",
    "from azureml.core.authentication import InteractiveLoginAuthentication\r\n",
    "from esml import ESMLDataset, ESMLProject\r\n",
    "\r\n",
    "p = ESMLProject()\r\n",
    "p.dev_test_prod=\"dev\"\r\n",
    "auth = InteractiveLoginAuthentication(tenant_id = p.tenant)\r\n",
    "ws, config_name = p.authenticate_workspace_and_write_config(auth)\r\n",
    "######  NB!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# ESML - accelerator: Batch scoring pipeline. 6 acceleration benefits \r\n",
    "- 1) `AutoMap datalake` & init ESML project\r\n",
    "- 2) `Get correct environment` - via ESML config, to the correct Workspace(dev,test,prod)\r\n",
    "- 3) `1 line: Get Compute cluster` from ESML.get_training_aml_compute \r\n",
    "- 4) `1 line: Get earlier trained model` and `Inference ENVIRONMENT` from AutoML via ESMLProject.get_active_model_inference_config()\r\n",
    "- 5) `DATASET via properties:  `p.DatasetByName(\"ds01_diabetes\").Bronze `and `p.GoldToScore`  `and `p.GoldScored`\r\n",
    "    - Or via conventions name from portal `M11_ds02_other_inference_BRONZE` and `M11_GOLD_TO_SCORE` and  `M11_GOLD_SCORED`\r\n",
    "- 6) `Score and Writeback`: Save to scored data to \"somewhere\". Example: riteBack functionality to source\r\n",
    "\r\n",
    "# USAGE:\r\n",
    "You can use ESMLPipeline factory like this notebook:\r\n",
    "`ESMLPipeline factory will build the pipeline automatically`, all steps based on the dataset array in the `model_settings.json` and witht the `ESML Datamodel: Bronze->Silver-Gold` \r\n",
    "\r\n",
    "## THIS CODE... ↓\r\n",
    "```\r\n",
    "p = ESMLProject()\r\n",
    "p_factory = ESMLPipelineFactory(p, \"Y\")\r\n",
    "\r\n",
    "p_factory.create_dataset_scripts_from_template() # Do this once, then edit them manually\r\n",
    "batch_pipeline = p_factory.create_batch_scoring_pipe()\r\n",
    "pipeline_run = p_factory.execute_pipeline(batch_pipeline)\r\n",
    "\r\n",
    "pipeline_run.wait_for_completion(show_output=False)\r\n",
    "```\r\n",
    "\r\n",
    "## ...WILL GIVE YOU THAT PIPELINE ↓ (Note: Datastore is Azure Datlake GEN2 : )\r\n",
    "You will get an image like this, if you have 2 datasets in lake_settings.json\r\n",
    "\r\n",
    "![](../azure-enterprise-scale-ml/esml/images/pipeline_IN2GOLD.png)\r\n",
    "\r\n",
    "- Above PIPELINE: \"dataset_folder_names\": [\"ds01_diabetes\", \"ds02_other\"]\r\n",
    "- You can edit each `step-file.py` which is generate by ESML"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1) `One cell of code: 3 lines` below, to create above, and execute it"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import repackage\r\n",
    "repackage.add(\"../azure-enterprise-scale-ml/esml/common/\")\r\n",
    "from esml import ESMLProject\r\n",
    "from baselayer_azure_ml_pipeline import ESMLPipelineFactory, esml_pipeline_types\r\n",
    "p = ESMLProject()\r\n",
    "\r\n",
    "p.active_model = 11 # Y=11, price=12\r\n",
    "p_factory = ESMLPipelineFactory(p)\r\n",
    "\r\n",
    "#scoring_date = '2021-01-01 10:35:01.243860'\r\n",
    "#p_factory.batch_pipeline_parameters[1].default_value = scoring_date # overrides ESMLProject.date_scoring_folder.\r\n",
    "\r\n",
    "p.inference_mode = True\r\n",
    "p_factory.describe()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Using lake_settings.json with ESML version 1.4 - Models array support including LABEL\n",
      "\n",
      " ---- Q: WHICH files are generated as templates, for you to EDIT? ---- \n",
      "A: These files & locations:\n",
      "File to EDIT (step: IN_2_SILVER_1): ../../../2_A_aml_pipeline/4_inference/batch/M11/in2silver_ds01_diabetes.py\n",
      "File to EDIT (step: IN_2_SILVER_2): ../../../2_A_aml_pipeline/4_inference/batch/M11/in2silver_ds02_other.py\n",
      "File to EDIT (step: SILVER_MERGED_2_GOLD): ../../../2_A_aml_pipeline/4_inference/batch/M11/silver_merged_2_gold.py\n",
      "File to EDIT (step: SCORING_GOLD): ../../../2_A_aml_pipeline/4_inference/batch/M11/scoring_gold.py\n",
      "File to EDIT a lot (reference in step-scripts Custom code): ../../../2_A_aml_pipeline/4_inference/batch/M11/your_code/your_custom_code.py\n",
      "\n",
      " ---- WHAT model to SCORE with, & WHAT data 'date_folder'? ---- \n",
      "InferenceModelVersion (model version to score with): 1\n",
      "Date_scoring_folder (data to score) : 2021-06-08 15:35:01.243860\n",
      "ESML environment: dev\n",
      "\n",
      " ---- ESML Datalake locations: ESML Datasets (IN-data) ---- \n",
      "Name (lake folder): ds01_diabetes and AzureName IN: M11_ds01_diabetes_inference_IN\n",
      "IN projects/project002/11_diabetes_model_reg/inference/1/ds01_diabetes/in/dev/2021/06/08/\n",
      "Bronze projects/project002/11_diabetes_model_reg/inference/1/ds01_diabetes/out/bronze/dev/\n",
      "Silver projects/project002/11_diabetes_model_reg/inference/1/ds01_diabetes/out/silver/dev/\n",
      "\n",
      "Name (lake folder): ds02_other and AzureName IN: M11_ds02_other_inference_IN\n",
      "IN projects/project002/11_diabetes_model_reg/inference/1/ds02_other/in/dev/2021/06/08/\n",
      "Bronze projects/project002/11_diabetes_model_reg/inference/1/ds02_other/out/bronze/dev/\n",
      "Silver projects/project002/11_diabetes_model_reg/inference/1/ds02_other/out/silver/dev/\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1a) - BUILD & `RUN` pipeline (3 lines)\r\n",
    "- `Iterate` until you are happy with the pipeline (edit step files etc)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "## BUILD IN_2_GOLD_SCORING\r\n",
    "p_factory.create_dataset_scripts_from_template(overwrite_if_exists=True) # Do this once, then edit them manually. overwrite_if_exists=False is DEFAULT\r\n",
    "batch_pipeline = p_factory.create_batch_pipeline(esml_pipeline_types.IN_2_GOLD_SCORING) # Creates pipeline from template"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Creates template step_files.py for user to edit at:\n",
      "Edit at ../../../2_A_aml_pipeline/4_inference/batch/M11/in2silver_ds01_diabetes.py\n",
      "Edit at ../../../2_A_aml_pipeline/4_inference/batch/M11/in2silver_ds02_other.py\n",
      "Edit at ../../../2_A_aml_pipeline/4_inference/batch/M11/silver_merged_2_gold.py\n",
      "Edit at ../../../2_A_aml_pipeline/4_inference/batch/M11/scoring_gold.py\n",
      "Edit at ../../../2_A_aml_pipeline/4_inference/batch/M11/your_code/your_custom_code.py\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Warning: Falling back to use azure cli login credentials.\n",
      "If you run your code in unattended mode, i.e., where you can't give a user input, then we recommend to use ServicePrincipalAuthentication or MsiAuthentication.\n",
      "Please refer to aka.ms/aml-notebook-auth for different authentication mechanisms in azureml-sdk.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Using GEN2 as Datastore\n",
      "found model via REMOTE FILTER: Experiment TAGS: model name and version\n",
      "Note: OVERRIDING enterprise performance settings with project specifics. (to change, set flag in 'dev_test_prod_settings.json' -> override_enterprise_settings_with_model_specific=False)\n",
      "Using a model specific cluster, per configuration in project specific settings, (the integer of 'model_number' is the base for the name)\n",
      "Note: OVERRIDING enterprise performance settings with project specifics. (to change, set flag in 'dev_test_prod_settings.json' -> override_enterprise_settings_with_model_specific=False)\n",
      "Found existing cluster prj02-m11-dev for project and environment, using it.\n",
      "Succeeded\n",
      "AmlCompute wait for completion finished\n",
      "\n",
      "Minimum number of nodes requested have been provisioned\n",
      "image_build_compute = prj02-m11-dev\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "## RUN\r\n",
    "pipeline_run = p_factory.execute_pipeline(batch_pipeline)\r\n",
    "pipeline_run.wait_for_completion(show_output=False)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Created step IN 2 SILVER - ds01_diabetes [dbed732f][f368c343-3f6b-4e6a-8d3f-027e127aab17], (This step will run and generate new outputs)\n",
      "Created step IN 2 SILVER - ds02_other [a3aea9a1][6585b8e7-42af-4f09-848c-7dbb82eef202], (This step will run and generate new outputs)\n",
      "Created step SILVER MERGED 2 GOLD [74eda51d][c0feb0a7-5842-4554-86b4-2ca403bd2f51], (This step will run and generate new outputs)\n",
      "Created step SCORING GOLD [a1eaf671][0caf5173-d9d8-441e-ba6a-6a228b080fe9], (This step will run and generate new outputs)\n",
      "Submitted PipelineRun 6ffc4b12-5266-4d1b-af93-056f652cc248\n",
      "Link to Azure Machine Learning Portal: https://ml.azure.com/runs/6ffc4b12-5266-4d1b-af93-056f652cc248?wsid=/subscriptions/ca0a8c40-b06a-4e4e-8434-63c03a1dee34/resourcegroups/MSFT-WEU-EAP_PROJECT02_AI-DEV-RG/workspaces/msft-weu-DEV-eap-proj02_ai-amls&tid=72f988bf-86f1-41af-91ab-2d7cd011db47\n",
      "Pipeline submitted for execution!\n",
      "PipelineRunId: 6ffc4b12-5266-4d1b-af93-056f652cc248\n",
      "Link to Azure Machine Learning Portal: https://ml.azure.com/runs/6ffc4b12-5266-4d1b-af93-056f652cc248?wsid=/subscriptions/ca0a8c40-b06a-4e4e-8434-63c03a1dee34/resourcegroups/MSFT-WEU-EAP_PROJECT02_AI-DEV-RG/workspaces/msft-weu-DEV-eap-proj02_ai-amls&tid=72f988bf-86f1-41af-91ab-2d7cd011db47\n",
      "{'runId': '6ffc4b12-5266-4d1b-af93-056f652cc248', 'status': 'Completed', 'startTimeUtc': '2021-10-04T11:58:35.203434Z', 'endTimeUtc': '2021-10-04T12:08:00.323497Z', 'properties': {'azureml.runsource': 'azureml.PipelineRun', 'runSource': 'SDK', 'runType': 'SDK', 'azureml.parameters': '{\"esml_inference_model_version\":\"1\",\"esml_scoring_folder_date\":\"2021-06-08 15:35:01.243860\",\"esml_optional_unique_scoring_folder\":\"*\",\"esml_environment_dev_test_prod\":\"dev\"}'}, 'inputDatasets': [], 'outputDatasets': [], 'logFiles': {'logs/azureml/executionlogs.txt': 'https://sajxvzyuylcu5jc.blob.core.windows.net/azureml/ExperimentRun/dcid.6ffc4b12-5266-4d1b-af93-056f652cc248/logs/azureml/executionlogs.txt?sv=2019-07-07&sr=b&sig=acp0cviT5GJKvTI%2FkUzf3FQVIldcXUdYxlMt5EStlFI%3D&skoid=bd6cbaae-1027-4ac2-b1e7-dea7bd468eb3&sktid=72f988bf-86f1-41af-91ab-2d7cd011db47&skt=2021-10-04T11%3A32%3A47Z&ske=2021-10-05T19%3A42%3A47Z&sks=b&skv=2019-07-07&st=2021-10-04T11%3A51%3A48Z&se=2021-10-04T20%3A01%3A48Z&sp=r', 'logs/azureml/stderrlogs.txt': 'https://sajxvzyuylcu5jc.blob.core.windows.net/azureml/ExperimentRun/dcid.6ffc4b12-5266-4d1b-af93-056f652cc248/logs/azureml/stderrlogs.txt?sv=2019-07-07&sr=b&sig=9xY6%2FQs3ZYtQItgT2yWTMdaTnBYtTPQHvujqwSn4YCw%3D&skoid=bd6cbaae-1027-4ac2-b1e7-dea7bd468eb3&sktid=72f988bf-86f1-41af-91ab-2d7cd011db47&skt=2021-10-04T11%3A32%3A47Z&ske=2021-10-05T19%3A42%3A47Z&sks=b&skv=2019-07-07&st=2021-10-04T11%3A51%3A48Z&se=2021-10-04T20%3A01%3A48Z&sp=r', 'logs/azureml/stdoutlogs.txt': 'https://sajxvzyuylcu5jc.blob.core.windows.net/azureml/ExperimentRun/dcid.6ffc4b12-5266-4d1b-af93-056f652cc248/logs/azureml/stdoutlogs.txt?sv=2019-07-07&sr=b&sig=GQDDv94Kq%2Bv%2F%2FeTCUKusaA3m2N45NF9cTeWi4vOting%3D&skoid=bd6cbaae-1027-4ac2-b1e7-dea7bd468eb3&sktid=72f988bf-86f1-41af-91ab-2d7cd011db47&skt=2021-10-04T11%3A32%3A47Z&ske=2021-10-05T19%3A42%3A47Z&sks=b&skv=2019-07-07&st=2021-10-04T11%3A51%3A48Z&se=2021-10-04T20%3A01%3A48Z&sp=r'}, 'submittedBy': 'Joakim Åström'}\n",
      "\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'Finished'"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1b) When satisfied - `PUBLISH` pipeline (or rebuild and publish)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# REBUILD - if you haven't runned the above cell, uncommen below:\r\n",
    "p_factory.create_dataset_scripts_from_template(overwrite_if_exists=False) # overwrite_if_exists=False is default\r\n",
    "batch_pipeline = p_factory.create_batch_pipeline(esml_pipeline_types.IN_2_GOLD_SCORING) # Gets workspace, connects to lake, creates pipeline.\r\n",
    "p.ws = p.get_workspace_from_config()\r\n",
    "\r\n",
    "# PUBLISH\r\n",
    "published_pipeline, endpoint = p_factory.publish_pipeline(batch_pipeline, \"_4\") # \"_4\" is optional    to create a NEW pipeline with 0 history, not ADD version to existing pipe & endpoint"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Did NOT overwrite script-files with template-files such as 'scoring_gold.py', since overwrite_if_exists=False\n",
      "Using GEN2 as Datastore\n",
      "Note: OVERRIDING enterprise performance settings with project specifics. (to change, set flag in 'dev_test_prod_settings.json' -> override_enterprise_settings_with_model_specific=False)\n",
      "Loading AutoML config settings from: dev\n",
      "Loading AutoML config settings from: dev\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "WARNING:root:The version of the SDK does not match the version the model was trained on.\n",
      "WARNING:root:The consistency in the result may not be guaranteed.\n",
      "WARNING:root:Package:azureml-automl-core, training version:1.30.0, current version:1.26.0\n",
      "Package:azureml-automl-runtime, training version:1.30.0, current version:1.26.0\n",
      "Package:azureml-core, training version:1.30.0, current version:1.26.0\n",
      "Package:azureml-dataprep, training version:2.15.1, current version:2.13.2\n",
      "Package:azureml-dataprep-native, training version:33.0.0, current version:32.0.0\n",
      "Package:azureml-dataprep-rslex, training version:1.13.0, current version:1.11.2\n",
      "Package:azureml-dataset-runtime, training version:1.30.0, current version:1.26.0\n",
      "Package:azureml-defaults, training version:1.30.0, current version:1.26.0\n",
      "Package:azureml-interpret, training version:1.30.0, current version:1.26.0\n",
      "Package:azureml-pipeline-core, training version:1.30.0, current version:1.26.0\n",
      "Package:azureml-telemetry, training version:1.30.0, current version:1.26.0\n",
      "Package:azureml-train-automl-client, training version:1.30.0, current version:1.26.0\n",
      "Package:azureml-train-automl-runtime, training version:1.30.0, current version:1.26.0\n",
      "WARNING:root:Below packages were used for model training but missing in current environment:\n",
      "WARNING:root:Package:azureml-mlflow, training version:1.30.0\n",
      "WARNING:root:Please ensure the version of your local conda dependencies match the version on which your model was trained in order to properly retrieve your model.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Using a model specific cluster, per configuration in project specific settings, (the integer of 'model_number' is the base for the name)\n",
      "Note: OVERRIDING enterprise performance settings with project specifics. (to change, set flag in 'dev_test_prod_settings.json' -> override_enterprise_settings_with_model_specific=False)\n",
      "Found existing cluster prj02-m11-dev for project and environment, using it.\n",
      "Succeeded\n",
      "AmlCompute wait for completion finished\n",
      "\n",
      "Minimum number of nodes requested have been provisioned\n",
      "image_build_compute = prj02-m11-dev\n",
      "Created step IN 2 SILVER - ds01_diabetes [765b1b27][59ee5823-5533-47ef-8954-6db1cb9667dc], (This step is eligible to reuse a previous run's output)Created step IN 2 SILVER - ds02_other [d00e55da][03310cbe-a12e-4a22-a88b-141c5731f12e], (This step is eligible to reuse a previous run's output)\n",
      "\n",
      "Created step SILVER MERGED 2 GOLD [1c8c506d][48b865c8-282d-40f8-8008-bbef4ef1e1b4], (This step is eligible to reuse a previous run's output)\n",
      "Created step SCORING GOLD [979271e7][36ecf9ed-8171-403e-b8ec-e682a4acf60d], (This step is eligible to reuse a previous run's output)\n",
      "pub_pipe.name 11_diabetes_model_reg_batch_scoring_pipe_EP_3\n",
      "pub_pipe.id 7659731b-2e34-452a-babe-8149ecd15b70\n",
      "pub_pipe.name 11_diabetes_model_reg_batch_scoring_pipe_EP_4\n",
      "pub_pipe.id e5ed3b30-89fd-4b70-a2a0-403451ea2228\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Another pipeline-type - IN_2_GOLD"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## BUILD IN_2_GOLD_SCORING\r\n",
    "p_factory.create_dataset_scripts_from_template(overwrite_if_exists=True) # Do this once, then edit them manually. overwrite_if_exists=False is DEFAULT\r\n",
    "batch_pipeline = p_factory.create_batch_pipeline(esml_pipeline_types.IN_2_GOLD) # Creates pipeline from template"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2) `CONSUME` pipeline: HowTo"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2a) Consume from `Azure Data factory - BATCH_SCORE Pipeline activity`"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "print(\"2 parameters needed to be set, to call PIPELINE activity\") \r\n",
    "print(\"\")\r\n",
    "print(\"esml_inference_model_version=0 \")\r\n",
    "print(\" - 0 means use latest version, but you can pick whatever version in the DROPDOWN in Azure data factory you want\")\r\n",
    "print(\"esml_scoring_folder_date='2021-06-08 15:35:01.243860'\")\r\n",
    "print(\" - DateTime in UTC format. Example: For daily scoring 'datetime.datetime.now()'\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2 parameters needed to be set, to call PIPELINE activity\n",
      "\n",
      "esml_inference_model_version=0 \n",
      " - 0 means use latest version, but you can pick whatever version in the DROPDOWN in Azure data factory you want\n",
      "esml_scoring_folder_date='2021-06-08 15:35:01.243860'\n",
      " - DateTime in UTC format. Example: For daily scoring 'datetime.datetime.now()'\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(\"2) Fetch scored data: Below needed for Azure Data factory PIPELINE activity (Pipeline OR Endpoint. Choose the latter\") \r\n",
    "print (\"- Endpoint ID\")\r\n",
    "print(\"Endpoint ID:  {}\".format(endpoint.id))\r\n",
    "print(\"Endpoint Name:  {}\".format(endpoint.name))\r\n",
    "print(\"Experiment name:  {}\".format(p_factory.experiment_name))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2) Fetch scored data: Below needed for Azure Data factory WRITEBACK activity (Pipeline OR Endpoint. Choose the latter\n",
      "- Endpoint ID\n",
      "Endpoint ID:  7659731b-2e34-452a-babe-8149ecd15b70\n",
      "Endpoint Name:  11_diabetes_model_reg_batch_scoring_pipe_EP_3\n",
      "Experiment name:  11_diabetes_model_reg_batch_scoring_pipe\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2b) Consume from `Azure Data factory - WriteBack Pipeline activity`"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "from azureml.core.dataset import Dataset\r\n",
    "from azureml.core import Experiment\r\n",
    "from  azureml.pipeline.core import PipelineRun\r\n",
    "\r\n",
    "# 1st you need a \"Post scoring\" activity, to get metadata of \"scored_gold_path\" from \"last_gold_run.csv\"\r\n",
    "ds1 = Dataset.get_by_name(workspace = p.ws, name =  p.dataset_gold_scored_runinfo_name_azure)\r\n",
    "run_id = ds1.to_pandas_dataframe().iloc[0][\"pipeline_run_id\"] # ['pipeline_run_id', 'scored_gold_path', 'date_in_parameter', 'date_at_pipeline_run','model_version'])\r\n",
    "scored_gold_path = ds1.to_pandas_dataframe().iloc[0][\"scored_gold_path\"]\r\n",
    "\r\n",
    "print(\"Read this meta-dataset from ADF: {}/last_gold_run.csv\".format(p.path_inference_gold_scored_runinfo))\r\n",
    "print(\"- To get the column 'scored_gold_path' which points to the scored-data:\")\r\n",
    "print(\"{}*.parquet\".format(scored_gold_path))\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Read this meta-dataset from ADF: projects/project002/11_diabetes_model_reg/inference/active/gold_scored_runinfo/last_gold_run.csv\n",
      "- To get the column 'scored_gold_path' which points to the scored-data:\n",
      "projects/project002/11_diabetes_model_reg/inference/1/scored/dev/2021/06/08/21bf4e04-df7b-4c3a-a8f8-f3b32bf3a02e/*.parquet\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2b) Consume from `from PYTHON`\r\n",
    "- Run a pipeline endpoint (`Python SDK` call)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "from azureml.pipeline.core import PipelineEndpoint\r\n",
    "pipeline_endpoint = PipelineEndpoint.get(workspace=p.ws, name=p_factory.name_batch_pipeline_endpoint)\r\n",
    "pipeline_run_sdk = pipeline_endpoint.submit(p_factory.experiment_name)\r\n",
    "pipeline_run_sdk.id"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "pipeline_run_sdk.status"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'Running'"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2c) Consume from `from PYTHON`\r\n",
    "- Run via REST call"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "from azureml.pipeline.core import PublishedPipeline,PipelineEndpoint,PipelineRun\r\n",
    "import requests\r\n",
    "from azureml.core.authentication import ServicePrincipalAuthentication # InteractiveLoginAuthentication, AzureCliAuthentication\r\n",
    "\r\n",
    "sp = p.get_authenticaion_header_sp()\r\n",
    "auth_header = sp.get_authentication_header()\r\n",
    "date_folder = str(p.date_scoring_folder)\r\n",
    "pipeline_endpoint = PipelineEndpoint.get(workspace=p.ws, name=p_factory.name_batch_pipeline_endpoint)\r\n",
    "\r\n",
    "response = requests.post(pipeline_endpoint.endpoint,\r\n",
    "                         headers=auth_header,\r\n",
    "                         json={\"ExperimentName\": p_factory.experiment_name,\r\n",
    "                               \"ParameterAssignments\": {\r\n",
    "                                     \"esml_inference_model_version\": p.inferenceModelVersion,\r\n",
    "                                     \"esml_scoring_folder_date\": date_folder\r\n",
    "                                     }\r\n",
    "                              })"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "try:\r\n",
    "    response.raise_for_status()\r\n",
    "except Exception:    \r\n",
    "    raise Exception(\"Received bad response from the endpoint: {}\\n\"\r\n",
    "                    \"Response Code: {}\\n\"\r\n",
    "                    \"Headers: {}\\n\"\r\n",
    "                    \"Content: {}\".format(rest_endpoint, response.status_code, response.headers, response.content))\r\n",
    "\r\n",
    "run_id = response.json().get('Id')\r\n",
    "print('Submitted pipeline run: ', run_id)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Submitted pipeline run:  ebfd7102-e8af-47ab-a255-97b9ce334e4e\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### View status from REST call, via SDK"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "from azureml.pipeline.core import PublishedPipeline,PipelineEndpoint,PipelineRun\r\n",
    "published_pipeline_run = PipelineRun(p.ws.experiments[p_factory.experiment_name], run_id)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "published_pipeline_run.status"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'Running'"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# `WHO is the caller, usually?` (Azure Data factory, Azure Devops)` - that sends PARAMETERS and WHY?` \r\n",
    "### Q: Why? \r\n",
    "- A: To use same DEV scoring pipeline, with either different data to be scored `daily scoring`, or `different model-version SAME day` to score with.\r\n",
    "- A: To have \"environment parameters (dev,test,prod) we can instatiate a ESMLProject what knows the lake, workspace, makes it easy to create 3 pipelines for dev,test,prod\r\n",
    "    - And data, if 1 LAKE or 3 LAKES (dev,test,prod), they all have data-folders \"dev,test,prod\"\r\n",
    "\r\n",
    "### Who gives input?\r\n",
    "- A) Azure Devops (CI/CD) will trigger TRAIN pipeline, that will end with creating this BATCH SCORING, with \r\n",
    "    - 2 parameters (`esml_environment, esml_inference_model_version`), to CREATE/UPDATE the BATCH pipeline with newly trained model\r\n",
    "    - 1 dummy (`esml_scoring_folder_date`) to test BATCH SCORING after creation.\r\n",
    "- B) Azure Datafactory (read from source, writes as .csv or .parquet to IN-folder), and will trigger BATCH SCORING with:\r\n",
    "    - 2 PIPELINE parameters (`esml_inference_model_version, esml_scoring_folder_date`), to read IN-DATA to be scored. Usually \"todays\" esml_scoring_folder_date\r\n",
    "    - Note: To solve \"many scorings same day\", a \"run.id\" folder is created before the actual data.parquet\r\n",
    "    - Note: `*esml_environment` is not really needed post creation - since we already created the pipleine in DEV, `locked and loaded`\r\n",
    "\r\n",
    "### Who needs `scored_data` and HOW to get it? META data:\r\n",
    "- `Azure Datafactory` can read meta data of `last scored GOLD`, to get datalake-path of SCORED_GOLD - can then \"`write back scored data`\" to source, or another `system`\r\n",
    "    - See next cells \"`Get previous RUN and PIPELINE via `ESML` metadata`\"\r\n",
    "- `Power BI` can read the meta-data to fetch `last scored GOLD` directly"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Get previous RUN and PIPELINE via `ESML` metadata\r\n",
    "- How to get path of `scored_gold_path` and how to see the actual `pipeline run`"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "from azureml.core.dataset import Dataset\r\n",
    "from azureml.core import Experiment\r\n",
    "from  azureml.pipeline.core import PipelineRun\r\n",
    "\r\n",
    "# Get \"Pipeline run\" info, for tghe most recent \"latest scored gold\"\r\n",
    "ds1 = Dataset.get_by_name(workspace = p.ws, name =  p.dataset_gold_scored_runinfo_name_azure)\r\n",
    "run_id = ds1.to_pandas_dataframe().iloc[0][\"pipeline_run_id\"] # ['pipeline_run_id', 'scored_gold_path', 'date_in_parameter', 'date_at_pipeline_run','model_version'])\r\n",
    "scored_gold_path = ds1.to_pandas_dataframe().iloc[0][\"scored_gold_path\"]\r\n",
    "\r\n",
    "print(\"dataset_gold_scored_runinfo, location: {}\".format)\r\n",
    "print(\"pipeline_run_id: {}\".format(run_id))\r\n",
    "print(\"scored_gold_path: '{}'\".format(scored_gold_path))\r\n",
    "\r\n",
    "experiment = Experiment(workspace=p.ws, name=p_factory.experiment_name)\r\n",
    "remote_run = PipelineRun(experiment=experiment, run_id=run_id)\r\n",
    "print(\"\\nFetched RUN object {}\".format(remote_run))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "pipeline_run_id: 0a7a6ad1-5493-4855-9cab-342ae7da29f0\n",
      "scored_gold_path: 'projects/project002/11_diabetes_model_reg/inference/1/scored/dev/2021/06/08/0a7a6ad1-5493-4855-9cab-342ae7da29f0/'\n",
      "\n",
      "Fetched RUN object Run(Experiment: 11_diabetes_model_reg_batch_scoring_pipe,\n",
      "Id: 0a7a6ad1-5493-4855-9cab-342ae7da29f0,\n",
      "Type: azureml.PipelineRun,\n",
      "Status: Completed)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# `What can you configure?` (parameters, step compute, custom code)\r\n",
    "## 1) Configure: Parameters\r\n",
    "- Pipeline parameters: scoring_date, model_version\r\n",
    "    - Why: To dynamically select different data & model to score with, with same pipeline/reuse.\r\n",
    "    - Who: Azure data factory can dynamically set these, and call AML pipline\r\n",
    "- Pipeline parameters (model specific): target_column_name\r\n",
    "    - Why: To merge datasets to GOLD.\r\n",
    "print(\"Model version (pipeline parameter): {}\".format(p_factory.batch_pipeline_parameters[0].default_value))\r\n",
    "print(\" - This default value is set from ESMLProject settings: {}\".format(p.inferenceModelVersion))\r\n",
    "print(\"Scoring datetime: {}\".format(p_factory.batch_pipeline_parameters[1].default_value))\r\n",
    "print(\" - This default value is set from ESMLProject settings: {}\".format(str(p.date_scoring_folder)))\r\n",
    "# Optional parameters to READ or SET\r\n",
    "#parameters[2].name: parameters[2].default_value, # esml_optional_unique_scoring_folder \r\n",
    "#parameters[3].name: parameters[3].default_value # par_esml_dev_test_prod\r\n",
    "## 2) Configure: Compute & Environment (via ESML config or inject your own)\r\n",
    "- `Different compute per step OR samee for all` [\"cpu\",\"gpu\", \"databricks\"], based on your ESML environment (dev,test,prod) compute settings, and Dataset properties.\r\n",
    " - A) `Different compute for all steps`\r\n",
    "\r\n",
    "        -  if(dataset.cpu_gpu_databricks == \"cpu\"):\r\n",
    "        -       compute, runconfig = self.init_cpu_environment()\r\n",
    "        -  elif(d.cpu_gpu_databricks == \"databricks\"):\r\n",
    "        -     compute, runconfig = self.init_databricks_environment()\r\n",
    "        -  elif(d.cpu_gpu_databricks == \"gpu\"):\r\n",
    "        -       compute, runconfig = self.init_gpu_environment()\r\n",
    "- B) `Same compute for all`: For the full pipeline, is the DEFAULT behaviour.\r\n",
    "\r\n",
    "        - def `create_batch_scoring_pipe(self, `same_compute_for_all=True`, `cpu_gpu_databricks=\"cpu\")`\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# DDL \"IN\" and \"WriteBack\" table: SQL Server\r\n",
    " - Tables to create for WriteBack demo"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "```sql\r\n",
    "-- 1) IN DATA to Lake, anonymized\r\n",
    "CREATE TABLE [dbo].[esml_diabetes]\r\n",
    "(\r\n",
    "    -- PersonId: Not needed for ML scoring. It is actually only noise for the Machine Learning brain. \r\n",
    "    PersonId INT IDENTITY(1,1) not null, -- But IF we want to reconnect scored RESULT to an individual, we need it.\r\n",
    "\tAGE FLOAT NOT NULL,\r\n",
    "\tSEX FLOAT NOT NULL,\r\n",
    "\tBMI FLOAT NOT NULL,\r\n",
    "\tBP FLOAT NOT NULL,\r\n",
    "\tS1 FLOAT NOT NULL,\r\n",
    "\tS2 FLOAT NOT NULL,\r\n",
    "\tS3 FLOAT NOT NULL,\r\n",
    "\tS4 FLOAT NOT NULL,\r\n",
    "\tS5 FLOAT NOT NULL,\r\n",
    "\tS6 FLOAT NOT NULL\r\n",
    ")\r\n",
    "\r\n",
    "-- 2) Scored data the PIPELINE WroteBack\r\n",
    "CREATE TABLE [dbo].[esml_personID_scoring]\r\n",
    "(\r\n",
    "    PersonId INT NOT NULL,\r\n",
    "    DiabetesMLScoring DECIMAL NULL,\r\n",
    "    scoring_time DATETIME NULL,\r\n",
    "    in_data_time DATETIME NULL,\r\n",
    "    ts DATETIME NOT NULL DEFAULT (GETDATE())\r\n",
    ")\r\n",
    "-- SELECT Count(*) as total_rows FROM [dbo].[esml_personID_scoring] -- 442 rows per RUN since \"UPSERT\" from Azure Datafactory on PersonID\r\n",
    "-- SELECT * FROM [dbo].[esml_personID_scoring]\r\n",
    "\r\n",
    "-- 3) VIEW Person connected to scoring: Risk of DIABETES\r\n",
    "\r\n",
    "--SELECT * FROM [dbo].[esml_diabetes] as a\r\n",
    "SELECT * FROM [dbo].[esml_person_info] as a\r\n",
    "LEFT JOIN [dbo].[esml_personID_scoring] as b\r\n",
    "ON a.PersonId = b.PersonId\r\n",
    "\r\n",
    "```"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3fec2c5a411dce07235ef28c8752b6cecf1f94423de7e7c24e62fc38b1bc47de"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.12 64-bit ('azure_automl': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}