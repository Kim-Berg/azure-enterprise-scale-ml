{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.12 64-bit ('azure_automl': conda)"
  },
  "interpreter": {
   "hash": "3fec2c5a411dce07235ef28c8752b6cecf1f94423de7e7c24e62fc38b1bc47de"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 1) Init ESML "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "sys.path.append(os.path.abspath(\"../azure-enterprise-scale-ml/esml/common/\"))  # NOQA: E402\r\n",
    "from esml import ESMLProject\r\n",
    "\r\n",
    "p = ESMLProject() # Makes it \"environment aware (dev,test,prod)\", and \"configuration aware\"\r\n",
    "ws = p.get_workspace_from_config()\r\n",
    "p.describe()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Inference version: 1\n",
      "\n",
      " - ds01_diabetes\n",
      "projects/project002/11_diabetes_model_reg/inference/1/ds01_diabetes/in/dev/2021/06/22/\n",
      "projects/project002/11_diabetes_model_reg/inference/1/ds01_diabetes/out/bronze/dev/\n",
      "projects/project002/11_diabetes_model_reg/inference/1/ds01_diabetes/out/silver/dev/\n",
      "\n",
      " - ds02_other\n",
      "projects/project002/11_diabetes_model_reg/inference/1/ds02_other/in/dev/2021/06/22/\n",
      "projects/project002/11_diabetes_model_reg/inference/1/ds02_other/out/bronze/dev/\n",
      "projects/project002/11_diabetes_model_reg/inference/1/ds02_other/out/silver/dev/\n",
      " \n",
      "\n",
      "Training GOLD (p.GoldPath)\n",
      "projects/project002/11_diabetes_model_reg/inference/1/gold/dev/\n",
      " \n",
      "\n",
      "[A) USAGE]: to_score_folder, scored_folder, date_folder = p.get_gold_scored_unique_path()\n",
      "A)INFERENCE ONLINE: GOLD to score (example if realtime - today)\n",
      "projects/project002/11_diabetes_model_reg/inference/1/gold/dev/2021_08_25/a78786896c7e42299141326377877c94/\n",
      " \n",
      "\n",
      "A)INFERENCE ONLINE: GOLD scored (example if realtime today)\n",
      "projects/project002/11_diabetes_model_reg/inference/1/scored/dev/2021_08_25/a78786896c7e42299141326377877c94/\n",
      " \n",
      "\n",
      "[B) USAGE]: to_score_folder_batch, scored_folder, date_folder = p.get_gold_scored_unique_path(p.date_scoring_folder)\n",
      "B)INFERENCE BATCH: GOLD to score (example batch, datetime from config)\n",
      "projects/project002/11_diabetes_model_reg/inference/1/gold/dev/2021_06_22/b9ccbc8103a642e1bfdff572cc2c75b3/\n",
      " \n",
      "\n",
      "B)INFERENCE BATCH: GOLD scored (example batch, datetime from config)\n",
      "projects/project002/11_diabetes_model_reg/inference/1/scored/dev/2021_06_22/b9ccbc8103a642e1bfdff572cc2c75b3/\n",
      " \n",
      "\n",
      "C) INFERENCE BATCH (SCENARIO 2): TODAY I scored data from X days AGO  (second datefolder from config - X days ago)\n",
      "projects/project002/11_diabetes_model_reg/inference/1/gold/dev/2021_06_22/b9ccbc8103a642e1bfdff572cc2c75b3/2021_06_22/\n",
      "projects/project002/11_diabetes_model_reg/inference/1/scored/dev/2021_06_22/b9ccbc8103a642e1bfdff572cc2c75b3/2021_06_22/\n",
      " \n",
      "\n",
      "ENVIRONMENT - DEV, TEST, or PROD?  [USAGE: p.dev_test_prod]\n",
      "ACTIVE ENVIRONMENT = dev\n",
      "ACTIVE subscription = ca0a8c40-b06a-4e4e-8434-63c03a1dee34\n",
      "- MSFT-WEU-EAP_PROJECT02_AI-DEV-RG\n",
      "- msft-weu-DEV-eap-proj02_ai-amls\n",
      "- westeurope\n",
      "- MSFT-WEU-EAP_CMN_AI-DEV-RG\n",
      "Active vNet: msft-weu-dev-cmnai-vnet\n",
      "Active SubNet: \n",
      "[USAGE] for the above: p.vNetForActiveEnvironment()\n",
      "Active Lake (storage account)  msftweudevcmnai2\n",
      "[USAGE] for the above: p.getLakeForActiveEnvironment()\n",
      "AML for docker: True\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "unregister_all_datasets=False"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "if(unregister_all_datasets):\r\n",
    "    p.unregister_all_datasets(ws) # For DEMO purpose\r\n",
    "\r\n",
    "p.init(ws)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "...\n",
      "Using GEN2 as Datastore\n",
      "Searching for setting in ESML datalake...\n",
      "ESML in-folder settings override = TRUE \n",
      " - Found settings in the ESML AutoLake  [active_in_folder.json,active_scoring_in_folder.json], to override ArgParse/GIT config with.\n",
      " - TRAIN in date:  2021/01/01\n",
      " - INFERENCE in date: 2021/06/08 and ModelVersion to score with: 1 (0=latest)\n",
      "\n",
      "Inference mode (False = Training mode): True\n",
      "Load data as Datasets....\n",
      "ds01_diabetes\n",
      "ds02_other\n",
      "\n",
      "####### Automap & Autoregister - SUCCESS!\n",
      "1) Auto mapped 2 ESML Dataset with registered Azure ML Datasets (potentially all 3: IN,BRONZE, SILVER) in Datastore project002 \n",
      "\n",
      "Dataset 'ds01_diabetes' status:\n",
      " - IN_Folder_has_files\n",
      " - BRONZE_Folder_has_files\n",
      " - SILVER_Folder_has_files\n",
      "Dataset 'ds02_other' status:\n",
      " - IN_Folder_has_files\n",
      " - BRONZE_Folder_has_files\n",
      " - SILVER_Folder_has_files\n",
      "\n",
      "2) Registered each Dataset with suffixes (_IN, _BRONZE, _SILVER) \n",
      " Tip: Use ESMLProject.Datasets list or .DatasetByName(myDatasetName) to read/write\n",
      "#######\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2) Test an `existing` Model webservice (AKS)?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2a) Lets simulate INFERENCE: data arives from CLIENT to a `DriverAPI` that itself will call `this ModelAPI` (AKS)\n",
    "Note: `Step 1 and 2 are out of scope` of from the MACHINE LEARNING solution (and ESML), other than...\n",
    " - `ESML` being the creator of AML pipeline - `Bronze_2_Gold` that you can reuse\n",
    " - `ESML lakedesign`, that should be used, if INFERENCE are to be stored/cached in the lake\n",
    "\n",
    "NOW - the scenario: \n",
    "\n",
    "- DriverAPI will:\n",
    "\n",
    "    - 1)Fetch missing datasources from different systems, to get all data sources ds01, ds02,ds03 & SAVE to INFERENCE/IN folder in the datalake\n",
    "        - Use the `ESML` lakedesign:  IF `inference_model_version=1` in `settings/project_specific/lake_settings.json` then...\n",
    "        - `ESML` will switch to write at `INFERENCE/v1/ds01/IN/2020/01/01` folder \n",
    "    - 2)Call `Bronze_2_Gold` batch pipeline (AML) - same pipeline that was used for training: `IN->Bronze->Silver->Gold`, except...\n",
    "        - `Bronze_2_Gold` pipeline will read from IN folder, but configured for \"INFERENCE\", read/write to `\"mirror\"` place in the datalake, that will `CACHE` online predicted results\n",
    "        - (LABEL values are missing of course)\n",
    "    - 3) `DriverAPI calls ModelAPI` (This AKS webservuce) with data to score -> `X_text`\n",
    "\n",
    "- `ESML` ModelAPI will:\n",
    "    - 4) `Score` the data, and return results, also `save the results to the datalake`\n",
    "        - if `p.rnd=True` nothing is stored.\n",
    "    - 5) `Connect the scored data` to a caller, and the individual scores to a identity (user/machine)\n",
    "        - GLOBALLY & ESML Managed: The scoring file gets a caller_id. \n",
    "            - You: Need to pass the called-guid as a parameter. \n",
    "                - Alternatively, you can have a column in the dataframe called `esml_caller_id_string`, and ESML will use that.\n",
    "        - LOCALLY: The rows in the dataframe has an identity. \n",
    "            - You: Here you need to have a column in the dataframe, such as `user_id`, to be able to connect each row to a scoring.\n",
    "\n",
    "\n",
    "Step 1 and 2 are out of scope of from the MACHINE LEARNING solution  \n",
    "We START this notebooke at step 3 - we have `X_test` in `GOLD_Validate` status "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "import datetime as dt\r\n",
    "now = dt.datetime.now()\r\n",
    "folder = now.strftime('%Y_%m_%d') \r\n",
    "print(folder)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2021_08_25\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "if(unregister_all_datasets): # Create a GOLD dataset, and SPLIT it\r\n",
    "    df_01 = p.DatasetByName(\"ds01_diabetes\").Silver.to_pandas_dataframe()\r\n",
    "    ds_gold_v1 = p.save_gold(df_01)\r\n",
    "\r\n",
    "    label = \"Y\"\r\n",
    "    train_6, validate_set_2, test_set_2 = p.split_gold_3(0.6, label)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "p.GoldValidate.name"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'M11_GOLD_VALIDATE'"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# 2) Bronze_2_Gold done, we can fetch X_test\r\n",
    "X_test, y_test, tags = p.get_gold_validate_Xy() # Version is default latest. \r\n",
    "print(tags)\r\n",
    "#Note: If you get error: AttributeError: 'NoneType' object has no attribute 'name'...you need to have splitted sot you have a GoldValidate Dataset"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "M11_GOLD_VALIDATE : (37, 11)\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyError",
     "evalue": "\"['Survived'] not found in axis\"",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-658dcd5ff51f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# 2) Bronze_2_Gold done, we can fetch X_test\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtags\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_gold_validate_Xy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Version is default latest.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#Note: If you get error: AttributeError: 'NoneType' object has no attribute 'name'...you need to have splitted sot you have a GoldValidate Dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\jostrm\\OneDrive - Microsoft\\0_GIT\\2_My\\github\\azure-enterprise-scale-ml\\azure-enterprise-scale-ml\\esml\\common\\esml.py\u001b[0m in \u001b[0;36mget_gold_validate_Xy\u001b[1;34m(self, label, ds_version)\u001b[0m\n\u001b[0;32m   1515\u001b[0m         \u001b[0mdf_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_pandas_dataframe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1516\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"{} : {}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGoldValidate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1517\u001b[1;33m         \u001b[0mX_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1518\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"X_test \"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1519\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\envs\\azure_automl\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mdrop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   4115\u001b[0m             \u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4116\u001b[0m             \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minplace\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4117\u001b[1;33m             \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4118\u001b[0m         )\n\u001b[0;32m   4119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\envs\\azure_automl\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mdrop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   3912\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;32min\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3913\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3914\u001b[1;33m                 \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_drop_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3915\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3916\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\envs\\azure_automl\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_drop_axis\u001b[1;34m(self, labels, axis, level, errors)\u001b[0m\n\u001b[0;32m   3944\u001b[0m                 \u001b[0mnew_axis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3945\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3946\u001b[1;33m                 \u001b[0mnew_axis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3947\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0maxis_name\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnew_axis\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3948\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\envs\\azure_automl\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mdrop\u001b[1;34m(self, labels, errors)\u001b[0m\n\u001b[0;32m   5338\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5339\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m\"ignore\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5340\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"{} not found in axis\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5341\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5342\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['Survived'] not found in axis\""
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "X_test.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2b) Call onlin ModelAPI - score 1 row (alt 1)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "import pandas as pd\n",
    "#3) `DriverAPI calls ModelAPI`\n",
    "\n",
    "keyvault = p.ws.get_default_keyvault()\n",
    "api_url = keyvault.get_secret(name=\"esml-dev-p02-m03-api\")\n",
    "api_key = keyvault.get_secret(name=\"esml-dev-p02-m03-apisecret\")  # esml-dev-p02-m03-apisecret\n",
    "\n",
    "df = ESMLProject.call_webservice_own_url(X_test, api_url,api_key) # rest call. STATIC method - simulate \"we dont need ESML for this\" - just an URL and KEY from DriverAPI\n",
    "df.head() # Print scoring"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2b) Call online ModelAPI - score all rows (alt 2) - `via ESML wrapper`\n",
    "- ESML benefits: Has built in `logic` to save scored_results, to unique folders during the day.\n",
    "- Note: We can also save a GUID/ID, `to see which CALLER/User-Guid the scoring is about` \n",
    "    - We can have a User_id_GUID as a \"feature/column\" in X_test, or as a parameter in the call"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "p.inferenceModelVersion=1\r\n",
    "print(p.ScoredPath)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "caller_user_id = '81965d9c-40ca-4e47-9723-5a608a32a0e4' # Connect the scoring to a caller/user, globally for all rows\r\n",
    "\r\n",
    "df = p.call_webservice(p.ws, X_test, caller_user_id, False) # Auto-fetch key from keyvault, 1stRowOnlye=False\r\n",
    "df.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3) How to GET SCORE data? how to FILTER scored results?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "X_test, y_test, tags = p.get_gold_validate_Xy()\r\n",
    "caller_user_id = '91965d9c-40ca-4e47-9723-5a608a32a0e4' # Connect the scoring to a caller/user, globally for all rows\r\n",
    "\r\n",
    "df = p.call_webservice(p.ws, X_test, caller_user_id, False) # Auto-fetch key from keyvault, 1stRowOnlye=False\r\n",
    "df.head()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "M03_GOLD_VALIDATE : (37, 11)\n",
      "X_test  (37, 10)\n",
      "y_test  (37,)\n",
      "Note: USING enterprise performance settings. (This can be overridden in 'dev_test_prod_settings.json' -> override_enterprise_settings_with_model_specific=True\n",
      "Note: Fetching keys automatically via workspace keyvault.\n",
      "Saving scoring to lake for project folder project002 and inference_model_version: 1 ...\n",
      "...\n",
      "\n",
      "Saved DATA to score successfully in LAKE, as file 'to_score_91965d9c-40ca-4e47-9723-5a608a32a0e4.parquet'\n",
      "Saved SCORED data in LAKE, as file 'scored_91965d9c-40ca-4e47-9723-5a608a32a0e4.parquet'\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "        AGE       SEX       BMI        BP        S1        S2        S3  \\\n",
       "0  0.048974  0.050680  0.123131  0.083844 -0.104765 -0.100895 -0.069172   \n",
       "1  0.070769 -0.044642  0.069241  0.037939  0.021822  0.001504 -0.036038   \n",
       "2  0.056239  0.050680 -0.030996  0.008101  0.019070  0.021233  0.033914   \n",
       "3  0.016281 -0.044642  0.017506 -0.022885  0.060349  0.044406  0.030232   \n",
       "4  0.023546 -0.044642  0.110198  0.063187  0.013567 -0.032942 -0.024993   \n",
       "\n",
       "         S4        S5        S6      result  \n",
       "0 -0.002592  0.036646 -0.030072  243.130371  \n",
       "1  0.039106  0.077633  0.106617  293.235993  \n",
       "2 -0.039493 -0.029528 -0.059067   94.014447  \n",
       "3 -0.002592  0.037232 -0.001078  148.809757  \n",
       "4  0.020655  0.099240  0.023775  252.999922  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AGE</th>\n",
       "      <th>SEX</th>\n",
       "      <th>BMI</th>\n",
       "      <th>BP</th>\n",
       "      <th>S1</th>\n",
       "      <th>S2</th>\n",
       "      <th>S3</th>\n",
       "      <th>S4</th>\n",
       "      <th>S5</th>\n",
       "      <th>S6</th>\n",
       "      <th>result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.048974</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>0.123131</td>\n",
       "      <td>0.083844</td>\n",
       "      <td>-0.104765</td>\n",
       "      <td>-0.100895</td>\n",
       "      <td>-0.069172</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>0.036646</td>\n",
       "      <td>-0.030072</td>\n",
       "      <td>243.130371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.070769</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>0.069241</td>\n",
       "      <td>0.037939</td>\n",
       "      <td>0.021822</td>\n",
       "      <td>0.001504</td>\n",
       "      <td>-0.036038</td>\n",
       "      <td>0.039106</td>\n",
       "      <td>0.077633</td>\n",
       "      <td>0.106617</td>\n",
       "      <td>293.235993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.056239</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>-0.030996</td>\n",
       "      <td>0.008101</td>\n",
       "      <td>0.019070</td>\n",
       "      <td>0.021233</td>\n",
       "      <td>0.033914</td>\n",
       "      <td>-0.039493</td>\n",
       "      <td>-0.029528</td>\n",
       "      <td>-0.059067</td>\n",
       "      <td>94.014447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.016281</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>0.017506</td>\n",
       "      <td>-0.022885</td>\n",
       "      <td>0.060349</td>\n",
       "      <td>0.044406</td>\n",
       "      <td>0.030232</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>0.037232</td>\n",
       "      <td>-0.001078</td>\n",
       "      <td>148.809757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.023546</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>0.110198</td>\n",
       "      <td>0.063187</td>\n",
       "      <td>0.013567</td>\n",
       "      <td>-0.032942</td>\n",
       "      <td>-0.024993</td>\n",
       "      <td>0.020655</td>\n",
       "      <td>0.099240</td>\n",
       "      <td>0.023775</td>\n",
       "      <td>252.999922</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "import datetime as dt\r\n",
    "now = dt.datetime.now()\r\n",
    "date_filter = now.strftime('%Y_%m_%d') \r\n",
    "print(date_filter)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2021_04_10\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "ds_list, df_all = p.get_scored(date_filter, \"10\",'81965d9c-40ca-4e47-9723-5a608a32a0e4')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "#df = p.get_scored('last_month')\r\n",
    "ds_list1, df_all1 = p.get_scored(date_filter, \"1\",'81965d9c-40ca-4e47-9723-5a608a32a0e4')\r\n",
    "print(\"Datasets found in filter\", len(ds_list1))\r\n",
    "print(\"Example: How many rows in 1st dataset? \", ds_list1[0].to_pandas_dataframe().shape[0])\r\n",
    "print(\"All rows\", df_all1.shape[0])\r\n",
    "\r\n",
    "df_all1.head()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Datasets found in filter 1\n",
      "Example: How many rows in 1st dataset?  37\n",
      "All rows 37\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "        AGE       SEX       BMI        BP        S1        S2        S3  \\\n",
       "0  0.048974  0.050680  0.123131  0.083844 -0.104765 -0.100895 -0.069172   \n",
       "1  0.070769 -0.044642  0.069241  0.037939  0.021822  0.001504 -0.036038   \n",
       "2  0.056239  0.050680 -0.030996  0.008101  0.019070  0.021233  0.033914   \n",
       "3  0.016281 -0.044642  0.017506 -0.022885  0.060349  0.044406  0.030232   \n",
       "4  0.023546 -0.044642  0.110198  0.063187  0.013567 -0.032942 -0.024993   \n",
       "\n",
       "         S4        S5        S6      result  \n",
       "0 -0.002592  0.036646 -0.030072  243.130371  \n",
       "1  0.039106  0.077633  0.106617  293.235993  \n",
       "2 -0.039493 -0.029528 -0.059067   94.014447  \n",
       "3 -0.002592  0.037232 -0.001078  148.809757  \n",
       "4  0.020655  0.099240  0.023775  252.999922  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AGE</th>\n",
       "      <th>SEX</th>\n",
       "      <th>BMI</th>\n",
       "      <th>BP</th>\n",
       "      <th>S1</th>\n",
       "      <th>S2</th>\n",
       "      <th>S3</th>\n",
       "      <th>S4</th>\n",
       "      <th>S5</th>\n",
       "      <th>S6</th>\n",
       "      <th>result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.048974</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>0.123131</td>\n",
       "      <td>0.083844</td>\n",
       "      <td>-0.104765</td>\n",
       "      <td>-0.100895</td>\n",
       "      <td>-0.069172</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>0.036646</td>\n",
       "      <td>-0.030072</td>\n",
       "      <td>243.130371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.070769</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>0.069241</td>\n",
       "      <td>0.037939</td>\n",
       "      <td>0.021822</td>\n",
       "      <td>0.001504</td>\n",
       "      <td>-0.036038</td>\n",
       "      <td>0.039106</td>\n",
       "      <td>0.077633</td>\n",
       "      <td>0.106617</td>\n",
       "      <td>293.235993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.056239</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>-0.030996</td>\n",
       "      <td>0.008101</td>\n",
       "      <td>0.019070</td>\n",
       "      <td>0.021233</td>\n",
       "      <td>0.033914</td>\n",
       "      <td>-0.039493</td>\n",
       "      <td>-0.029528</td>\n",
       "      <td>-0.059067</td>\n",
       "      <td>94.014447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.016281</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>0.017506</td>\n",
       "      <td>-0.022885</td>\n",
       "      <td>0.060349</td>\n",
       "      <td>0.044406</td>\n",
       "      <td>0.030232</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>0.037232</td>\n",
       "      <td>-0.001078</td>\n",
       "      <td>148.809757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.023546</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>0.110198</td>\n",
       "      <td>0.063187</td>\n",
       "      <td>0.013567</td>\n",
       "      <td>-0.032942</td>\n",
       "      <td>-0.024993</td>\n",
       "      <td>0.020655</td>\n",
       "      <td>0.099240</td>\n",
       "      <td>0.023775</td>\n",
       "      <td>252.999922</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "ds_list, df_all = p.get_scored(date_filter, \"1\")\r\n",
    "print(\"Datasets found in filter\", len(ds_list))\r\n",
    "print(\"Example: How many rows in 1st dataset? \", ds_list[0].to_pandas_dataframe().shape[0])\r\n",
    "print(\"All rows\", df_all.shape[0])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Datasets found in filter 2\n",
      "Example: How many rows in 1st dataset?  37\n",
      "All rows 74\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    " ** Train model & register ....this happended already in another notebook/MLops pipline step\n",
    "> model = p.register_active_model()  \n",
    "> print(model.name, model.description, model.version)  \n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3a) ESML `Deploy a new ONLINE` webservice (AKS)\n",
    "- Deploy \"offline\" from old `AutoML run` for `DEV` environment\n",
    "- To →  `DEV`, `TEST` or `PROD` environment\n",
    "- ESML saves `API_key in Azure keyvault automatically`\n",
    "- ESML auto-config solves 4 common 'errors/things': `correct compute name` and `valid replicas, valid agents, valid auto scaling`\n",
    "    - Tip: You can adjust the number of replicas, and different CPU/memory configuration, or using a different compute target."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "inference_config, model, best_run = p.get_active_model_inference_config(ws) #  AutoML support \r\n",
    "service,api_uri, kv_aks_api_secret= p.deploy_automl_model_to_aks(model,inference_config, True) # overwrite_endpoint=True"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Test the NEW AKS WebService"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "X_test, y_test, tags = p.get_gold_validate_Xy() # Version is default latest\r\n",
    "print(tags)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "X_test.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df = p.call_webservice(p.ws, X_test) # Auto-fetch key from keyvault, 1stRowOnlye=False  |   p.call_webservice(p.ws, X_test, caller_user_id, False)\r\n",
    "df.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Code to `embed in YOUR DriverApi` - to call this webservice `without ESML`\n",
    "- `NB!` This will `NOT cache` the results automatically, since not going via the ESML SDK. You need to save the scoring yourself ( if this is needed)\n",
    "    - ESML benefits: Has built in `logic` to save scored_results, to unique folders during the day.\n",
    "            # Note: We can also save a GUID/ID, for which CALLER/User-Guid it is about. We can have a User_id_GUID as a \"feature/column\" in X_test, or as a parameter"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def call_webservice_code(rows, api_uri,api_key, allowSelfSigned=True):\r\n",
    "    # bypass the server certificate verification on client side\r\n",
    "    if allowSelfSigned and not os.environ.get('PYTHONHTTPSVERIFY', '') and getattr(ssl, '_create_unverified_context', None):\r\n",
    "        ssl._create_default_https_context = ssl._create_unverified_context\r\n",
    "\r\n",
    "    X_test_json_works = json.dumps({'data': rows.to_dict(orient='records')})\r\n",
    "\r\n",
    "    headers = {'Content-Type':'application/json', 'Authorization': 'Bearer ' + api_key}\r\n",
    "    resp = requests.post(api_uri, X_test_json_works , headers=headers) \r\n",
    "\r\n",
    "    # Here you can pass forward the response, save it to the datalake, or as below return a PANDAS dataframe\r\n",
    "    res_dict = json.loads(resp.text)\r\n",
    "    res_dict_ast = ast.literal_eval(res_dict)\r\n",
    "    return pd.read_json(res_dict) # to pandas"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3b) ESML `Deploy BATCH` pipeline\n",
    "- Deploy same model \"offline / previous\" `AutoML Run` for `DEV` environment\n",
    "- To →  `DEV`, `TEST` or `PROD` environment\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 5a alt2) If not using AutoML - you need to manuallt create `environemnt + entryscript + inference config`\n",
    "https://github.com/Azure/MachineLearningNotebooks/blob/bda592a236eaf2dbc54b394e1fa1b539e0297908/how-to-use-azureml/deployment/production-deploy-to-aks/production-deploy-to-aks.ipynb"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Create ENVIRONMENT - If not AUTOML\r\n",
    "from azureml.core import Environment\r\n",
    "from azureml.core.conda_dependencies import CondaDependencies \r\n",
    "\r\n",
    "conda_deps = CondaDependencies.create(conda_packages=['numpy','scikit-learn==0.19.1','scipy'], pip_packages=['azureml-defaults', 'inference-schema'])\r\n",
    "myenv = Environment(name='myenv')\r\n",
    "myenv.python.conda_dependencies = conda_deps"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%%writefile score.py\r\n",
    "import os\r\n",
    "import pickle\r\n",
    "import json\r\n",
    "import numpy\r\n",
    "from sklearn.externals import joblib\r\n",
    "from sklearn.linear_model import Ridge\r\n",
    "\r\n",
    "def init():\r\n",
    "    global model\r\n",
    "    # AZUREML_MODEL_DIR is an environment variable created during deployment.\r\n",
    "    # It is the path to the model folder (./azureml-models/$MODEL_NAME/$VERSION)\r\n",
    "    # For multiple models, it points to the folder containing all deployed models (./azureml-models)\r\n",
    "    model_path = os.path.join(os.getenv('AZUREML_MODEL_DIR'), 'sklearn_regression_model.pkl')\r\n",
    "    # deserialize the model file back into a sklearn model\r\n",
    "    model = joblib.load(model_path)\r\n",
    "\r\n",
    "# note you can pass in multiple rows for scoring\r\n",
    "def run(raw_data):\r\n",
    "    try:\r\n",
    "        data = json.loads(raw_data)['data']\r\n",
    "        data = numpy.array(data)\r\n",
    "        result = model.predict(data)\r\n",
    "        # you can return any data type as long as it is JSON-serializable\r\n",
    "        return result.tolist()\r\n",
    "    except Exception as e:\r\n",
    "        error = str(e)\r\n",
    "        return error"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from azureml.core.model import InferenceConfig\r\n",
    "inf_config = InferenceConfig(entry_script='score.py', environment=myenv)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# OTHER - What more can you retrieve from AutoMLFactory and ESMLProject?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from baselayer_azure_ml import AutoMLFactory\r\n",
    "from baselayer_azure_ml import ComputeFactory\r\n",
    "\r\n",
    "target_model, target_best_run_id = AutoMLFactory().get_latest_model(ws)\r\n",
    "print(target_model.name)\r\n",
    "print(target_best_run_id)\r\n",
    "\r\n",
    "run, exp = p.get_active_model_run_and_experiment()\r\n",
    "inference_config = p.get_active_model_inference_config()\r\n",
    "print(run)\r\n",
    "print(exp.name)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ]
}