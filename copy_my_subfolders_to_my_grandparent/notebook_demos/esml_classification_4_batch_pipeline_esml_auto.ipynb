{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# ESML - accelerator: Batch scoring pipeline. 6 acceleration benefits \r\n",
    "- 1) `AutoMap datalake` & init ESML project\r\n",
    "- 2) `Get correct environment` - via ESML config, to the correct Workspace(dev,test,prod)\r\n",
    "- 3) `1 line: Get Compute cluster` from ESML.get_training_aml_compute \r\n",
    "- 4) `1 line: Get earlier trained model` and `Inference ENVIRONMENT` from AutoML via ESMLProject.get_active_model_inference_config()\r\n",
    "- 5) `DATASET via properties:  `p.DatasetByName(\"ds01_diabetes\").Bronze `and `p.GoldToScore`  `and `p.GoldScored`\r\n",
    "    - Or via conventions name from portal `M11_ds02_other_inference_BRONZE` and `M11_GOLD_TO_SCORE` and  `M11_GOLD_SCORED`\r\n",
    "- 6) `Score and Writeback`: Save to scored data, with metadata for easy `WriteBack` functionality to source from Azure Data factory\r\n",
    "\r\n",
    "# USAGE:\r\n",
    "You can use ESMLPipeline factory like this notebook:\r\n",
    "`ESMLPipeline factory will build the pipeline automatically`, all steps based on the dataset array in the `model_settings.json` and witht the `ESML Datamodel: Bronze->Silver-Gold` \r\n",
    "\r\n",
    "## THIS CODE... ↓\r\n",
    "```\r\n",
    "p = ESMLProject()\r\n",
    "p_factory = ESMLPipelineFactory(p, \"Y\")\r\n",
    "\r\n",
    "p_factory.create_dataset_scripts_from_template() # Do this once, then edit them manually\r\n",
    "batch_pipeline = p_factory.create_batch_scoring_pipe()\r\n",
    "\r\n",
    "pipeline_run = p_factory.execute_pipeline(batch_pipeline)\r\n",
    "pipeline_run.wait_for_completion(show_output=False)\r\n",
    "```\r\n",
    "\r\n",
    "## ...WILL GIVE YOU THAT PIPELINE ↓ (Note: Datastore is Azure Datlake GEN2 : )\r\n",
    "You will get an image like this, if you have 2 datasets in lake_settings.json\r\n",
    "\r\n",
    "![](../azure-enterprise-scale-ml/esml/images/aml-pipeline_batch_ppt-3.png)\r\n",
    "\r\n",
    "- Above PIPELINE: \"dataset_folder_names\": [\"ds01_diabetes\", \"ds02_other\"]\r\n",
    "- You can edit each `step-file.py` which is generate by ESML"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "######  NB! This,InteractiveLoginAuthentication, is only needed to run 1st time, then when ws_config is written, use later CELL in notebook, that just reads that file\r\n",
    "import repackage\r\n",
    "repackage.add(\"../azure-enterprise-scale-ml/esml/common/\")\r\n",
    "from azureml.core import Workspace\r\n",
    "from azureml.core.authentication import InteractiveLoginAuthentication\r\n",
    "from esml import ESMLDataset, ESMLProject\r\n",
    "\r\n",
    "p = ESMLProject()\r\n",
    "p.dev_test_prod=\"dev\"\r\n",
    "auth = InteractiveLoginAuthentication(tenant_id = p.tenant)\r\n",
    "ws, config_name = p.authenticate_workspace_and_write_config(auth)\r\n",
    "######  NB!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1) `One cell of code: 3 lines` below, to create above, and execute it"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import repackage\r\n",
    "repackage.add(\"../azure-enterprise-scale-ml/esml/common/\")\r\n",
    "from esml import ESMLProject\r\n",
    "from baselayer_azure_ml_pipeline import ESMLPipelineFactory, esml_pipeline_types\r\n",
    "p = ESMLProject()\r\n",
    "\r\n",
    "p_factory = ESMLPipelineFactory(p, \"Survived\")\r\n",
    "scoring_date = '2021-01-01 10:35:01.243860'\r\n",
    "p_factory.batch_pipeline_parameters[1].default_value = scoring_date # overrides ESMLProject.date_scoring_folder.\r\n",
    "p_factory.describe()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      " ---- Q: WHICH files are generated as templates, for you to EDIT? ---- \n",
      "A: These files & locations:\n",
      "File to EDIT (step: IN_2_SILVER_1): ../../../2_A_aml_pipeline/4_inference/batch/M10/in2silver_ds01_titanic.py\n",
      "File to EDIT (step: IN_2_SILVER_2): ../../../2_A_aml_pipeline/4_inference/batch/M10/in2silver_ds02_haircolor.py\n",
      "File to EDIT (step: IN_2_SILVER_3): ../../../2_A_aml_pipeline/4_inference/batch/M10/in2silver_ds03_housing.py\n",
      "File to EDIT (step: IN_2_SILVER_4): ../../../2_A_aml_pipeline/4_inference/batch/M10/in2silver_ds04_lightsaber.py\n",
      "File to EDIT (step: SILVER_MERGED_2_GOLD): ../../../2_A_aml_pipeline/4_inference/batch/M10/silver_merged_2_gold.py\n",
      "File to EDIT (step: SCORING_GOLD): ../../../2_A_aml_pipeline/4_inference/batch/M10/scoring_gold.py\n",
      "File to EDIT a lot (reference in step-scripts Custom code): ../../../2_A_aml_pipeline/4_inference/batch/M10/your_code/your_custom_code.py\n",
      "\n",
      " ---- WHAT model to SCORE with, & WHAT data 'date_folder'? ---- \n",
      "InferenceModelVersion (model version to score with): 1\n",
      "Date_scoring_folder (data to score) : 2021-01-01 10:35:01.243860\n",
      "ESML environment: dev\n",
      "\n",
      " ---- ESML Datalake locations: ESML Datasets (IN-data) ---- \n",
      "Name (lake folder): ds01_titanic and AzureName IN: M10_ds01_titanic_inference_IN\n",
      "IN projects/project002/10_titanic_model_clas/inference/1/ds01_titanic/in/dev/2021/06/08/\n",
      "Bronze projects/project002/10_titanic_model_clas/inference/1/ds01_titanic/out/bronze/dev/\n",
      "Silver projects/project002/10_titanic_model_clas/inference/1/ds01_titanic/out/silver/dev/\n",
      "\n",
      "Name (lake folder): ds02_haircolor and AzureName IN: M10_ds02_haircolor_inference_IN\n",
      "IN projects/project002/10_titanic_model_clas/inference/1/ds02_haircolor/in/dev/2021/06/08/\n",
      "Bronze projects/project002/10_titanic_model_clas/inference/1/ds02_haircolor/out/bronze/dev/\n",
      "Silver projects/project002/10_titanic_model_clas/inference/1/ds02_haircolor/out/silver/dev/\n",
      "\n",
      "Name (lake folder): ds03_housing and AzureName IN: M10_ds03_housing_inference_IN\n",
      "IN projects/project002/10_titanic_model_clas/inference/1/ds03_housing/in/dev/2021/06/08/\n",
      "Bronze projects/project002/10_titanic_model_clas/inference/1/ds03_housing/out/bronze/dev/\n",
      "Silver projects/project002/10_titanic_model_clas/inference/1/ds03_housing/out/silver/dev/\n",
      "\n",
      "Name (lake folder): ds04_lightsaber and AzureName IN: M10_ds04_lightsaber_inference_IN\n",
      "IN projects/project002/10_titanic_model_clas/inference/1/ds04_lightsaber/in/dev/2021/06/08/\n",
      "Bronze projects/project002/10_titanic_model_clas/inference/1/ds04_lightsaber/out/bronze/dev/\n",
      "Silver projects/project002/10_titanic_model_clas/inference/1/ds04_lightsaber/out/silver/dev/\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1a) - BUILD & `RUN` pipeline (2-liner)\r\n",
    "- `Iterate` until you are happy with the pipeline (edit step files etc)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "## BUILD\r\n",
    "p_factory.create_dataset_scripts_from_template(overwrite_if_exists=True) # Do this once, then edit them manually. overwrite_if_exists=False is DEFAULT\r\n",
    "batch_pipeline = p_factory.create_batch_pipeline(esml_pipeline_types.IN_2_GOLD_SCORING) # Creates pipeline from template\r\n",
    "\r\n",
    "## RUN\r\n",
    "pipeline_run = p_factory.execute_pipeline(batch_pipeline)\r\n",
    "pipeline_run.wait_for_completion(show_output=False)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Creates template step_files.py for user to edit at:\n",
      "Edit at ../../../2_A_aml_pipeline/4_inference/batch/M10/in2silver_ds01_titanic.py\n",
      "Edit at ../../../2_A_aml_pipeline/4_inference/batch/M10/in2silver_ds02_haircolor.py\n",
      "Edit at ../../../2_A_aml_pipeline/4_inference/batch/M10/in2silver_ds03_housing.py\n",
      "Edit at ../../../2_A_aml_pipeline/4_inference/batch/M10/in2silver_ds04_lightsaber.py\n",
      "Edit at ../../../2_A_aml_pipeline/4_inference/batch/M10/silver_merged_2_gold.py\n",
      "Edit at ../../../2_A_aml_pipeline/4_inference/batch/M10/scoring_gold.py\n",
      "Edit at ../../../2_A_aml_pipeline/4_inference/batch/M10/your_code/your_custom_code.py\n",
      "Using GEN2 as Datastore\n",
      "found model via REMOTE FILTER: Experiment TAGS: model name and version\n",
      "Note: OVERRIDING enterprise performance settings with project specifics. (to change, set flag in 'dev_test_prod_settings.json' -> override_enterprise_settings_with_model_specific=False)\n",
      "Using a model specific cluster, per configuration in project specific settings, (the integer of 'model_number' is the base for the name)\n",
      "Note: OVERRIDING enterprise performance settings with project specifics. (to change, set flag in 'dev_test_prod_settings.json' -> override_enterprise_settings_with_model_specific=False)\n",
      "Found existing cluster prj02-m10-dev for project and environment, using it.\n",
      "Succeeded\n",
      "AmlCompute wait for completion finished\n",
      "\n",
      "Minimum number of nodes requested have been provisioned\n",
      "image_build_compute = prj02-m10-dev\n",
      "Created step IN 2 SILVER - ds01_titanic [6a42c915][76b5cf4d-f19c-49b1-84ab-8871aef261db], (This step will run and generate new outputs)Created step IN 2 SILVER - ds02_haircolor [bbd323b4][091ad8d5-52f5-4b4e-86a6-3936a2e2401e], (This step will run and generate new outputs)\n",
      "\n",
      "Created step IN 2 SILVER - ds03_housing [a639cdcc][ff85cb27-2535-485b-b03e-1d55876fc830], (This step will run and generate new outputs)\n",
      "Created step IN 2 SILVER - ds04_lightsaber [098a06c1][a3f144b9-3aec-4c06-bfc7-f7fe5b32c594], (This step will run and generate new outputs)\n",
      "Created step SILVER MERGED 2 GOLD [c073b102][6c537e86-0672-4e2f-b55d-dfe0222a9b80], (This step will run and generate new outputs)\n",
      "Created step SCORING GOLD [f2258ff9][ecec8f97-70bf-47ee-ace5-79d20a6aa609], (This step will run and generate new outputs)\n",
      "Submitted PipelineRun 7a182c1f-69ff-4323-bc0a-5f5979b46399\n",
      "Link to Azure Machine Learning Portal: https://ml.azure.com/runs/7a182c1f-69ff-4323-bc0a-5f5979b46399?wsid=/subscriptions/ca0a8c40-b06a-4e4e-8434-63c03a1dee34/resourcegroups/MSFT-WEU-EAP_PROJECT02_AI-DEV-RG/workspaces/msft-weu-DEV-eap-proj02_ai-amls&tid=72f988bf-86f1-41af-91ab-2d7cd011db47\n",
      "Pipeline submitted for execution!\n",
      "PipelineRunId: 7a182c1f-69ff-4323-bc0a-5f5979b46399\n",
      "Link to Azure Machine Learning Portal: https://ml.azure.com/runs/7a182c1f-69ff-4323-bc0a-5f5979b46399?wsid=/subscriptions/ca0a8c40-b06a-4e4e-8434-63c03a1dee34/resourcegroups/MSFT-WEU-EAP_PROJECT02_AI-DEV-RG/workspaces/msft-weu-DEV-eap-proj02_ai-amls&tid=72f988bf-86f1-41af-91ab-2d7cd011db47\n",
      "{'runId': '7a182c1f-69ff-4323-bc0a-5f5979b46399', 'status': 'Completed', 'startTimeUtc': '2021-09-01T16:14:30.682845Z', 'endTimeUtc': '2021-09-01T16:27:56.388381Z', 'properties': {'azureml.runsource': 'azureml.PipelineRun', 'runSource': 'SDK', 'runType': 'SDK', 'azureml.parameters': '{\"esml_inference_model_version\":\"1\",\"esml_scoring_folder_date\":\"2021-01-01 10:35:01.243860\",\"esml_optional_unique_scoring_folder\":\"*\",\"esml_environment_dev_test_prod\":\"dev\"}'}, 'inputDatasets': [], 'outputDatasets': [], 'logFiles': {'logs/azureml/executionlogs.txt': 'https://sajxvzyuylcu5jc.blob.core.windows.net/azureml/ExperimentRun/dcid.7a182c1f-69ff-4323-bc0a-5f5979b46399/logs/azureml/executionlogs.txt?sv=2019-07-07&sr=b&sig=Z6WpdMcjL1SkNd%2FlsR4lo8esYKMmqhPR3wnxS4xG47Y%3D&st=2021-09-01T16%3A17%3A57Z&se=2021-09-02T00%3A27%3A57Z&sp=r', 'logs/azureml/stderrlogs.txt': 'https://sajxvzyuylcu5jc.blob.core.windows.net/azureml/ExperimentRun/dcid.7a182c1f-69ff-4323-bc0a-5f5979b46399/logs/azureml/stderrlogs.txt?sv=2019-07-07&sr=b&sig=ZK%2B6YqmUzbn6W9kM5uywJmocHlYpdhMUaHd8sLOjZMw%3D&st=2021-09-01T16%3A17%3A57Z&se=2021-09-02T00%3A27%3A57Z&sp=r', 'logs/azureml/stdoutlogs.txt': 'https://sajxvzyuylcu5jc.blob.core.windows.net/azureml/ExperimentRun/dcid.7a182c1f-69ff-4323-bc0a-5f5979b46399/logs/azureml/stdoutlogs.txt?sv=2019-07-07&sr=b&sig=9e2I5MRhwb5TObrWBWdxzsXQcvKNgh9qxb7J8%2FNfhbk%3D&st=2021-09-01T16%3A17%3A57Z&se=2021-09-02T00%3A27%3A57Z&sp=r'}, 'submittedBy': 'Joakim Åström'}\n",
      "\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'Finished'"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1b) When satisfied - `PUBLISH` pipeline (or rebuild and publish)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# REBUILD - if you haven't runned the above cell, uncomment below:\r\n",
    "#p_factory.create_dataset_scripts_from_template(overwrite_if_exists=False) # overwrite_if_exists=False is default\r\n",
    "#batch_pipeline = p_factory.create_batch_pipeline(esml_pipeline_types.IN_2_GOLD_SCORING) # Gets workspace, connects to lake, creates pipeline.\r\n",
    "#p.ws = p.get_workspace_from_config()\r\n",
    "\r\n",
    "# PUBLISH\r\n",
    "published_pipeline, endpoint = p_factory.publish_pipeline(batch_pipeline, \"_4\") # \"_4\" is optional    to create a NEW pipeline with 0 history, not ADD version to existing pipe & endpoint"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Did NOT overwrite script-files with template-files such as 'scoring_gold.py', since overwrite_if_exists=False\n",
      "Using GEN2 as Datastore\n",
      "found model via REMOTE FILTER: Experiment TAGS: model name and version\n",
      "Using a model specific cluster, per configuration in project specific settings, (the integer of 'model_number' is the base for the name)\n",
      "Note: OVERRIDING enterprise performance settings with project specifics. (to change, set flag in 'dev_test_prod_settings.json' -> override_enterprise_settings_with_model_specific=False)\n",
      "Found existing cluster prj02-m10-dev for project and environment, using it.\n",
      "Succeeded\n",
      "AmlCompute wait for completion finished\n",
      "\n",
      "Minimum number of nodes requested have been provisioned\n",
      "image_build_compute = prj02-m10-dev\n",
      "Created step IN 2 SILVER - ds01_titanic [0b0421cb][76b5cf4d-f19c-49b1-84ab-8871aef261db], (This step is eligible to reuse a previous run's output)\n",
      "Created step IN 2 SILVER - ds02_haircolor [2dc76aa5][091ad8d5-52f5-4b4e-86a6-3936a2e2401e], (This step is eligible to reuse a previous run's output)\n",
      "Created step IN 2 SILVER - ds03_housing [6414ca86][ff85cb27-2535-485b-b03e-1d55876fc830], (This step is eligible to reuse a previous run's output)\n",
      "Created step IN 2 SILVER - ds04_lightsaber [ae9f49ad][a3f144b9-3aec-4c06-bfc7-f7fe5b32c594], (This step is eligible to reuse a previous run's output)\n",
      "Created step SILVER MERGED 2 GOLD [1c961809][6c537e86-0672-4e2f-b55d-dfe0222a9b80], (This step is eligible to reuse a previous run's output)\n",
      "Created step SCORING GOLD [4faa04b8][ecec8f97-70bf-47ee-ace5-79d20a6aa609], (This step is eligible to reuse a previous run's output)\n",
      "pub_pipe.name 11_diabetes_model_reg_batch_scoring_pipe_EP_4\n",
      "pub_pipe.id e5ed3b30-89fd-4b70-a2a0-403451ea2228\n",
      "pub_pipe.name 11_diabetes_model_reg_batch_scoring_pipe_EP_1\n",
      "pub_pipe.id c21397ba-8e26-4582-b7eb-0cf9bee5cc0c\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2) `CONSUME` pipeline: HowTo"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2a) Consume from `Azure Data factory - BATCH_SCORE Pipeline activity`"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "print(\"2 parameters needed to be set, to call PIPELINE activity\") \r\n",
    "print(\"\")\r\n",
    "print(\"esml_inference_model_version=0 \")\r\n",
    "print(\" - 0 means use latest version, but you can pick whatever version in the DROPDOWN in Azure data factory you want\")\r\n",
    "print(\"esml_scoring_folder_date='2021-06-08 15:35:01.243860'\")\r\n",
    "print(\" - DateTime in UTC format. Example: For daily scoring 'datetime.datetime.now()'\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2 parameters needed to be set, to call PIPELINE activity\n",
      "\n",
      "esml_inference_model_version=0 \n",
      " - 0 means use latest version, but you can pick whatever version in the DROPDOWN in Azure data factory you want\n",
      "esml_scoring_folder_date='2021-06-08 15:35:01.243860'\n",
      " - DateTime in UTC format. Example: For daily scoring 'datetime.datetime.now()'\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "print(\"2) Fetch scored data: Below needed for Azure Data factory PIPELINE activity (Pipeline OR Endpoint. Choose the latter\") \r\n",
    "print (\"- Endpoint ID\")\r\n",
    "print(\"Endpoint ID:  {}\".format(endpoint.id))\r\n",
    "print(\"Endpoint Name:  {}\".format(endpoint.name))\r\n",
    "print(\"Experiment name:  {}\".format(p_factory.experiment_name))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2) Fetch scored data: Below needed for Azure Data factory PIPELINE activity (Pipeline OR Endpoint. Choose the latter\n",
      "- Endpoint ID\n",
      "Endpoint ID:  aa67ac38-da18-4b2d-973a-df87998aa1b6\n",
      "Endpoint Name:  10_titanic_model_clas_batch_scoring_pipe_EP_4\n",
      "Experiment name:  10_titanic_model_clas_batch_scoring_pipe\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2b) Consume from `Azure Data factory - WriteBack Pipeline activity`"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "from azureml.core.dataset import Dataset\r\n",
    "from azureml.core import Experiment\r\n",
    "from  azureml.pipeline.core import PipelineRun\r\n",
    "\r\n",
    "# 1st you need a \"Post scoring\" activity, to get metadata of \"scored_gold_path\" from \"last_gold_run.csv\"\r\n",
    "ds1 = Dataset.get_by_name(workspace = p.ws, name =  p.dataset_gold_scored_runinfo_name_azure)\r\n",
    "run_id = ds1.to_pandas_dataframe().iloc[0][\"pipeline_run_id\"] # ['pipeline_run_id', 'scored_gold_path', 'date_in_parameter', 'date_at_pipeline_run','model_version'])\r\n",
    "scored_gold_path = ds1.to_pandas_dataframe().iloc[0][\"scored_gold_path\"]\r\n",
    "\r\n",
    "print(\"Read this meta-dataset from ADF: {}/last_gold_run.csv\".format(p.path_inference_gold_scored_runinfo))\r\n",
    "print(\"- To get the column 'scored_gold_path' which points to the scored-data:\")\r\n",
    "print(\"{}*.parquet\".format(scored_gold_path))\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Read this meta-dataset from ADF: projects/project002/10_titanic_model_clas/inference/active/gold_scored_runinfo/last_gold_run.csv\n",
      "- To get the column 'scored_gold_path' which points to the scored-data:\n",
      "projects/project002/10_titanic_model_clas/inference/1/scored/dev/2021/01/01/7a182c1f-69ff-4323-bc0a-5f5979b46399/*.parquet\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2b) Consume from `from PYTHON`\r\n",
    "- Run a pipeline endpoint (`Python SDK` call)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "from azureml.pipeline.core import PipelineEndpoint\r\n",
    "pipeline_endpoint = PipelineEndpoint.get(workspace=p.ws, name=p_factory.name_batch_pipeline_endpoint)\r\n",
    "pipeline_run_sdk = pipeline_endpoint.submit(p_factory.experiment_name)\r\n",
    "pipeline_run_sdk.id"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "pipeline_run_sdk.status"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'Running'"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2c) Consume from `from PYTHON`\r\n",
    "- Run via REST call"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "from azureml.pipeline.core import PublishedPipeline,PipelineEndpoint,PipelineRun\r\n",
    "import requests\r\n",
    "from azureml.core.authentication import ServicePrincipalAuthentication # InteractiveLoginAuthentication, AzureCliAuthentication\r\n",
    "\r\n",
    "sp = p.get_authenticaion_header_sp()\r\n",
    "auth_header = sp.get_authentication_header()\r\n",
    "date_folder = str(p.date_scoring_folder)\r\n",
    "pipeline_endpoint = PipelineEndpoint.get(workspace=p.ws, name=p_factory.name_batch_pipeline_endpoint)\r\n",
    "\r\n",
    "response = requests.post(pipeline_endpoint.endpoint,\r\n",
    "                         headers=auth_header,\r\n",
    "                         json={\"ExperimentName\": p_factory.experiment_name,\r\n",
    "                               \"ParameterAssignments\": {\r\n",
    "                                     \"esml_inference_model_version\": p.inferenceModelVersion,\r\n",
    "                                     \"esml_scoring_folder_date\": date_folder\r\n",
    "                                     }\r\n",
    "                              })"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "try:\r\n",
    "    response.raise_for_status()\r\n",
    "except Exception:    \r\n",
    "    raise Exception(\"Received bad response from the endpoint: {}\\n\"\r\n",
    "                    \"Response Code: {}\\n\"\r\n",
    "                    \"Headers: {}\\n\"\r\n",
    "                    \"Content: {}\".format(rest_endpoint, response.status_code, response.headers, response.content))\r\n",
    "\r\n",
    "run_id = response.json().get('Id')\r\n",
    "print('Submitted pipeline run: ', run_id)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Submitted pipeline run:  ebfd7102-e8af-47ab-a255-97b9ce334e4e\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### View status from REST call, via SDK"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "from azureml.pipeline.core import PublishedPipeline,PipelineEndpoint,PipelineRun\r\n",
    "published_pipeline_run = PipelineRun(p.ws.experiments[p_factory.experiment_name], run_id)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "published_pipeline_run.status"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'Running'"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# `WHO is the caller, usually?` (Azure Data factory, Azure Devops)` - that sends PARAMETERS and WHY?` \r\n",
    "### Q: Why? \r\n",
    "- A: To use same DEV scoring pipeline, with either different data to be scored `daily scoring`, or `different model-version SAME day` to score with.\r\n",
    "- A: To have \"environment parameters (dev,test,prod) we can instatiate a ESMLProject what knows the lake, workspace, makes it easy to create 3 pipelines for dev,test,prod\r\n",
    "    - And data, if 1 LAKE or 3 LAKES (dev,test,prod), they all have data-folders \"dev,test,prod\"\r\n",
    "\r\n",
    "### Who gives input?\r\n",
    "- A) Azure Devops (CI/CD) will trigger TRAIN pipeline, that will end with creating this BATCH SCORING, with \r\n",
    "    - 2 parameters (`esml_environment, esml_inference_model_version`), to CREATE/UPDATE the BATCH pipeline with newly trained model\r\n",
    "    - 1 dummy (`esml_scoring_folder_date`) to test BATCH SCORING after creation.\r\n",
    "- B) Azure Datafactory (read from source, writes as .csv or .parquet to IN-folder), and will trigger BATCH SCORING with:\r\n",
    "    - 2 PIPELINE parameters (`esml_inference_model_version, esml_scoring_folder_date`), to read IN-DATA to be scored. Usually \"todays\" esml_scoring_folder_date\r\n",
    "    - Note: To solve \"many scorings same day\", a \"run.id\" folder is created before the actual data.parquet\r\n",
    "    - Note: `*esml_environment` is not really needed post creation - since we already created the pipleine in DEV, `locked and loaded`\r\n",
    "\r\n",
    "### Who needs `scored_data` and HOW to get it? META data:\r\n",
    "- `Azure Datafactory` can read meta data of `last scored GOLD`, to get datalake-path of SCORED_GOLD - can then \"`write back scored data`\" to source, or another `system`\r\n",
    "    - See next cells \"`Get previous RUN and PIPELINE via `ESML` metadata`\"\r\n",
    "- `Power BI` can read the meta-data to fetch `last scored GOLD` directly"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Get previous RUN and PIPELINE via `ESML` metadata\r\n",
    "- How to get path of `scored_gold_path` and how to see the actual `pipeline run`"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "from azureml.core.dataset import Dataset\r\n",
    "from azureml.core import Experiment\r\n",
    "from  azureml.pipeline.core import PipelineRun\r\n",
    "\r\n",
    "# Get \"Pipeline run\" info, for tghe most recent \"latest scored gold\"\r\n",
    "ds1 = Dataset.get_by_name(workspace = p.ws, name =  p.dataset_gold_scored_runinfo_name_azure)\r\n",
    "run_id = ds1.to_pandas_dataframe().iloc[0][\"pipeline_run_id\"] # ['pipeline_run_id', 'scored_gold_path', 'date_in_parameter', 'date_at_pipeline_run','model_version'])\r\n",
    "scored_gold_path = ds1.to_pandas_dataframe().iloc[0][\"scored_gold_path\"]\r\n",
    "\r\n",
    "print(\"dataset_gold_scored_runinfo, location: {}\".format)\r\n",
    "print(\"pipeline_run_id: {}\".format(run_id))\r\n",
    "print(\"scored_gold_path: '{}'\".format(scored_gold_path))\r\n",
    "\r\n",
    "experiment = Experiment(workspace=p.ws, name=p_factory.experiment_name)\r\n",
    "remote_run = PipelineRun(experiment=experiment, run_id=run_id)\r\n",
    "print(\"\\nFetched RUN object {}\".format(remote_run))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "pipeline_run_id: 0a7a6ad1-5493-4855-9cab-342ae7da29f0\n",
      "scored_gold_path: 'projects/project002/11_diabetes_model_reg/inference/1/scored/dev/2021/06/08/0a7a6ad1-5493-4855-9cab-342ae7da29f0/'\n",
      "\n",
      "Fetched RUN object Run(Experiment: 11_diabetes_model_reg_batch_scoring_pipe,\n",
      "Id: 0a7a6ad1-5493-4855-9cab-342ae7da29f0,\n",
      "Type: azureml.PipelineRun,\n",
      "Status: Completed)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# `What can you configure?` (parameters, step compute, custom code)\r\n",
    "## 1) Configure: Parameters\r\n",
    "- Pipeline parameters: scoring_date, model_version\r\n",
    "    - Why: To dynamically select different data & model to score with, with same pipeline/reuse.\r\n",
    "    - Who: Azure data factory can dynamically set these, and call AML pipline\r\n",
    "- Pipeline parameters (model specific): target_column_name\r\n",
    "    - Why: To merge datasets to GOLD.\r\n",
    "print(\"Model version (pipeline parameter): {}\".format(p_factory.batch_pipeline_parameters[0].default_value))\r\n",
    "print(\" - This default value is set from ESMLProject settings: {}\".format(p.inferenceModelVersion))\r\n",
    "print(\"Scoring datetime: {}\".format(p_factory.batch_pipeline_parameters[1].default_value))\r\n",
    "print(\" - This default value is set from ESMLProject settings: {}\".format(str(p.date_scoring_folder)))\r\n",
    "# Optional parameters to READ or SET\r\n",
    "#parameters[2].name: parameters[2].default_value, # esml_optional_unique_scoring_folder \r\n",
    "#parameters[3].name: parameters[3].default_value # par_esml_dev_test_prod\r\n",
    "## 2) Configure: Compute & Environment (via ESML config or inject your own)\r\n",
    "- `Different compute per step OR samee for all` [\"cpu\",\"gpu\", \"databricks\"], based on your ESML environment (dev,test,prod) compute settings, and Dataset properties.\r\n",
    " - A) `Different compute for all steps`\r\n",
    "\r\n",
    "        -  if(dataset.cpu_gpu_databricks == \"cpu\"):\r\n",
    "        -       compute, runconfig = self.init_cpu_environment()\r\n",
    "        -  elif(d.cpu_gpu_databricks == \"databricks\"):\r\n",
    "        -     compute, runconfig = self.init_databricks_environment()\r\n",
    "        -  elif(d.cpu_gpu_databricks == \"gpu\"):\r\n",
    "        -       compute, runconfig = self.init_gpu_environment()\r\n",
    "- B) `Same compute for all`: For the full pipeline, is the DEFAULT behaviour.\r\n",
    "\r\n",
    "        - def `create_batch_scoring_pipe(self, `same_compute_for_all=True`, `cpu_gpu_databricks=\"cpu\")`\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# DDL \"IN\" and \"WriteBack\" table: SQL Server\r\n",
    " - Tables to create for WriteBack demo"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "```sql\r\n",
    "-- 1) IN DATA to Lake, anonymized\r\n",
    "CREATE TABLE [dbo].[esml_diabetes]\r\n",
    "(\r\n",
    "    -- PersonId: Not needed for ML scoring. It is actually only noise for the Machine Learning brain. \r\n",
    "    PersonId INT IDENTITY(1,1) not null, -- But IF we want to reconnect scored RESULT to an individual, we need it.\r\n",
    "\tAGE FLOAT NOT NULL,\r\n",
    "\tSEX FLOAT NOT NULL,\r\n",
    "\tBMI FLOAT NOT NULL,\r\n",
    "\tBP FLOAT NOT NULL,\r\n",
    "\tS1 FLOAT NOT NULL,\r\n",
    "\tS2 FLOAT NOT NULL,\r\n",
    "\tS3 FLOAT NOT NULL,\r\n",
    "\tS4 FLOAT NOT NULL,\r\n",
    "\tS5 FLOAT NOT NULL,\r\n",
    "\tS6 FLOAT NOT NULL\r\n",
    ")\r\n",
    "\r\n",
    "-- 2) Scored data the PIPELINE WroteBack\r\n",
    "CREATE TABLE [dbo].[esml_personID_scoring]\r\n",
    "(\r\n",
    "    PersonId INT NOT NULL,\r\n",
    "    DiabetesMLScoring DECIMAL NULL,\r\n",
    "    scoring_time DATETIME NULL,\r\n",
    "    in_data_time DATETIME NULL,\r\n",
    "    ts DATETIME NOT NULL DEFAULT (GETDATE())\r\n",
    ")\r\n",
    "-- SELECT Count(*) as total_rows FROM [dbo].[esml_personID_scoring] -- 442 rows per RUN since \"UPSERT\" from Azure Datafactory on PersonID\r\n",
    "-- SELECT * FROM [dbo].[esml_personID_scoring]\r\n",
    "\r\n",
    "-- 3) VIEW Person connected to scoring: Risk of DIABETES\r\n",
    "\r\n",
    "--SELECT * FROM [dbo].[esml_diabetes] as a\r\n",
    "SELECT * FROM [dbo].[esml_person_info] as a\r\n",
    "LEFT JOIN [dbo].[esml_personID_scoring] as b\r\n",
    "ON a.PersonId = b.PersonId\r\n",
    "\r\n",
    "```"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3fec2c5a411dce07235ef28c8752b6cecf1f94423de7e7c24e62fc38b1bc47de"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.12 64-bit ('azure_automl': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}