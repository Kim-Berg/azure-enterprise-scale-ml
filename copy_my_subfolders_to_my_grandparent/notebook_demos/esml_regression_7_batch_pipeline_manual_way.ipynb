{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "######  NB! This,InteractiveLoginAuthentication, is only needed to run 1st time, then when ws_config is written, use later CELL in notebook, that just reads that file\n",
    "import repackage\n",
    "repackage.add(\"../azure-enterprise-scale-ml/esml/common/\")\n",
    "from azureml.core import Workspace\n",
    "from azureml.core.authentication import InteractiveLoginAuthentication\n",
    "from esml import ESMLDataset, ESMLProject\n",
    "\n",
    "p = ESMLProject()\n",
    "p.dev_test_prod=\"dev\"\n",
    "auth = InteractiveLoginAuthentication(tenant_id = p.tenant)\n",
    "ws, config_name = p.authenticate_workspace_and_write_config(auth)\n",
    "######  NB!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# `ESML - BATCH PIPELINE \"Classic Creation via notebook`\r\n",
    "Note: \"Classic Creation via notebook`: This can be done with 0% ESML SDK involved. \r\n",
    "- Note: This full notebook (15 pages of code, 17-32h of work), can be done via the ESML SDK with a 1-liner. See other notebook example for that.\r\n",
    "\r\n",
    "## WHAT\r\n",
    "- Get correct environment and Auzre ML workspace (dev,test,prod) via ESML\r\n",
    "- Get compute via ESML\r\n",
    "- Get AutoML environment and model.pickle via ESML\r\n",
    "- Create without ESML, Azure ML dependency only\r\n",
    "    - Steps: In -> Silver\r\n",
    "        - Get indata \"to score\" dynamically. Azuda data factory sends 2 parameters to control this. (model verision, scoring date)\r\n",
    "    - Step: Silver2Gold\r\n",
    "    - Step: ScoreGold\r\n",
    "- `Scoring & Writeback orchestration` Supports scenario:  `\"daily scoring & writeback\"\r\n",
    "    - This notebook gives Azure data factory the \"meta-data\" needed to call this Azure ML Pipline, and also know what scoring to write back\r\n",
    "\r\n",
    "## EFFORT avoided \"head ache\" for you : ) \r\n",
    "- To write & debug this notebook, took me ~100 pipeline runs a´ 5-10min = `20h work`.\r\n",
    "- Why? To support the  scenario `\"daily scoring & writeback\"`, keeping the scored data historically in date_time folders there was...\r\n",
    "    -  ...A lot of headache `integrating` Azure ML Pipeline with Azure Datafactory with dynamic dataset for ADLS GEN 2 (and lack of support for dynamic Dataset paths)\r\n",
    "        - Example: `A djungle of choices to \"pass data\"` between pipeline steps \r\n",
    "            - 4 alternatives was tested, including this: `PipelineData VS PipelineParameter + DataPath + DataPathComputeBinding VS Dataset + DatasetConsumptionConfig and OutputFileDatasetConfig`\r\n",
    "                - ....DataReference was not tested, since very legacy. But also PipelineData is sort of going away, hence avoided that - did not work for ADLS GEN 2 datasets anyway...\r\n",
    "        - Example `Lack of support to control path` for Dataset IN our OUT data \r\n",
    "            - ADLS GEN 2 does not support DataPath, which Azure Datafactory can set (for Blob storage)\r\n",
    "            - Azure Datafactory cannot read or write from Azure ML Datasets (which would be great, to use version-number to get a registered dataset)\r\n",
    "            - OutputFileDatasetConfig nly support for {run-id} and {output-name} for output, which Azure Datafactory cannot receive from activity...we sort of need \"date_folder\" concept\r\n",
    "        - `ESML Solution:`\r\n",
    "            - `IN Dynamic path`: An own \"template\" path, that support date_folder and model_version, besides {run-id}. This is set in runtime, from Azure data factory parameters `\"Scoring_Date\"`\r\n",
    "                - How: Set \"dummy\" paths, or a§ \"default\" dataset with a default path....reset the path during runtime.\r\n",
    "                - Cons': The path that is is set during pipline runtime, is not reflected in the Azure ML Studio UI, graphical representaiton, and we cannot \"update the path\" to be reflected (but works)\r\n",
    "                    - The path that is set when pipeline is CREATED/PUBLISHED, is `\"static\"` in the UI. \r\n",
    "                    - Example: If we have a `\"mydata/2020/01/01\"` when creating pipe, this will show even if we run it `\"mydata/2021/05/03` with PipelineParameters \"2021/05/03\"\r\n",
    "            - `OUT Dynamic path`: Write \"run-meta-data\" to a file in ADLS GEN 2, that Azure datafactory can read after run is done, able to \"WriteBack\" data from a specific \"date_folder\"\r\n",
    "                - `last_scored_gold.csv` is then read by Azure Datafactory post AML pipeline activity, to fetch the \"path\" of the `correct` scored gold..with `2021/05/03` in its path, not `2020-01-01`\r\n",
    "# TIP:\r\n",
    "You can use ESMLPipeline factory instead of running/managing a notebook like this.\r\n",
    "\r\n",
    "`ESMLPipeline factory will build the pipeline automatically`, all steps based on the dataset array in the `model_settings.json` and witht the `ESML Datamodel: Bronze->Silver-Gold` \r\n",
    "\r\n",
    "    - p_factory = ESMLPipelineFactory(p, \"Y\") \r\n",
    "    - batch_pipeline = p_factory.create_batch_scoring_pipe()"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import repackage\r\n",
    "repackage.add(\"../azure-enterprise-scale-ml/esml/common/\")\r\n",
    "from esml import ESMLProject\r\n",
    "\r\n",
    "p = ESMLProject() # Will search in ROOT for your copied SETTINGS folder '../../../settings', you should copy template settings from '../settings'\r\n",
    "p.ws = p.get_workspace_from_config() #2) Load DEV or TEST or PROD Azure ML Studio workspace\r\n",
    "p.inference_mode = True"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# `01_Get COMPUTE & ENVIRONMENT, Connect to LAKE`"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "import repackage\r\n",
    "repackage.add(\"../azure-enterprise-scale-ml/esml/common/\")\r\n",
    "from esml import ESMLProject\r\n",
    "\r\n",
    "param_esml_env = \"dev\"\r\n",
    "param_inference_model_version = \"4\"\r\n",
    "param_scoring_folder_date = \"2021-06-22 15:35:01.243860\" # will become both IN and GOLD path:  \r\n",
    "param_train_in_folder_date = \"2021-01-22 15:35:01.243860\" # \r\n",
    "optional_param_my_unique_scoring_folder = \"88483b56c3f8450a86f2eeac000bb834\"\r\n",
    "\r\n",
    "p = ESMLProject(param_esml_env,param_inference_model_version,param_scoring_folder_date,param_train_in_folder_date)\r\n",
    "p.ws = p.get_workspace_from_config()\r\n",
    "p.describe()"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "aml_compute = p.get_training_aml_compute(p.ws) # Get compute, for active environment, either use same as training, or create a separate INFERENCE compute\r\n",
    "datastore = p.connect_to_lake()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Note: OVERRIDING enterprise performance settings with project specifics. (to change, set flag in 'dev_test_prod_settings.json' -> override_enterprise_settings_with_model_specific=False)\n",
      "Using a model specific cluster, per configuration in project specific settings, (the integer of 'model_number' is the base for the name)\n",
      "Note: OVERRIDING enterprise performance settings with project specifics. (to change, set flag in 'dev_test_prod_settings.json' -> override_enterprise_settings_with_model_specific=False)\n",
      "Found existing cluster prj02-m11-dev for project and environment, using it.\n",
      "Succeeded\n",
      "AmlCompute wait for completion finished\n",
      "\n",
      "Minimum number of nodes requested have been provisioned\n",
      "image_build_compute = prj02-m11-dev\n",
      "Using GEN2 as Datastore\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 02 `Create a local ScriptFolder`\r\n",
    "- Later to be a Snapshot and below to the pipeline\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "import os\r\n",
    "# python scripts folder\r\n",
    "script_folder = './common/pipeline/scripts'\r\n",
    "os.makedirs(script_folder, exist_ok=True)\r\n",
    "print('Script is in {}.'.format(os.path.realpath(script_folder)))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Script is in c:\\Users\\jostrm\\OneDrive - Microsoft\\0_GIT\\2_My\\github\\azure-enterprise-scale-ml\\notebook_demos\\common\\pipeline\\scripts.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 03 `Download a model to use to score with`"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "#2a - Download from Azure\r\n",
    "inference_config_to_override_and_inject, model, best_run = p.get_active_model_inference_config() # 1) You can override this scoring_script - get a baseline, then modify...\r\n",
    "inference_env = best_run.get_environment() # Batch scoring Environment(), for ScriptRunConfig\r\n",
    "best_run.download_file(\r\n",
    "    \"outputs/model.pkl\", os.path.join(script_folder, \"model.pkl\") # Download model.pkl for SNAPSHOT\r\n",
    ")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loading AutoML config settings from: dev\n",
      "Loading AutoML config settings from: dev\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "WARNING:root:The version of the SDK does not match the version the model was trained on.\n",
      "WARNING:root:The consistency in the result may not be guaranteed.\n",
      "WARNING:root:Package:azureml-automl-core, training version:1.30.0, current version:1.26.0\n",
      "Package:azureml-automl-runtime, training version:1.30.0, current version:1.26.0\n",
      "Package:azureml-core, training version:1.30.0, current version:1.26.0\n",
      "Package:azureml-dataprep, training version:2.15.1, current version:2.13.2\n",
      "Package:azureml-dataprep-native, training version:33.0.0, current version:32.0.0\n",
      "Package:azureml-dataprep-rslex, training version:1.13.0, current version:1.11.2\n",
      "Package:azureml-dataset-runtime, training version:1.30.0, current version:1.26.0\n",
      "Package:azureml-defaults, training version:1.30.0, current version:1.26.0\n",
      "Package:azureml-interpret, training version:1.30.0, current version:1.26.0\n",
      "Package:azureml-pipeline-core, training version:1.30.0, current version:1.26.0\n",
      "Package:azureml-telemetry, training version:1.30.0, current version:1.26.0\n",
      "Package:azureml-train-automl-client, training version:1.30.0, current version:1.26.0\n",
      "Package:azureml-train-automl-runtime, training version:1.30.0, current version:1.26.0\n",
      "WARNING:root:Below packages were used for model training but missing in current environment:\n",
      "WARNING:root:Package:azureml-mlflow, training version:1.30.0\n",
      "WARNING:root:Please ensure the version of your local conda dependencies match the version on which your model was trained in order to properly retrieve your model.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 04 `Create parameters, and set defaults`"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Who gives input? parameters? and Why? \r\n",
    "### Q: Why? \r\n",
    "- A: To use same DEV scoring pipeline, with either different data to be scored `daily scoring`, or `different model-version SAME day` to score with.\r\n",
    "- A: To have \"environment parameters (dev,test,prod) we can instatiate a ESMLProject what knows the lake, workspace, makes it easy to create 3 pipelines for dev,test,prod\r\n",
    "    - And data, if 1 LAKE or 3 LAKES (dev,test,prod), they all have data-folders \"dev,test,prod\"\r\n",
    "\r\n",
    "### Who gives input? \r\n",
    "- A) Azure Devops (CI/CD) will trigger TRAIN pipeline, that will end with creating this BATCH SCORING, with \r\n",
    "    - 2 parameters (`esml_environment, esml_inference_model_version`), to CREATE/UPDATE the BATCH pipeline with newly trained model\r\n",
    "    - 1 dummy (`esml_scoring_folder_date`) to test BATCH SCORING after creation.\r\n",
    "- B) Azure Datafactory will trigger BATCH SCORING, with \r\n",
    "    - 2 PIPELINE parameters (`esml_inference_model_version, esml_scoring_folder_date`), to read IN-DATA to be scored. Usually \"todays\" esml_scoring_folder_date\r\n",
    "    - 1 Optional PIPELINE parameter: `esml_optional_unique_scoring_folder`. ESML Supports multiple data writes/scorings per day. But if not all \"guid-folders\" below date should be merged and scored, 1 folder can be passed as filter.\r\n",
    "    - Note: `*esml_environment` is not really needed post creation - since we already created the pipleine in DEV, `locked and loaded`\r\n",
    "    \r\n",
    "Note: Prerequisite: Before Azure ML pipleine is called, AZURE DATA FACTORY has written either a .csv or .parquet or .jpg in the IN folder, and passed the \"scoring_date\" and \"model_version\" "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "from azureml.pipeline.core import PipelineParameter\r\n",
    "\r\n",
    "# Must be a REAL dataset at Pipelime creation. even though \"dummy\"\r\n",
    "par_esml_model_version = PipelineParameter(name=\"esml_inference_model_version\", default_value=p.inferenceModelVersion) \r\n",
    "par_esml_scoring_date = PipelineParameter(name=\"esml_scoring_folder_date\", default_value=str(p.date_scoring_folder))\r\n",
    "par_esml_guid_folder = PipelineParameter(name=\"esml_optional_unique_scoring_folder\", default_value=\"*\") "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Below is just an easy way to use ESML not having to remember the Datalake and Azure ML dataset design\r\n",
    "- The cell also remove ALL dependancy to ESML SDK on further cells\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "from azureml.core.dataset import Dataset\r\n",
    "\r\n",
    "esml_environment = p.dev_test_prod # PIPELINE \"locked and loaded\", e.g. to a TEST Azure ML workspace, and TEST datalake\r\n",
    "ds01 = p.DatasetByName(\"ds01_diabetes\")\r\n",
    "ds01_in_name = ds01.AzureName_IN # 'M11_ds01_diabetes_inference_IN'\r\n",
    "ds01_silver_name = ds01.AzureName_Silver # 'M11_ds01_diabetes_inference_SILVER'\r\n",
    "ds01_silver_path = ds01.SilverPath\r\n",
    "ds01_template_path = ds01.InPathTemplate\r\n",
    "\r\n",
    "ds02 = p.DatasetByName(\"ds02_other\")\r\n",
    "ds02_in_name = ds02.AzureName_IN # 'M11_ds02_other_inference_IN'\r\n",
    "ds02_silver_name = ds02.AzureName_Silver # 'M11_ds02_other_inference_SILVER'\r\n",
    "ds02_silver_path = ds02.SilverPath\r\n",
    "\r\n",
    "gold_to_score_name = p.dataset_gold_to_score_name_azure # 'M11_GOLD_TO_SCORE'\r\n",
    "gold_to_score_path = p.GoldPathToScoreBatch\r\n",
    "target_column_name = \"Y\"\r\n",
    "\r\n",
    "dataset_to_score = p.GoldToScore # Dataset.get_by_name(workspace = p.ws, name = gold_to_score_name)\r\n",
    "\r\n",
    "gold_scored_name = p.dataset_gold_scored_name_azure\r\n",
    "to_score_folder_batch, scored_folder, date_folder = p.get_gold_scored_unique_path(p.date_scoring_folder)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 04 `Bronze2Gold Environment: RunConfig`\r\n",
    "You can have separate environments to each pipeline step. For ease we here use same for all steps.\r\n",
    "- We wil just use a curated here: AzureML-Tutorial (USE_CURATED_ENV)\r\n",
    "\r\n",
    "#### Example: \r\n",
    "- `Pipeline 1: Dataprep & Batch scoring`: For In->Bronze->Silver-GOLD_TO_SCORE we will also use pandas, scikit-learn and automl, pyarrow for the pipeline steps. \r\n",
    "    - We defining the runconfig for that.\r\n",
    "- `Pipeline 2: AutoML train pipeline`:  For a pipline with:  In->Bronze->Silver-GOLD_TO_TRAIN->Training we will also need automl.\r\n",
    "    - We don't need that here, since only scoring."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "from azureml.core import Environment\r\n",
    "from azureml.core.runconfig import DEFAULT_CPU_IMAGE\r\n",
    "from azureml.core.runconfig import DockerConfiguration\r\n",
    "\r\n",
    "from azureml.core.runconfig import RunConfiguration\r\n",
    "from azureml.core.conda_dependencies import CondaDependencies\r\n",
    "\r\n",
    "aml_run_config = RunConfiguration()\r\n",
    "aml_run_config.target = aml_compute # `compute_target` as defined in \"Azure Machine Learning compute\" section above\r\n",
    "\r\n",
    "USE_CURATED_ENV = True\r\n",
    "if USE_CURATED_ENV :\r\n",
    "    curated_environment = Environment.get(workspace=p.ws, name=\"AzureML-Tutorial\") # \"AzureML-AutoMLTutorial\" https://docs.microsoft.com/en-us/azure/machine-learning/resource-curated-environments\r\n",
    "    aml_run_config.environment = curated_environment\r\n",
    "else:\r\n",
    "    aml_run_config.environment.python.user_managed_dependencies = False\r\n",
    "    \r\n",
    "    # Add some packages relied on by data prep step\r\n",
    "    aml_run_config.environment.python.conda_dependencies = CondaDependencies.create(\r\n",
    "        conda_packages=['pandas==0.25.1','scikit-learn==0.22.1', 'numpy==1.18.5',''], \r\n",
    "        pip_packages=['azureml-defaults', 'azureml-dataprep[fuse,pandas]'], # azureml-sdk\r\n",
    "        pin_sdk_version=False)\r\n",
    "    \r\n",
    "docker_config = DockerConfiguration(use_docker=True)\r\n",
    "aml_run_config.docker = docker_config"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 05a Pipline step: `01_In2Silver`\r\n",
    "- Used both for TRAIN and INFERENCE\r\n",
    "- Depends on data exists in IN folder, by ADF pipeline System -> IN"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 05b scrip file for step: `01_In2Silver` "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "%%writefile $script_folder/ds01_bronze2silver.py\r\n",
    "from azureml.core import Run\r\n",
    "from azureml.core import Dataset\r\n",
    "import pandas as pd \r\n",
    "import argparse\r\n",
    "import datetime\r\n",
    "\r\n",
    "parser = argparse.ArgumentParser()\r\n",
    "# REGULAR PARAMETERS \r\n",
    "parser.add_argument('--par_esml_env', dest='par_esml_env',type=str, required=True)\r\n",
    "parser.add_argument('--esml_input_lake_template', dest='esml_input_lake_template', required=True)\r\n",
    "\r\n",
    "# PIPELINE PARAMETERS \r\n",
    "parser.add_argument('--par_esml_model_version', dest='par_esml_model_version',type=str, required=True)\r\n",
    "parser.add_argument('--par_esml_scoring_date', dest='par_esml_scoring_date',type=str, required=True)\r\n",
    "args = parser.parse_args()\r\n",
    "\r\n",
    "# CREATE IN FOLDER from parameters\r\n",
    "date_infolder = datetime.datetime.strptime(args.par_esml_scoring_date, '%Y-%m-%d %H:%M:%S.%f') \r\n",
    "esml_scoring_date_in = date_infolder.strftime('%Y/%m/%d') #  String to folder structure 2020/01/01\r\n",
    "esml_model_version = args.par_esml_model_version\r\n",
    "esml_env = args.par_esml_env\r\n",
    "print(\"Scoring date IN folder: {}\".format(esml_scoring_date_in))\r\n",
    "\r\n",
    "input_path = args.esml_input_lake_template.format(inference_model_version = esml_model_version, dev_test_prod = esml_env, scoring_folder_date=esml_scoring_date_in)\r\n",
    "input_path_csv = input_path + '*.csv'\r\n",
    "input_path_parquet = input_path + '*.parquet'\r\n",
    "print(\"Scoring full path, input_path: {}\".format(input_path_csv))\r\n",
    "\r\n",
    "run = Run.get_context()\r\n",
    "ws = run.experiment.workspace\r\n",
    "datastore = ws.get_default_datastore()\r\n",
    "M11_ds01_diabetes_inference_IN_csv = None\r\n",
    "\r\n",
    "try:\r\n",
    "    M11_ds01_diabetes_inference_IN_csv = Dataset.Tabular.from_delimited_files(path = [(datastore, input_path_csv)])\r\n",
    "except Exception as e:\r\n",
    "    print(\"Could not load .CSV files from IN dataset. Now trying .PARQUET instead:  {}\".format(input_path_parquet))\r\n",
    "    M11_ds01_diabetes_inference_IN_csv = Dataset.Tabular.from_delimited_files(path = [(datastore, input_path_parquet)])\r\n",
    "\r\n",
    "print(\"ESML Alt2 did ALSO work: Dataset.Tabular.from_delimited_files(path = [(datastore, input_path_csv)]) = {}\".format(M11_ds01_diabetes_inference_IN_csv.name))\r\n",
    "\r\n",
    "try:\r\n",
    "    # Test to SET input : ) \r\n",
    "    name_dummy_ds = M11_ds01_diabetes_inference_IN_csv.name\r\n",
    "    run.input_datasets[name_dummy_ds] = M11_ds01_diabetes_inference_IN_csv.as_named_input(name_dummy_ds) # Get Dataset\r\n",
    "    print(\"ESML set input_datasets did work : ) \")\r\n",
    "except Exception as e:\r\n",
    "    print(\"Error: ESML Alt3 next(iter(run.input_datasets)) -> run.input_datasets[dataset_name] , did not work {}\".format(e.message))\r\n",
    "\r\n",
    "################################### EDIT BELOW - feature engieering ########################\r\n",
    "\r\n",
    "# DROP Y...for DEMPO purpose. Real world has no Y set.\r\n",
    "target_column_name = \"Y\"\r\n",
    "df = M11_ds01_diabetes_inference_IN_csv.to_pandas_dataframe().drop(target_column_name, axis=1) # Simulate feature engineering...source system might not know column name for Y, and certainly not values\r\n",
    "df.at[0,'AGE'] = 0.099\r\n",
    "df.at[0,'BMI'] = 0.099\r\n",
    "df.at[0,'BP'] = 0.099\r\n",
    "\r\n",
    "\r\n",
    "################################### EDIT ABOVE - feature engieering ########################\r\n",
    "# Save as Dataset\r\n",
    "df.reset_index(inplace=True, drop=True)\r\n",
    "output_silver_dataset_key1 =  next(iter(run.output_datasets)) \r\n",
    "output_silver_dataset = run.output_datasets[output_silver_dataset_key1]\r\n",
    "\r\n",
    "if not (output_silver_dataset is None):\r\n",
    "    os.makedirs(output_silver_dataset, exist_ok=True)\r\n",
    "    print(\"%s created\" % output_silver_dataset)\r\n",
    "    path = output_silver_dataset + \"/silver.parquet\"\r\n",
    "    write_df = df.to_parquet(path,engine='pyarrow', index=False,use_deprecated_int96_timestamps=True,allow_truncated_timestamps=False)\r\n",
    "\r\n",
    "print(f\"Wrote prepped data to {output_silver_dataset}/silver.parquet\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Overwriting ./common/pipeline/scripts/ds01_bronze2silver.py\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "import os\r\n",
    "import datetime\r\n",
    "from azureml.pipeline.steps import PythonScriptStep\r\n",
    "from azureml.core.dataset import Dataset\r\n",
    "from azureml.data import OutputFileDatasetConfig"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 06a Pipline step: `02_In2Silver`\r\n",
    "- Used both for TRAIN and INFERENCE\r\n",
    "- Depends on data exists in IN folder, by ADF pipeline System -> IN"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "# IN DATASET: CREATE \"Template Dummy dataset, but with LATEST path\"\r\n",
    "date_infolder = datetime.datetime.strptime(par_esml_scoring_date.default_value, '%Y-%m-%d %H:%M:%S.%f')\r\n",
    "esml_scoring_date_in = date_infolder.strftime('%Y/%m/%d') #  String 2020/01/01\r\n",
    "input_path = ds01_template_path.format(inference_model_version = par_esml_model_version.default_value, dev_test_prod = esml_environment, scoring_folder_date=esml_scoring_date_in)\r\n",
    "input_path_csv = input_path + \"*.csv\"\r\n",
    "\r\n",
    "# Must be a real dataset, evb though \"dummy/template\". Or else: # DatasetValidationError.  Cannot load any data from the specified path. Make sure the path is accessible and contains data.\r\n",
    "M11_ds01_diabetes_inference_IN_csv = Dataset.Tabular.from_delimited_files(path = [(datastore, input_path_csv)], validate=False) \r\n",
    "ds_dataset_consumption_config = M11_ds01_diabetes_inference_IN_csv.as_named_input(ds01_in_name)\r\n",
    "\r\n",
    "# OUT DATASET: \r\n",
    "path = ds01_silver_path + \"{run-id}/\"\r\n",
    "stepdata10_ds01_diabetes_silver = (\r\n",
    "    OutputFileDatasetConfig(\r\n",
    "            name=ds01_silver_name,\r\n",
    "            destination=(datastore, path))\r\n",
    "            .as_upload(overwrite=True)\r\n",
    "            .read_parquet_files()\r\n",
    "            .register_on_complete(name=ds01_silver_name)\r\n",
    ")\r\n",
    "\r\n",
    "step_diabetes2silver = PythonScriptStep( \r\n",
    "    runconfig=aml_run_config,\r\n",
    "    script_name=\"ds01_bronze2silver.py\",\r\n",
    "    name=\"BRONZE to SILVER\",\r\n",
    "    arguments=[\"--esml_input_lake_template\", ds01.InPathTemplate,\r\n",
    "    \"--par_esml_env\",esml_environment, \"--par_esml_model_version\",par_esml_model_version,\"--par_esml_scoring_date\",par_esml_scoring_date], \r\n",
    "    inputs=[ds_dataset_consumption_config], # IN dataset\r\n",
    "    outputs=[stepdata10_ds01_diabetes_silver], # OUT dataset\r\n",
    "    source_directory=script_folder, \r\n",
    "    compute_target=aml_compute,\r\n",
    "    allow_reuse=False\r\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "source": [
    "esml_step_types.BRONZE_2_SILVER"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'BRONZE_2_SILVER'"
      ]
     },
     "metadata": {},
     "execution_count": 35
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 06b scrip file for step: `02_In2Silver`"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "%%writefile $script_folder/ds02_bronze2silver.py\r\n",
    "from azureml.core import Run\r\n",
    "from azureml.core import Dataset\r\n",
    "import pandas as pd \r\n",
    "import argparse\r\n",
    "import os\r\n",
    "import datetime\r\n",
    "\r\n",
    "# IN/OUT folder paths\r\n",
    "parser = argparse.ArgumentParser()\r\n",
    "parser.add_argument('--esml_input_lake_template', dest='esml_input_lake_template', required=True)\r\n",
    "parser.add_argument('--par_esml_env', dest='par_esml_env', required=True)\r\n",
    "parser.add_argument('--par_esml_model_version', dest='par_esml_model_version', required=True)\r\n",
    "parser.add_argument('--par_esml_scoring_date', dest='par_esml_scoring_date', required=True)\r\n",
    "args = parser.parse_args()\r\n",
    "\r\n",
    "# GENEREATE PATH to data\r\n",
    "date_infolder = datetime.datetime.strptime(args.par_esml_scoring_date, '%Y-%m-%d %H:%M:%S.%f')\r\n",
    "esml_scoring_date_in = date_infolder.strftime('%Y/%m/%d') #  String to folder structure 2020/01/01\r\n",
    "esml_model_version = args.par_esml_model_version\r\n",
    "esml_env = args.par_esml_env\r\n",
    "print(\"Scoring date IN folder: {}\".format(esml_scoring_date_in))\r\n",
    "\r\n",
    "input_path = args.esml_input_lake_template.format(inference_model_version = esml_model_version, dev_test_prod = esml_env, scoring_folder_date=esml_scoring_date_in)\r\n",
    "input_path_csv = input_path + '*.csv'\r\n",
    "input_path_parquet = input_path + '*.parquet'\r\n",
    "print(\"IN Dataset. INPUT full path: {}\".format(input_path_csv))\r\n",
    "\r\n",
    "# 1) GET INPUT - Get .CSV or .PARQUET Dataset\r\n",
    "run = Run.get_context()\r\n",
    "ws = run.experiment.workspace\r\n",
    "datastore = ws.get_default_datastore()\r\n",
    "\r\n",
    "try:\r\n",
    "    M11_ds02_diabetes_inference_IN_csv = Dataset.Tabular.from_delimited_files(path = [(datastore, input_path_csv)]) \r\n",
    "except Exception as e:\r\n",
    "    print(\"Could not load .CSV files from IN dataset. Now trying .PARQUET instead:  {}\".format(input_path_parquet))\r\n",
    "    M11_ds02_diabetes_inference_IN_csv = Dataset.Tabular.from_parquet_files(path = [(datastore, input_path_parquet)])\r\n",
    "\r\n",
    "# 2) UPDATE INPUT Dataset\r\n",
    "#name_ds = M11_ds02_diabetes_inference_IN_csv.name\r\n",
    "#run.input_datasets[name_ds] = M11_ds02_diabetes_inference_IN_csv.as_named_input(name_ds) # Get Dataset\r\n",
    "\r\n",
    "################################### 3) EDIT BELOW - feature engieering ########################\r\n",
    "\r\n",
    "target_column_name = \"Y\" # for DEMO purpose, to drop label data\r\n",
    "# DROP Y, for DEMO purpose\r\n",
    "df = M11_ds02_diabetes_inference_IN_csv.to_pandas_dataframe().drop(target_column_name, axis=1) # Simulate feature engineering...source system might not know column name for Y, and certainly not values\r\n",
    "\r\n",
    "\r\n",
    "################################### EDIT ABOVE - feature engieering ########################\r\n",
    "\r\n",
    "# Save as Dataset\r\n",
    "df.reset_index(inplace=True, drop=True)\r\n",
    "output_silver_dataset_key1 =  next(iter(run.output_datasets)) # Get 1st key in dictionary\r\n",
    "output_silver_dataset = run.output_datasets[output_silver_dataset_key1] # args.output_silver_dataset\r\n",
    "\r\n",
    "if not (output_silver_dataset is None):\r\n",
    "    os.makedirs(output_silver_dataset, exist_ok=True)\r\n",
    "    print(\"%s created\" % output_silver_dataset)\r\n",
    "    path = output_silver_dataset + \"/silver.parquet\"\r\n",
    "    write_df = df.to_parquet(path, engine='pyarrow', index=False,use_deprecated_int96_timestamps=True,allow_truncated_timestamps=False)\r\n",
    "\r\n",
    "print(f\"Wrote prepped data to {output_silver_dataset}/silver.parquet\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Overwriting ./common/pipeline/scripts/ds02_bronze2silver.py\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "par_esml_model_version.default_value"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "# CREATE \"Template Dummy dataset, but with LATEST path\"\r\n",
    "date_infolder = datetime.datetime.strptime(par_esml_scoring_date.default_value, '%Y-%m-%d %H:%M:%S.%f') # DateTime \r\n",
    "esml_scoring_date_in = date_infolder.strftime('%Y/%m/%d') #  String 2020/01/01\r\n",
    "input_path_ds02 = ds02.InPathTemplate.format(inference_model_version = par_esml_model_version.default_value, dev_test_prod = esml_environment, scoring_folder_date=esml_scoring_date_in)\r\n",
    "input_path_csv_ds02 = input_path_ds02 + \"*.csv\"\r\n",
    "\r\n",
    "# IN: \r\n",
    "M11_ds02_other_inference_IN_csv = Dataset.Tabular.from_delimited_files(path = [(datastore, input_path_csv_ds02)])\r\n",
    "# OUT:\r\n",
    "path_out02 = ds02_silver_path + \"{run-id}/\" # + \"{run-id}/{output-name}\"\r\n",
    "\r\n",
    "stepdata11_ds02_other_silver = (\r\n",
    "    OutputFileDatasetConfig(\r\n",
    "            name=ds02_silver_name,\r\n",
    "            destination=(datastore, path_out02)) # partition_format='/{PipelineUniqueRunId}/silver.parquet'\r\n",
    "            .as_mount()\r\n",
    "            .read_parquet_files()\r\n",
    "            .register_on_complete(name=ds02_silver_name)\r\n",
    ")\r\n",
    "\r\n",
    "step_other2silver = PythonScriptStep(\r\n",
    "    runconfig=aml_run_config,\r\n",
    "    script_name='ds02_bronze2silver.py',\r\n",
    "    name=\"BRONZE to SILVER\",\r\n",
    "    arguments=[\"--esml_input_lake_template\", ds02.InPathTemplate,\r\n",
    "    \"--par_esml_env\",esml_environment, \"--par_esml_model_version\",par_esml_model_version,\"--par_esml_scoring_date\",par_esml_scoring_date ],\r\n",
    "    inputs=[M11_ds02_other_inference_IN_csv.as_named_input(ds02_in_name)],\r\n",
    "    outputs=[stepdata11_ds02_other_silver],\r\n",
    "    source_directory=script_folder,\r\n",
    "    compute_target=aml_compute,\r\n",
    "    allow_reuse=False\r\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 07 Pipline step: `Merge Silvers 2 GOLD_TO_SCORE` \r\n",
    "- Used for INFERENCE. Merges all silver datasets to a GOLD_TO_SCORE dataset\r\n",
    "- Depends on data exists in SILVER datasets. Default takes latest version."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "p.path_gold_to_score_template()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'projects/project002/11_diabetes_model_reg/inference/{model_version}/gold/dev/'"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "p.path_gold_to_score_template().format(model_version = 0) # 0 = Latest"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'projects/project002/11_diabetes_model_reg/inference/0/gold/dev/'"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "p.path_gold_to_score_template(True,True)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'projects/project002/11_diabetes_model_reg/inference/{model_version}/gold/dev/{date_folder}/{id_folder}/'"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "# OUT:\r\n",
    "#gold_to_score_folder = gold_to_score_path+'{run-id}'\r\n",
    "\r\n",
    "path_gold_to_score_template_latest = p.path_gold_to_score_template()\r\n",
    "path_gold_to_score_template_pars = p.path_gold_to_score_template(True,True)\r\n",
    "gold_to_score_folder = path_gold_to_score_template_latest.format(model_version = 0) # 0 means \"latest\" par_esml_model_version.default_value\r\n",
    "\r\n",
    "gold_to_score = (\r\n",
    "    OutputFileDatasetConfig(name=gold_to_score_name,destination=(datastore,gold_to_score_folder))\r\n",
    "    .as_upload(overwrite=True) # as_mount() also works\r\n",
    "    .read_parquet_files()  # To promote File to Tabular Dataset. This, or .read_delimited_files()  will return/converts to an \"OutputTabularDatasetConfig\"\r\n",
    "    .register_on_complete(name=gold_to_score_name)\r\n",
    ")\r\n",
    "\r\n",
    "step_gold_merged = PythonScriptStep(\r\n",
    "    runconfig=aml_run_config,\r\n",
    "    script_name='silvers2gold.py',\r\n",
    "    name=\"SILVER's merged to GOLD_TO_SCORE\",\r\n",
    "    arguments=[\"--input_10_silver_name\",stepdata10_ds01_diabetes_silver.name,\"--input_11_silver_name\",stepdata11_ds02_other_silver.name,\"--output_20_merged_gold\",gold_to_score, \"--target_column_name\",target_column_name,\r\n",
    "    \"--par_esml_scoring_date\",par_esml_scoring_date, \"--par_esml_model_version\",par_esml_model_version,\"--esml_output_lake_template\",path_gold_to_score_template_pars,\r\n",
    "    \"--param1\",\"ParameterValueTest\"],\r\n",
    "    inputs=[stepdata10_ds01_diabetes_silver.as_input(stepdata10_ds01_diabetes_silver.name) , stepdata11_ds02_other_silver.as_input(stepdata11_ds02_other_silver.name)], # needed to be able to fetch dataset from 'run.input_datasets' array\r\n",
    "    outputs=[gold_to_score], # Do not use. Maybe it looks nice in UI graph? \r\n",
    "    source_directory=script_folder,\r\n",
    "    compute_target=aml_compute,\r\n",
    "    allow_reuse=False\r\n",
    ")\r\n",
    "\r\n",
    "print(\"step_gold_merged created.\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "step_gold_merged created.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 07 scrip file for step: `Silvers 2 GOLD_TO_SCORE` "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "%%writefile $script_folder/silvers2gold.py\r\n",
    "import argparse\r\n",
    "import os\r\n",
    "import datetime\r\n",
    "import numpy as np\r\n",
    "from azureml.core import Run\r\n",
    "from azureml.core import Dataset\r\n",
    "from azureml.data.dataset_factory import FileDatasetFactory\r\n",
    "\r\n",
    "parser = argparse.ArgumentParser(\"gold\")\r\n",
    "parser.add_argument('--input_10_silver_name', dest='input_10_silver_name', help=\"10,11,12,.. can be processed in parallell\", required=True)\r\n",
    "parser.add_argument('--input_11_silver_name', dest='input_11_silver_name',help=\"10,11,12,.. can be processed in parallell\", required=True)\r\n",
    "parser.add_argument('--target_column_name', dest='target_column_name',type=str, help=\"Target Label - column to add\", required=True)\r\n",
    "parser.add_argument('--output_20_merged_gold', dest='output_20_merged_gold',help='Path to write GOLD_TO_SCORE',required=True)\r\n",
    "\r\n",
    "parser.add_argument('--par_esml_scoring_date', dest='par_esml_scoring_date',help='Date_folder in lake  to score',required=True)\r\n",
    "parser.add_argument('--par_esml_model_version', dest='par_esml_model_version',help='Model version to score with 1,2,3',required=True)\r\n",
    "parser.add_argument('--esml_output_lake_template', dest='esml_output_lake_template',help='Template path with plae holders to write GOLD_TO_SCORE',required=True)\r\n",
    "\r\n",
    "\r\n",
    "# OPTIONAL - \r\n",
    "parser.add_argument('--param1', dest='param1',help='My test parameter', required=False)\r\n",
    "args = parser.parse_args()\r\n",
    "\r\n",
    "print(\"Argument (output merge data path): %s\" % args.output_20_merged_gold)\r\n",
    "print(\"Argument OutputFileDatasetConfig datatype: %s\" % type(args.output_20_merged_gold))\r\n",
    "\r\n",
    "print(\"Merge SILVERs data to GOLD\")\r\n",
    "run = Run.get_context()\r\n",
    "\r\n",
    "# INPUTS\r\n",
    "for indata in run.input_datasets:\r\n",
    "    print(\"input_datasets is not empty\")\r\n",
    "    print(str(type(indata)))\r\n",
    "    print(indata)\r\n",
    "\r\n",
    "ws = run.experiment.workspace\r\n",
    "datastore = ws.get_default_datastore()\r\n",
    "\r\n",
    "M11_ds01_diabetes_inference_IN_csv = run.input_datasets[args.input_10_silver_name] # Alt 1) Get via input_datastes array (just demo purpose, to use same way makes mor sense)\r\n",
    "df1 = M11_ds01_diabetes_inference_IN_csv.to_pandas_dataframe()\r\n",
    "\r\n",
    "M11_ds02_other_inference_SILVER = Dataset.get_by_name(workspace=ws, name=args.input_11_silver_name,  version='latest') # Alt 2) Get via workspace (just demo purpose, use this for all)\r\n",
    "df2 = M11_ds02_other_inference_SILVER.to_pandas_dataframe() \r\n",
    "\r\n",
    "################################### EDIT BELOW if NEEDED - end up with a datafram called 'combined_df'  ########################\r\n",
    "# Merge data, and sample 10%, simulate a \"filter\" for junk\r\n",
    "combined_df = df1.append(df2, ignore_index=True)\r\n",
    "combined_df.reset_index(inplace=True, drop=True)\r\n",
    "df_to_score = combined_df.sample(frac=0.1, replace=True, random_state=1)\r\n",
    "\r\n",
    "# Add LABEL column, to score\r\n",
    "df_to_score['Y'] = np.nan\r\n",
    "\r\n",
    "################################### EDIT ABOVE if NEEDED - end up with a datafram called 'combined_df' ########################\r\n",
    "\r\n",
    "output_to_score_gold_name =  next(iter(run.output_datasets)) # Get 1st key in dictionary\r\n",
    "output_to_score_gold = run.output_datasets[output_to_score_gold_name]\r\n",
    "\r\n",
    "\r\n",
    "# SET PATH to \"parameters\": model-version, date-folder\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "# 1) UPLOAD Files\r\n",
    "\r\n",
    "# 2) Create a DATASET from files. and register them\r\n",
    "#...the pipeline has \"no output\" then...but LAKE looks right.\r\n",
    "\r\n",
    "\r\n",
    "# Save GOLD as .parquet\r\n",
    "# OUTPUT - M11_GOLD_TO_SCORE\r\n",
    "if not (output_to_score_gold is None):\r\n",
    "    os.makedirs(output_to_score_gold, exist_ok=True)\r\n",
    "    print(\"%s created\" % output_to_score_gold)\r\n",
    "    path = output_to_score_gold + \"/gold_to_score.parquet\"\r\n",
    "    \r\n",
    "    # 1) Save/Overwrite \"latest\" data: 'projects/project002/11_diabetes_model_reg/inference/0/gold/dev/', for SCORE_GOLD step to read\r\n",
    "    write_df = combined_df.to_parquet(path,engine='pyarrow', index=False,use_deprecated_int96_timestamps=True,allow_truncated_timestamps=False)\r\n",
    "    \r\n",
    "    # Copy also to a \"date_folder\", for history in the lake\r\n",
    "    date_infolder = datetime.datetime.strptime(args.par_esml_scoring_date, '%Y-%m-%d %H:%M:%S.%f')\r\n",
    "    esml_scoring_date_out = date_infolder.strftime('%Y/%m/%d') #  Save scoring same date as IN-data 'in/2020/01/01' for 'gold_scored/2020/01/01'\r\n",
    "\r\n",
    "    # 2) Save historic data, with runtime parameters 'projects/project002/11_diabetes_model_reg/inference/{model_version}/gold/dev/{date_folder}/{id_folder}/'\r\n",
    "    print(\"run.run_id {}\".format(run.id))\r\n",
    "    \r\n",
    "    new_path = args.esml_output_lake_template.format(model_version = args.par_esml_model_version, date_folder = esml_scoring_date_out,id_folder= run.id)\r\n",
    "    FileDatasetFactory.upload_directory(src_dir=output_to_score_gold, target=(datastore, new_path), pattern=None, overwrite=True, show_progress=False)\r\n",
    "'''\r\n",
    "if not (args.output_20_merged_gold is None):\r\n",
    "    os.makedirs(args.output_20_merged_gold, exist_ok=True)\r\n",
    "    print(\"%s created\" % args.output_20_merged_gold)\r\n",
    "    path = args.output_20_merged_gold + \"/gold_to_score.parquet\"\r\n",
    "    write_df = combined_df.to_parquet(path,engine='pyarrow', index=False,use_deprecated_int96_timestamps=True,allow_truncated_timestamps=False)\r\n",
    "'''"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Overwriting ./common/pipeline/scripts/silvers2gold.py\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 08 PIPELINE STEP: `SCORE GOLD STEP`"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "p.path_gold_scored_template().format(model_version=par_esml_model_version.default_value)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'projects/project002/11_diabetes_model_reg/inference/1/scored/dev/'"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "# Get paths and names in whatever way you like for ESML AutoLake design. Here we use ESMLProject.properties and template methods\r\n",
    "latest_scored_folder = p.path_gold_scored_template().format(model_version=0) # 0= latest scored\r\n",
    "latest_gold_scored_path = latest_scored_folder + \"{run-id}\"\r\n",
    "scored_folder_template = p.path_gold_scored_template(True,True)\r\n",
    "\r\n",
    "# OUT:\r\n",
    "scored_gold = (\r\n",
    "    OutputFileDatasetConfig(name= p.dataset_gold_scored_name_azure,destination=(datastore,latest_gold_scored_path))\r\n",
    "    .as_upload(overwrite=True) # as_mount() also works\r\n",
    "    .read_parquet_files()  # To promote File to Tabular Dataset. This, or .read_delimited_files()  will return/converts to an \"OutputTabularDatasetConfig\"\r\n",
    "    .register_on_complete(name= p.dataset_gold_scored_name_azure)\r\n",
    ")\r\n",
    "\r\n",
    "last_gold_run = (\r\n",
    "    OutputFileDatasetConfig(name=p.dataset_gold_scored_runinfo_name_azure,destination=(datastore,p.path_inference_gold_scored_runinfo))\r\n",
    "    .as_upload(overwrite=True) # as_mount() also works\r\n",
    "    .read_delimited_files()  # To promote File to Tabular Dataset. This, or .read_delimited_files()  will return/converts to an \"OutputTabularDatasetConfig\"\r\n",
    "    .register_on_complete(name=p.dataset_gold_scored_runinfo_name_azure)\r\n",
    ")\r\n",
    "\r\n",
    "active_folder = (\r\n",
    "    OutputFileDatasetConfig(name=p.dataset_active_name_azure,destination=(datastore, p.path_inference_active))\r\n",
    "    .as_upload(overwrite=True) # as_mount() also works\r\n",
    "    #.read_delimited_files()  # To promote File to Tabular Dataset. This, or .read_delimited_files()  will return/converts to an \"OutputTabularDatasetConfig\"\r\n",
    "    #.register_on_complete(name=p.dataset_active_name_azure)\r\n",
    ")\r\n",
    "\r\n",
    "step_score_gold = PythonScriptStep(\r\n",
    "    runconfig=aml_run_config,\r\n",
    "    script_name='score_gold.py',\r\n",
    "    name=\"SCORING GOLD\",\r\n",
    "    arguments=[\"--input_gold_name\",gold_to_score.name, \"--par_esml_scoring_date\",par_esml_scoring_date, \"--par_esml_model_version\",par_esml_model_version\r\n",
    "    , \"--target_column_name\",target_column_name, \"--esml_output_lake_template\",scored_folder_template],\r\n",
    "    inputs=[gold_to_score.as_input(gold_to_score.name)], \r\n",
    "    outputs=[scored_gold,last_gold_run,active_folder], \r\n",
    "    source_directory=script_folder,\r\n",
    "    compute_target=aml_compute,\r\n",
    "    allow_reuse=False\r\n",
    ")\r\n",
    "\r\n",
    "print(\"SCORING step created.\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "SCORING step created.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "%%writefile $script_folder/score_gold.py\r\n",
    "\r\n",
    "import logging\r\n",
    "import os\r\n",
    "import pickle\r\n",
    "import numpy as np\r\n",
    "import pandas as pd\r\n",
    "import joblib\r\n",
    "import azureml.automl.core\r\n",
    "from azureml.automl.core.shared import logging_utilities, log_server\r\n",
    "from azureml.telemetry import INSTRUMENTATION_KEY\r\n",
    "import argparse\r\n",
    "from azureml.core import Dataset\r\n",
    "from azureml.core import Run\r\n",
    "from azureml.data.dataset_factory import FileDatasetFactory\r\n",
    "\r\n",
    "# Exists not in CURATED environment \"AzureML-Tutorial\", but only in \"AzureML-AutoML\"\r\n",
    "#from inference_schema.schema_decorators import input_schema, output_schema\r\n",
    "#from inference_schema.parameter_types.numpy_parameter_type import NumpyParameterType\r\n",
    "#from inference_schema.parameter_types.pandas_parameter_type import PandasParameterType\r\n",
    "\r\n",
    "input_sample = pd.DataFrame({\"AGE\": pd.Series([0.0], dtype=\"float64\"), \"SEX\": pd.Series([0.0], dtype=\"float64\"), \"BMI\": pd.Series([0.0], dtype=\"float64\"), \"BP\": pd.Series([0.0], dtype=\"float64\"), \"S1\": pd.Series([0.0], dtype=\"float64\"), \"S2\": pd.Series([0.0], dtype=\"float64\"), \"S3\": pd.Series([0.0], dtype=\"float64\"), \"S4\": pd.Series([0.0], dtype=\"float64\"), \"S5\": pd.Series([0.0], dtype=\"float64\"), \"S6\": pd.Series([0.0], dtype=\"float64\")})\r\n",
    "output_sample = np.array([0])\r\n",
    "try:\r\n",
    "    log_server.enable_telemetry(INSTRUMENTATION_KEY)\r\n",
    "    log_server.set_verbosity('INFO')\r\n",
    "    logger = logging.getLogger('azureml.automl.core.scoring_script')\r\n",
    "except:\r\n",
    "    pass\r\n",
    "\r\n",
    "def init():\r\n",
    "    global model, probabilities, gold_to_score_df, output_scored_gold,datastore,historic_path,last_gold_run,run_id,active_folder,date_in,model_version_in\r\n",
    "\r\n",
    "    parser = argparse.ArgumentParser(\"Scoring the model\")\r\n",
    "    parser.add_argument('--input_gold_name', dest=\"input_gold_name\", type=str, required=True)\r\n",
    "    parser.add_argument('--target_column_name', dest=\"target_column_name\", type=str, required=True)\r\n",
    "    parser.add_argument('--par_esml_scoring_date', dest=\"par_esml_scoring_date\", required=True)\r\n",
    "    parser.add_argument('--par_esml_model_version', dest=\"par_esml_model_version\", required=False)\r\n",
    "    parser.add_argument('--esml_output_lake_template', dest=\"esml_output_lake_template\", required=False)\r\n",
    "    \r\n",
    "    args = parser.parse_args()\r\n",
    "\r\n",
    "    try:\r\n",
    "        logger.info(\"Loading model from path: model.pkl\")\r\n",
    "        model = joblib.load(\"model.pkl\")\r\n",
    "\r\n",
    "        logger.info(\"Loading data to score\")\r\n",
    "        run = Run.get_context()\r\n",
    "        ws = run.experiment.workspace\r\n",
    "        datastore = ws.get_default_datastore()\r\n",
    "\r\n",
    "        gold_to_score = Dataset.get_by_name(workspace=ws, name=args.input_gold_name) #Latest should be correct version\r\n",
    "        gold_to_score_df = gold_to_score.to_pandas_dataframe().reset_index(drop=True)\r\n",
    "\r\n",
    "        if args.target_column_name in gold_to_score_df: # REMOVE TARGET column, if exists (for demo, sometimes TESTSET are used to score)\r\n",
    "            gold_to_score_df.drop(columns=[args.target_column_name], inplace=True)\r\n",
    "            print(\"Dropped target column: {}\".format(args.target_column_name))\r\n",
    "        \r\n",
    "        logger.info(\"Dataset GOLD to score, loaded successfully\")\r\n",
    "        logger.info(\"Loading SCORED_GOLD path, via OutputFileDatasetConfig to from output_datasets[next(iter(run.output_datasets)) ]\")\r\n",
    "\r\n",
    "        ### PATHS - save in 2 places\r\n",
    "        # 1) Save LATEST GOLD_SCORED - for Azure Data factory able to know the PATH, since static in time, able to \"WriteBack\" scored data\r\n",
    "        it = iter(run.output_datasets)\r\n",
    "        output_scored_gold_name =  next(it) # Get 1st key in dictionary\r\n",
    "        output_scored_gold = run.output_datasets[output_scored_gold_name]\r\n",
    "\r\n",
    "        # 2) Save META data:\"score_gold path, run_id, pipeline_id etc\r\n",
    "        last_gold_run_name =  next(it) # Save meta as dataset also, for visibility in Azure ML Studio\r\n",
    "        last_gold_run = run.output_datasets[last_gold_run_name]\r\n",
    "\r\n",
    "        active_folder_name =  next(it) # 3rd item. Good to show where files are LOCATED inlake aslo, for Azure Data factory\r\n",
    "        active_folder = run.output_datasets[active_folder_name]\r\n",
    "\r\n",
    "        # 2) Save HISTORIC scoring - with parameter in real time: DATE_FOLDER, MODEL_VERSIOM from calling applicatiom (Data factory)\r\n",
    "        date_in = args.par_esml_scoring_date\r\n",
    "        date_infolder = datetime.datetime.strptime(date_in, '%Y-%m-%d %H:%M:%S.%f') # UTC string to DateTime object\r\n",
    "        esml_scoring_date_out = date_infolder.strftime('%Y/%m/%d') #  Save scoring same date as IN data 'in/2020/01/01' and 'gold_scored/2020/01/01' (but can be different, depends on choice of meta)\r\n",
    "        run_id = run.id\r\n",
    "        model_version_in = args.par_esml_model_version\r\n",
    "        historic_path = args.esml_output_lake_template.format(model_version = model_version_in, date_folder = esml_scoring_date_out,id_folder= run_id)\r\n",
    "        # Example: 'projects/project002/11_diabetes_model_reg/inference/{model_version}/gold/[dev]/{date_folder}/{id_folder}/'  ...where [dev] is set during [CREATION] not {RUNTIME} parameter.\r\n",
    "\r\n",
    "    except Exception as e:\r\n",
    "        logging_utilities.log_traceback(e, logger)\r\n",
    "        raise\r\n",
    "\r\n",
    "\r\n",
    "#@input_schema('gold_to_score_df', PandasParameterType(input_sample))\r\n",
    "#@output_schema(NumpyParameterType(output_sample))\r\n",
    "def run(gold_to_score_df):\r\n",
    "    try:\r\n",
    "        logger.info(\"model.predict with gold_to_score\")\r\n",
    "        X_test_df = gold_to_score_df.reset_index(drop=True)\r\n",
    "        result = model.predict(X_test_df)\r\n",
    "\r\n",
    "        # Format result to a dataframe, join SCORING with its FEATURES\r\n",
    "        df_res  = pd.DataFrame(result, columns=['prediction'])\r\n",
    "        df_out = gold_to_score_df.join(df_res[['prediction']],how = 'left')\r\n",
    "        \r\n",
    "        logger.info(\"Saving prediction to GOLD_SCORED dataset\")\r\n",
    "        if not (output_scored_gold is None):\r\n",
    "            os.makedirs(output_scored_gold, exist_ok=True)\r\n",
    "            print(\"%s created\" % output_scored_gold)\r\n",
    "            path = output_scored_gold + \"/gold_scored.parquet\"\r\n",
    "            logger.info(\"Saving result as PARQUET at: {}\".format(path))\r\n",
    "            written_df = df_out.to_parquet(path,engine='pyarrow', index=False,use_deprecated_int96_timestamps=True,allow_truncated_timestamps=False)\r\n",
    "\r\n",
    "            # Alt 2) Note: This can also be done by Azure Data factory instead of this CPU cluster node. In a ADF Copy activity, post this pipeline, using 'latest_batch_score_run.csv file that has the 'historic_path'\r\n",
    "            print(\"Also save to HISTORIC path, output_scored_gold is {}\".format(output_scored_gold))\r\n",
    "            FileDatasetFactory.upload_directory(src_dir=output_scored_gold, target=(datastore, historic_path), pattern=None, overwrite=True, show_progress=False)\r\n",
    "\r\n",
    "        last_gold_run_filename = \"last_gold_run.csv\"\r\n",
    "        if not (last_gold_run is None):\r\n",
    "            os.makedirs(last_gold_run, exist_ok=True)\r\n",
    "            print(\"%s created\" % last_gold_run)\r\n",
    "            path_last_gold_run = last_gold_run + \"/\"+last_gold_run_filename\r\n",
    "            logger.info(\"Saving last_gold_run.csv at: {}\".format(path_last_gold_run))\r\n",
    "\r\n",
    "            # create the pandasd dataframe with meta, save to .csv for \"Azure datafactory WriteBack pipeline/step\" to use\r\n",
    "            date_now_str = str(datetime.datetime.now())\r\n",
    "\r\n",
    "            last_gold_run_data = [[run_id, historic_path,date_in,date_now_str,model_version_in]]\r\n",
    "            df2 = pd.DataFrame(last_gold_run_data, columns = ['pipeline_run_id', 'scored_gold_path', 'date_in_parameter', 'date_at_pipeline_run','model_version'])\r\n",
    "            written_df2 = df2.to_csv(path_last_gold_run, encoding='utf-8',index=False)\r\n",
    "\r\n",
    "            # Also save full FOLDER\r\n",
    "        if not (active_folder is None):\r\n",
    "            os.makedirs(active_folder, exist_ok=True)\r\n",
    "            path_active_folder = active_folder + \"/\"+last_gold_run_filename\r\n",
    "            written_df3 = df2.to_csv(path_active_folder, encoding='utf-8',index=False) # DUMMY 2nd Write needed?\r\n",
    "\r\n",
    "    except Exception as e:\r\n",
    "        logging_utilities.log_traceback(e, logger)\r\n",
    "        raise\r\n",
    "\r\n",
    "if __name__ == \"__main__\":\r\n",
    "    init()\r\n",
    "    run(gold_to_score_df)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Overwriting ./common/pipeline/scripts/score_gold.py\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "str(datetime.datetime.now())"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'2021-08-20 15:20:26.287695'"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 10 - `GOLD PIPELINE`: Put it together and RUN"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "par_esml_scoring_date.default_value"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'2021-06-22 15:35:01.243860'"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "par_esml_model_version.default_value"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "from azureml.pipeline.core import Pipeline\r\n",
    "from azureml.pipeline.core import StepSequence\r\n",
    "from azureml.widgets import RunDetails\r\n",
    "from azureml.core import Experiment\r\n",
    "\r\n",
    "pipeline = Pipeline(workspace = p.ws, steps= [step_diabetes2silver,step_other2silver, step_score_gold]) # This works also. Looks better, since not \"double up\" on dependancy arrows in Azure ML Studio\r\n",
    "print(\"Pipeline is built.\")\r\n",
    "\r\n",
    "# Create Experiment\r\n",
    "experiment_name = p.experiment_name + \"_batch_scoring_pipe\" if (p.experiment_name is not None) else \"11_diabetes_batch_scoring_pipe\"\r\n",
    "experiment = Experiment(p.ws,experiment_name)\r\n",
    "\r\n",
    "# PARAMETERS: scoring_date\r\n",
    "scoring_date = '2021-06-23 10:35:01.243860'\r\n",
    "par_dic = {par_esml_model_version.name: 1, par_esml_scoring_date.name: scoring_date}\r\n",
    "pipeline_run = experiment.submit(pipeline, regenerate_outputs=True,pipeline_parameters=par_dic,\r\n",
    "    tags={\r\n",
    "            \"training_run_id\": best_run.id,\r\n",
    "            \"run_algorithm\": best_run.properties[\"run_algorithm\"],\r\n",
    "            \"valid_score\": best_run.properties[\"score\"],\r\n",
    "            \"primary_metric\": best_run.properties[\"primary_metric\"],\r\n",
    "        })\r\n",
    "\r\n",
    "print(\"Pipeline submitted for execution.\")\r\n",
    "pipeline_run.log(\"run_algorithm\", best_run.properties[\"run_algorithm\"])\r\n",
    "\r\n",
    "# ERROR: Can't resolve parameters, top level graph doesn't contain parameter with name data_path_pipeline_param\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Pipeline is built.\n",
      "Created step BRONZE to SILVER [4af08a8f][18de75d6-237b-49cf-be0d-0c2d966bf156], (This step will run and generate new outputs)\n",
      "Created step BRONZE to SILVER [4b8d688b][8ea13e25-727d-4733-a4ec-63fcb110e729], (This step will run and generate new outputs)\n",
      "Created step SCORING GOLD [fa5d2bf3][8d6e178d-42bc-4eb9-8251-8843c495f0f1], (This step will run and generate new outputs)\n",
      "Created step SILVER's merged to GOLD_TO_SCORE [99abcffd][65e3c334-d57a-406c-a558-a94635161afb], (This step will run and generate new outputs)\n",
      "Submitted PipelineRun 06465ecb-2d1b-460d-b3b4-0e2bb268b22d\n",
      "Link to Azure Machine Learning Portal: https://ml.azure.com/runs/06465ecb-2d1b-460d-b3b4-0e2bb268b22d?wsid=/subscriptions/ca0a8c40-b06a-4e4e-8434-63c03a1dee34/resourcegroups/MSFT-WEU-EAP_PROJECT02_AI-DEV-RG/workspaces/msft-weu-DEV-eap-proj02_ai-amls&tid=72f988bf-86f1-41af-91ab-2d7cd011db47\n",
      "Pipeline submitted for execution.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "# Before we proceed we need to wait for the run to complete.\r\n",
    "pipeline_run.wait_for_completion(show_output=False)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "PipelineRunId: 06465ecb-2d1b-460d-b3b4-0e2bb268b22d\n",
      "Link to Azure Machine Learning Portal: https://ml.azure.com/runs/06465ecb-2d1b-460d-b3b4-0e2bb268b22d?wsid=/subscriptions/ca0a8c40-b06a-4e4e-8434-63c03a1dee34/resourcegroups/MSFT-WEU-EAP_PROJECT02_AI-DEV-RG/workspaces/msft-weu-DEV-eap-proj02_ai-amls&tid=72f988bf-86f1-41af-91ab-2d7cd011db47\n",
      "{'runId': '06465ecb-2d1b-460d-b3b4-0e2bb268b22d', 'status': 'Completed', 'startTimeUtc': '2021-08-20T13:20:32.995606Z', 'endTimeUtc': '2021-08-20T13:32:03.903101Z', 'properties': {'azureml.runsource': 'azureml.PipelineRun', 'runSource': 'SDK', 'runType': 'SDK', 'azureml.parameters': '{\"esml_inference_model_version\":\"1\",\"esml_scoring_folder_date\":\"2021-06-23 10:35:01.243860\"}'}, 'inputDatasets': [], 'outputDatasets': [], 'logFiles': {'logs/azureml/executionlogs.txt': 'https://sajxvzyuylcu5jc.blob.core.windows.net/azureml/ExperimentRun/dcid.06465ecb-2d1b-460d-b3b4-0e2bb268b22d/logs/azureml/executionlogs.txt?sv=2019-07-07&sr=b&sig=hek%2Fuwxy9LilRLCD3bocmBNSF9KeHw7jm1eZupYz0yY%3D&st=2021-08-20T14%3A25%3A40Z&se=2021-08-20T22%3A35%3A40Z&sp=r', 'logs/azureml/stderrlogs.txt': 'https://sajxvzyuylcu5jc.blob.core.windows.net/azureml/ExperimentRun/dcid.06465ecb-2d1b-460d-b3b4-0e2bb268b22d/logs/azureml/stderrlogs.txt?sv=2019-07-07&sr=b&sig=u0HfQgk45thppiXfE%2BJ7iikzFatDvPdyouJO4s8TXxc%3D&st=2021-08-20T14%3A25%3A40Z&se=2021-08-20T22%3A35%3A40Z&sp=r', 'logs/azureml/stdoutlogs.txt': 'https://sajxvzyuylcu5jc.blob.core.windows.net/azureml/ExperimentRun/dcid.06465ecb-2d1b-460d-b3b4-0e2bb268b22d/logs/azureml/stdoutlogs.txt?sv=2019-07-07&sr=b&sig=IZilZoVaVuAI3I1o%2B0ezL6Yqe7LFblZCuLY62kYLrH4%3D&st=2021-08-20T14%3A25%3A40Z&se=2021-08-20T22%3A35%3A40Z&sp=r'}, 'submittedBy': 'Joakim Åström'}\n",
      "\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'Finished'"
      ]
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Re-run pipline, with NEW datefolder (`23rd` instead of `22nd`), and new model version `1` instead of `4`\r\n",
    "- ADF ingest https://docs.microsoft.com/en-us/azure/machine-learning/how-to-data-ingest-adf"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "new_scoring_date = '2021-06-23 15:35:01.243860'\r\n",
    "par_dic = {par_esml_model_version.name: 1, par_esml_scoring_date.name: new_scoring_date}\r\n",
    "pipeline_run = experiment.submit(pipeline, regenerate_outputs=True,pipeline_parameters=par_dic)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Created step BRONZE to SILVER [03db1c1d][a0e1f4ce-f858-4a78-8072-3fadbfe75280], (This step will run and generate new outputs)\n",
      "Created step BRONZE to SILVER [7d68defe][0ac3a0de-a38d-4d66-9c23-bd8c98333244], (This step will run and generate new outputs)\n",
      "Created step SCORING GOLD [a5fa1d3c][f0d5204b-2214-4ab3-8732-d65e8f6d117a], (This step will run and generate new outputs)\n",
      "Created step SILVER's merged to GOLD_TO_SCORE [f61e6a46][ff273512-e681-4a10-9a38-80893b5a2a9e], (This step will run and generate new outputs)\n",
      "Submitted PipelineRun 7c893dfd-6a9f-4594-a4b0-99efa43c625d\n",
      "Link to Azure Machine Learning Portal: https://ml.azure.com/runs/7c893dfd-6a9f-4594-a4b0-99efa43c625d?wsid=/subscriptions/ca0a8c40-b06a-4e4e-8434-63c03a1dee34/resourcegroups/MSFT-WEU-EAP_PROJECT02_AI-DEV-RG/workspaces/msft-weu-DEV-eap-proj02_ai-amls&tid=72f988bf-86f1-41af-91ab-2d7cd011db47\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "# Before we proceed we need to wait for the run to complete.\r\n",
    "pipeline_run.wait_for_completion(show_output=False)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "PipelineRunId: 7c893dfd-6a9f-4594-a4b0-99efa43c625d\n",
      "Link to Azure Machine Learning Portal: https://ml.azure.com/runs/7c893dfd-6a9f-4594-a4b0-99efa43c625d?wsid=/subscriptions/ca0a8c40-b06a-4e4e-8434-63c03a1dee34/resourcegroups/MSFT-WEU-EAP_PROJECT02_AI-DEV-RG/workspaces/msft-weu-DEV-eap-proj02_ai-amls&tid=72f988bf-86f1-41af-91ab-2d7cd011db47\n",
      "{'runId': '7c893dfd-6a9f-4594-a4b0-99efa43c625d', 'status': 'Completed', 'startTimeUtc': '2021-08-20T10:47:55.562889Z', 'endTimeUtc': '2021-08-20T11:00:38.274031Z', 'properties': {'azureml.runsource': 'azureml.PipelineRun', 'runSource': 'SDK', 'runType': 'SDK', 'azureml.parameters': '{\"esml_inference_model_version\":\"1\",\"esml_scoring_folder_date\":\"2021-06-23 15:35:01.243860\"}'}, 'inputDatasets': [], 'outputDatasets': [], 'logFiles': {'logs/azureml/executionlogs.txt': 'https://sajxvzyuylcu5jc.blob.core.windows.net/azureml/ExperimentRun/dcid.7c893dfd-6a9f-4594-a4b0-99efa43c625d/logs/azureml/executionlogs.txt?sv=2019-07-07&sr=b&sig=%2B55PIhIEohZAdSkDD5UTu%2BzNdB8TPcCvwb%2FJ76hBnfo%3D&st=2021-08-20T11%3A38%3A27Z&se=2021-08-20T19%3A48%3A27Z&sp=r', 'logs/azureml/stderrlogs.txt': 'https://sajxvzyuylcu5jc.blob.core.windows.net/azureml/ExperimentRun/dcid.7c893dfd-6a9f-4594-a4b0-99efa43c625d/logs/azureml/stderrlogs.txt?sv=2019-07-07&sr=b&sig=jkWNMvIUfXSYaicoIstQD%2B%2B6ZWapJMTO8oVNgBtOo5o%3D&st=2021-08-20T11%3A38%3A27Z&se=2021-08-20T19%3A48%3A27Z&sp=r', 'logs/azureml/stdoutlogs.txt': 'https://sajxvzyuylcu5jc.blob.core.windows.net/azureml/ExperimentRun/dcid.7c893dfd-6a9f-4594-a4b0-99efa43c625d/logs/azureml/stdoutlogs.txt?sv=2019-07-07&sr=b&sig=%2FmO%2BsVmWcghQ6SNmA8J7SAzTBdVT13Egas%2Bl%2BukhPiU%3D&st=2021-08-20T11%3A38%3A27Z&se=2021-08-20T19%3A48%3A27Z&sp=r'}, 'submittedBy': 'Joakim Åström'}\n",
      "\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'Finished'"
      ]
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "# Get Steps\r\n",
    "for step in pipeline_run.get_steps():\r\n",
    "    print(\"Outputs of step \" + step.name)\r\n",
    "    \r\n",
    "    # Get a dictionary of StepRunOutputs with the output name as the key \r\n",
    "    output_dict = step.get_outputs()\r\n",
    "    \r\n",
    "    for name, output in output_dict.items():\r\n",
    "        \r\n",
    "        output_reference = output.get_port_data_reference() # Get output port data reference\r\n",
    "        print(\"\\tname: \" + name)\r\n",
    "        print(\"\\tdatastore: \" + output_reference.datastore_name)\r\n",
    "        print(\"\\tpath on datastore: \" + output_reference.path_on_datastore)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Outputs of step SCORING GOLD\n",
      "\tname: M11_GOLD_SCORED_RUNINFO\n",
      "\tdatastore: project002\n",
      "\tpath on datastore: last_gold_scored\n",
      "\tname: M11_GOLD_SCORED\n",
      "\tdatastore: project002\n",
      "\tpath on datastore: projects/project002/11_diabetes_model_reg/inference/{model_version}/scored/dev/0bda3163-b237-410a-8165-33ae9e87fa71\n",
      "\tname: M11_active_folder\n",
      "\tdatastore: project002\n",
      "\tpath on datastore: projects/project002/11_diabetes_model_reg/inference/active\n",
      "Outputs of step SILVER's merged to GOLD_TO_SCORE\n",
      "\tname: M11_GOLD_TO_SCORE\n",
      "\tdatastore: project002\n",
      "\tpath on datastore: projects/project002/11_diabetes_model_reg/inference/0/gold/dev/\n",
      "Outputs of step BRONZE to SILVER\n",
      "\tname: M11_ds01_diabetes_inference_SILVER\n",
      "\tdatastore: project002\n",
      "\tpath on datastore: projects/project002/11_diabetes_model_reg/inference/1/ds01_diabetes/out/silver/dev/835da2f9-9bef-48ea-a932-a731e18ed1c1/\n",
      "Outputs of step BRONZE to SILVER\n",
      "\tname: M11_ds02_other_inference_SILVER\n",
      "\tdatastore: project002\n",
      "\tpath on datastore: projects/project002/11_diabetes_model_reg/inference/1/ds02_other/out/silver/dev/ee55fe7a-9784-435b-b53b-f46f93d090a3/\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# functions to download output to local and fetch as dataframe\r\n",
    "def get_download_path(download_path, output_name):\r\n",
    "    output_folder = os.listdir(download_path + '/azureml')[0]\r\n",
    "    path =  download_path + '/azureml/' + output_folder + '/' + output_name\r\n",
    "    return path\r\n",
    "\r\n",
    "def fetch_df(current_step, output_name):\r\n",
    "    output_data = current_step.get_output_data(output_name)\r\n",
    "    print(type(output_data)) # <class 'azureml.pipeline.core.graph.PortDataReference'>\r\n",
    "    download_path = './outputs/' + output_name\r\n",
    "    output_data.download(download_path, overwrite=True) # # AttributeError: 'AzureDataLakeGen2Datastore' object has no attribute 'download'\r\n",
    "    df_path = get_download_path(download_path, output_name) + '/silver.parquet'\r\n",
    "    return pd.read_parquet(df_path)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### View SILVER diabetes, other"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "diabetes_silver_step = pipeline_run.find_step_run(step_diabetes2silver.name)[0]\r\n",
    "other_silver_step = pipeline_run.find_step_run(step_other2silver.name)[0]\r\n",
    "\r\n",
    "# <class 'azureml.pipeline.core.graph.PortDataReference'>\r\n",
    "# AttributeError: 'AzureDataLakeGen2Datastore' object has no attribute 'download'\r\n",
    "diabetes_df = fetch_df(diabetes_silver_step, stepdata10_ds01_diabetes_silver.name)\r\n",
    "other_df = fetch_df(other_silver_step, stepdata11_ds02_other_silver.name)\r\n",
    "\r\n",
    "display(diabetes_df.head(5))\r\n",
    "display(other_df.head(5))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### View `GOLD` dataset"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "merged_gold_step = pipeline_run.find_step_run(step_gold_merged.name)[0]\r\n",
    "gold_df = fetch_df(merged_gold_step, gold_to_score.name)\r\n",
    "display(gold_df.describe())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# `05b_BatchScoring pipeline`\r\n",
    "- Used both for INFERENCE. \r\n",
    "- Depends on `02_Bronze2Gold`"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import joblib\r\n",
    "from azureml.core.model import Model\r\n",
    "model_path = script_folder + \"/model.pkl\"\r\n",
    "model = joblib.load(model_path)\r\n",
    "\r\n",
    "# versus...\r\n",
    "#my_model_path = Model.get_model_path(p.model_folder_name) # Model 11_diabetes_model_reg not found in cache at azureml-models or in current working directory \r\n",
    "#model2 = joblib.load(my_model_path)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import pandas as pd\r\n",
    "df_res  = pd.DataFrame(result, columns=['prediction'])\r\n",
    "df_out = pd.merge(gold_to_score_df,df_res[['prediction']],how = 'left',left_index = True, right_index = True)\r\n",
    "df_out.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import pandas as pd\r\n",
    "df_res2  = pd.DataFrame(result, columns=['prediction'])\r\n",
    "df_out2 = gold_to_score_df.join(df_res2[['prediction']],how = 'left')\r\n",
    "df_out2.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3fec2c5a411dce07235ef28c8752b6cecf1f94423de7e7c24e62fc38b1bc47de"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.12 64-bit ('azure_automl': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}