{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import azureml.core\r\n",
    "print(\"SDK Version:\", azureml.core.VERSION)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1) ESML - TRAIN Classification, TITANIC model, and DEPLOY with predict_proba scoring"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "######  NB! This,InteractiveLoginAuthentication, is only needed to run 1st time, then when ws_config is written, use later CELL in notebook, that just reads that file\r\n",
    "import repackage\r\n",
    "repackage.add(\"../azure-enterprise-scale-ml/esml/common/\")\r\n",
    "from azureml.core import Workspace\r\n",
    "from azureml.core.authentication import InteractiveLoginAuthentication\r\n",
    "#sys.path.append(os.path.abspath(\"../azure-enterprise-scale-ml/esml/common/\"))  # NOQA: E402\r\n",
    "from esml import ESMLDataset, ESMLProject\r\n",
    "\r\n",
    "p = ESMLProject()\r\n",
    "#p.dev_test_prod=\"dev\"\r\n",
    "auth = InteractiveLoginAuthentication(tenant_id = p.tenant)\r\n",
    "ws, config_name = p.authenticate_workspace_and_write_config(auth)\r\n",
    "######  NB!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "######  NB! This,InteractiveLoginAuthentication, is only needed to run 1st time, then when ws_config is written, use later CELL in notebook, that just reads that file\r\n",
    "import repackage\r\n",
    "repackage.add(\"../azure-enterprise-scale-ml/esml/common/\")\r\n",
    "from azureml.core import Workspace\r\n",
    "from azureml.core.authentication import InteractiveLoginAuthentication\r\n",
    "#sys.path.append(os.path.abspath(\"../azure-enterprise-scale-ml/esml/common/\"))  # NOQA: E402\r\n",
    "from esml import ESMLDataset, ESMLProject\r\n",
    "\r\n",
    "p = ESMLProject()\r\n",
    "#p.dev_test_prod=\"dev\"\r\n",
    "auth = InteractiveLoginAuthentication(tenant_id = p.tenant)\r\n",
    "ws, config_name = p.authenticate_workspace_and_write_config(auth)\r\n",
    "######  NB!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import repackage\r\n",
    "repackage.add(\"../azure-enterprise-scale-ml/esml/common/\")\r\n",
    "from esml import ESMLProject\r\n",
    "import pandas as pd\r\n",
    "\r\n",
    "p = ESMLProject() # Will search in ROOT for your copied SETTINGS folder '../../../settings', you should copy template settings from '../settings'\r\n",
    "p.active_model = 10\r\n",
    "p.ws = p.get_workspace_from_config() #2) Load DEV or TEST or PROD Azure ML Studio workspace\r\n",
    "p.inference_mode = False\r\n",
    "\r\n",
    "unregister_all_datasets=False\r\n",
    "if(unregister_all_datasets):\r\n",
    "    p.unregister_all_datasets(p.ws) # For DEMO purpose\r\n",
    "\r\n",
    "p.describe()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def test_feature_engieering():\r\n",
    "    # R&D purpose: Try some data wrangling here...we will later incorporate this in an Azure ML Pipeline, as \"steps\"\r\n",
    "    esml_dataset = p.DatasetByName(\"ds01_titanic\") \r\n",
    "    df_bronze = esml_dataset.Bronze.to_pandas_dataframe()\r\n",
    "    df_bronze.columns = df_bronze.columns.str.replace(\"[/]\", \"_\") # Rename werid column names\r\n",
    "\r\n",
    "    df_silver = p.save_silver(esml_dataset,df_bronze) #Bronze -> Silver\r\n",
    "\r\n",
    "    esml_dataset2 = p.DatasetByName(\"ds02_haircolor\")\r\n",
    "    esml_dataset3 = p.DatasetByName(\"ds03_housing\")\r\n",
    "    esml_dataset4 = p.DatasetByName(\"ds04_lightsaber\")\r\n",
    "\r\n",
    "    p.save_silver(esml_dataset2,esml_dataset2.Bronze.to_pandas_dataframe()) #Bronze -> Silver\r\n",
    "    p.save_silver(esml_dataset3,esml_dataset3.Bronze.to_pandas_dataframe()) #Bronze -> Silver\r\n",
    "    p.save_silver(esml_dataset4,esml_dataset4.Bronze.to_pandas_dataframe()) #Bronze -> Silver\r\n",
    "\r\n",
    "    gold = p.save_gold(esml_dataset.Silver.to_pandas_dataframe())  #Silver -> Gold STEP\r\n",
    "    return gold"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "datastore = None\r\n",
    "gold = None\r\n",
    "gold_train = None\r\n",
    "try:\r\n",
    "    datastore = p.connect_to_lake()\r\n",
    "    gold = p.Gold\r\n",
    "    gold_train = p.GoldTrain\r\n",
    "    gold_train.name\r\n",
    "    print(\"Not 1st time. We have data mapped already. Now connected to LAKE\")\r\n",
    "except: # If 1st time....no Gold exists, nor any mapping\r\n",
    "    print(\"1st time. Lets init, map what data we have in LAKE, as Azure ML Datasets\")\r\n",
    "    datastore = p.init() # 3) Automapping from datalake to Azure ML datasets\r\n",
    "    gold = test_feature_engieering()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "p.Gold.to_pandas_dataframe().head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## SUMMARY - step 1\n",
    "- ESML has now `Automap` and `Autoregister` Azure ML Datasets as: `IN, SILVER, BRONZE, GOLD`\n",
    "- ESML has read configuration for correct environment (DEV, TEST, PROD). \n",
    "    - Both small customers, and large Enterprise customers often wants:  DEV, TEST, PROD in `diffferent Azure ML workspaces` (and different subscriptions)\n",
    "- User has done feature engineering, and saved GOLD `p.save_gold`"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3) Production purpose: \"once and only once\": Wrap code\r\n",
    "- 3 Callers: MLOps, AMLPipeline, and this notebook"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import repackage\r\n",
    "repackage.add(\"../2_A_aml_pipeline/4_inference/batch/M10/your_code/\")\r\n",
    "from your_custom_code import M01In2GoldProcessor\r\n",
    "\r\n",
    "#p.init()\r\n",
    "esml_dataset1 = p.DatasetByName(\"ds01_titanic\") # Get dataset 1\r\n",
    "df_bronze = esml_dataset1.Bronze.to_pandas_dataframe()\r\n",
    "silver1 = p.save_silver(esml_dataset1,df_bronze) #Bronze -> Silver\r\n",
    "\r\n",
    "esml_dataset2 = p.DatasetByName(\"ds02_haircolor\") # Get dataset 2\r\n",
    "df_bronze2 = esml_dataset2.Bronze.to_pandas_dataframe()\r\n",
    "silver2 = p.save_silver(esml_dataset2,df_bronze2) #Bronze -> Silver\r\n",
    "\r\n",
    "df1 = M01In2GoldProcessor().M01_ds01_process_in2silver(silver1.to_pandas_dataframe())  # You can then copy this statement in your pipeline-step \"in2silver_ds01...py\"\r\n",
    "df2 = M01In2GoldProcessor().M01_ds02_process_in2silver(silver2.to_pandas_dataframe())  # You can then copy this statement in your pipeline-step \"in2silver_ds02...py\"\r\n",
    "\r\n",
    "merged_gold = M01In2GoldProcessor().M01_merge_silvers(df1,df2) # # You can then copy this statement in your pipeline-step \"silver_merged_2_gold.py\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "merged_gold.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "label = p.active_model[\"label\"]\r\n",
    "train_6, validate_set_2, test_set_2 = p.split_gold_3(0.6,label) # Auto-register datasets in AZURE (GOLD_TRAIN | GOLD_VALIDATE | GOLD_TEST)   "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "label"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2) `ESML` Train model in `5 codelines`"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from esml import ESMLDataset, ESMLProject\r\n",
    "from baselayer_azure_ml import AutoMLFactory,azure_metric_regression,azure_metric_classification\r\n",
    "from azureml.train.automl import AutoMLConfig\r\n",
    "\r\n",
    "automl_performance_config = p.get_automl_performance_config() # 1)Get config, for active environment (dev,test or prod)\r\n",
    "aml_compute = p.get_training_aml_compute(p.ws) # 2)Get compute, for active environment\r\n",
    "\r\n",
    "automl_config = AutoMLConfig(task = 'classification', # 4) Override the ENV config, for model(that inhertits from enterprise DEV_TEST_PROD config baseline)\r\n",
    "                            primary_metric = azure_metric_classification.AUC, # # Note: Regression(MAPE) are not possible in AutoML\r\n",
    "                            compute_target = aml_compute,\r\n",
    "                            training_data = p.GoldTrain, # is 'train_6' pandas dataframe, but as an Azure ML Dataset\r\n",
    "                            experiment_exit_score = '0.922', # DEMO purpose (0.308 for diabetes regression, 0.6 for classification titanic)\r\n",
    "                            label_column_name = label,\r\n",
    "                            **automl_performance_config\r\n",
    "                        )\r\n",
    "via_pipeline = False # Consistent/same return values from both AutoML ALTERNATIVES (run or pipeline)\r\n",
    "best_run, fitted_model, experiment = AutoMLFactory(p).train_pipeline(automl_config) if via_pipeline else AutoMLFactory(p).train_as_run(automl_config)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- ESML has now fetched `configuration & train compute` for enterprise `environment (DEV,TEST or PROD)`\n",
    "- ESML has `autogenerated` a AutoML-experiment, optinally as `pipline`, in correct environment.\n",
    "- User has overridden some AutoML settings (`label, split percentage`, `target metric`), and use the `1-liner TRAIN` code snippet "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2b) ESML Scoring Drift/Concept Drift: Compare with `1-codeline`: Promote model or not? If better, then `Register model`\n",
    "- `IF` newly trained model in `current` environment (`DEV`, `TEST` or `PROD`) scores BETTER than existing model in `target` environment, then `new model` can be registered and promoted.\n",
    "- Q: Do we have `SCORING DRIFT / CONCEPT DRIFT?`\n",
    "- Q: Is a model trained on NEW data better? IS the one in production degraded? (not fit for the data it scores - real world changed, other CONCEPT)\n",
    "- A: - Lets check. Instead of `DataDrift`, lets look at `actual SCORING` on new data (and/or new code, feature engineering) - See if we should PROMOTE newly trained model..."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from baselayer_azure_ml import AutoMLFactory\r\n",
    "target_env = p.dev_test_prod #\"dev\", test, prod  = Target environment. Does Model A score better than Model B?\r\n",
    "print(\"SCORING DRIFT: If new model scores better in DEV (new data, or new code), we can promote this to TEST & PROD \\n\")\r\n",
    "\r\n",
    "promote, m1_name, r1_id, m2_name, r2_run_id = AutoMLFactory(p).compare_scoring_current_vs_new_model(target_env)  \r\n",
    "\r\n",
    "# To override (option 1) \"definition of BETTER\" - Adjust settings/model_settings.json to define & adjust \"what is best in my use case\"\r\n",
    "# To override (option 2) - just Register whatever model you see is the best. \"latest registered = best\"\r\n",
    "# - After, if you want to use AutoMLFactory(p).register_active_model(target_env), need to edit 'run_id' 'registered_model_version' in  /project_specifics/dev_test_prod/train/.../automl_active_model_dev.json\r\n",
    "\r\n",
    "# To override (option 3) - inject your own LAMBDA function \"your definition iof best\" \r\n",
    "#my_def_of_what_model_is_better = lambda sklearn_model_new,sklearn_model_current : (sklearn_model_new > sklearn_model_current)\r\n",
    "#promote, m1_name, r1_id, m2_name, r2_run_id = AutoMLFactory(p).compare_scoring_current_vs_new_model(target_env,my_def_of_what_model_is_better)\r\n",
    "\r\n",
    "print(\"New Model: {} in environment {}\".format(m1_name, p.dev_test_prod))\r\n",
    "print(\"Existing Model: {} in environment {}\".format(m2_name,target_env))\r\n",
    "\r\n",
    "if (promote and p.dev_test_prod == target_env): # Can register a model in same workspace (test->test) - need to retrain if going from dev->test (but copy from test->prod)\r\n",
    "    AutoMLFactory(p).register_active_model(target_env)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# TEST SET SCORING"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Test-set: Ensure we have a TEST_SET splitted"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "label = p.active_model[\"label\"]\r\n",
    "try:\r\n",
    "    p.GoldTest.name\r\n",
    "except: \r\n",
    "    p.connect_to_lake()\r\n",
    "    train_6, validate_set_2, test_set_2 = p.split_gold_3(0.6,label)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### NOW we can calcualate scoring on TEST_SET"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from baselayer_azure_ml import ESMLTestScoringFactory\r\n",
    "label = p.active_model[\"label\"]\r\n",
    "\r\n",
    "auc,accuracy,f1, precision,recall,matrix,matthews, plt = ESMLTestScoringFactory(p).get_test_scoring_7_classification(label)\r\n",
    "\r\n",
    "print(\"AUC:\")\r\n",
    "print(auc)\r\n",
    "print()\r\n",
    "print(\"Accuracy:\")\r\n",
    "print(accuracy)\r\n",
    "print()\r\n",
    "print(\"F1 Score:\")\r\n",
    "print(f1)\r\n",
    "print()\r\n",
    "print(\"Precision:\")\r\n",
    "print(precision)\r\n",
    "print()\r\n",
    "print(\"Recall:\")\r\n",
    "print(recall)\r\n",
    "print()\r\n",
    "print(\"Matchews correlation:\")\r\n",
    "print(matthews)\r\n",
    "print()\r\n",
    "print(\"Confusion Matrix:\")\r\n",
    "print(matrix)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3) ESML `Deploy model ONLINE` in `2 lines of code` (AKS) \n",
    "- Deploy \"offline\" MODEL from old `run` in environment To →  `DEV`, `TEST` or `PROD` environment\n",
    "- ESML saves `API_key in Azure keyvault automatically`\n",
    "- ESML auto-config solves 4 common 'errors/things': `correct compute name` and `valid replicas, valid agents, valid auto scaling`\n",
    "    - Tip: You can adjust the number of replicas, and different CPU/memory configuration, or using a different compute target."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "inference_config, model, best_run = p.get_active_model_inference_config(p.ws) #  Get compute power & lib-dependecies for DOCKER...for correct (Dev,Test or Prod) environment.\n",
    "service,api_uri, kv_aks_api_secret= p.deploy_automl_model_to_aks(model,inference_config,True) # Deploy: AKS dockerized with correct config (Dev,Test or Prod subscription & networking)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3b) DEPLOY TEST with ESML `2 lines of code`"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "X_test, y_test, tags = p.get_gold_validate_Xy() \n",
    "print(tags)\n",
    "caller_id = \"10965d9c-40ca-4e47-9723-5a608a32a0e4\"\n",
    "\n",
    "df = p.call_webservice(p.ws, X_test,caller_id) \n",
    "df.head()"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3b) ESML `DEPLOY - custom scoring` file - predict proba"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import os\r\n",
    "os.chdir(os.path.dirname(globals()['_dh'][0]))\r\n",
    "\r\n",
    "my_custom_script_instead = 'scoring_file_dev_M01_titanic.py'\r\n",
    "script_file_local = \"./settings/project_specific/model/dev_test_prod/train/automl/\"+my_custom_script_instead\r\n",
    "script_file_abs = os.path.abspath(script_file_local)\r\n",
    "\r\n",
    "inference_config_to_override_and_inject, model, best_run = p.get_active_model_inference_config(p.ws)\r\n",
    "inference_config_to_override_and_inject.entry_script = script_file_abs\r\n",
    "inference_config_to_override_and_inject.entry_script # Verify path"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# DEPLOY with custom InferenceConfig (custom scoring script)\r\n",
    "service,api_uri, kv_aks_api_secret= p.deploy_automl_model_to_aks(model,inference_config_to_override_and_inject, True) #2) (model,inference_config, overwrite_endpoint=True,deployment_config=None):"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# INFERENCE - Scenario \"Caller/Client\""
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1) Get MockData - Get some TEST-DATA via ESMLProject...the GoldTest"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import repackage\r\n",
    "repackage.add(\"../azure-enterprise-scale-ml/esml/common/\")\r\n",
    "from esml import ESMLDataset, ESMLProject\r\n",
    "\r\n",
    "p = ESMLProject() # Will search in ROOT for your copied SETTINGS folder '../../../settings', you should copy template settings from '../settings'\r\n",
    "p.inference_mode = False # We want \"TRAIN\" mode\r\n",
    "p.ws = p.get_workspace_from_config() #2) Load DEV or TEST or PROD Azure ML Studio workspace"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "label = p.active_model[\"label\"]\r\n",
    "to_score = None\r\n",
    "try:\r\n",
    "    X_test = p.GoldTest.to_pandas_dataframe()\r\n",
    "    to_score = X_test.drop([label], axis=1)\r\n",
    "    #print(to_score.head()) # gold_test_1 = Dataset.get_by_name(ws, name=p.dataset_gold_test_name_azure)\r\n",
    "except: \r\n",
    "    print (\"you need to have splitted GOLD dataset, GoldTest need to exist. Change next cell from MARKDOWN, to CODE, and run that. Try this again... \")\r\n",
    "# #X_test, y_test, tags = p.get_gold_validate_Xy() # Get the X_test data, ESML knows the SPLIT and LABEL already (due to training)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2) Call AKS Webservice in 3 ways (A,B,C)\n",
    "- A) Also let AKS save data to lake\n",
    "- B) Use the ESML helper method (fetched keys from vault AND joins result + features)\n",
    "- C) Simulate \"Rest only\" - No ESML dependency \n",
    "    - No ESML meaning: Fetch keys by your own from vault + join/format JSON yourself + save data yourself to lake)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Alt 1 - ESML.call_webservice, `get PANDAS joined` dataframe\n",
    "#### `Also saves to LAKE, automatically`"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#p.lakestore = p.set_lake_as_datastore(p.ws) # For AutoSave - this i NOT needed if p.init() is done...which usually is the case.\r\n",
    "p.call_webservice(p.ws, to_score,\"caller_id\").head() # (X_test, firstRowOnly=True,pandas_result=True, api_uri=None,api_key=\"auto from keyvault\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Alt 2 - use compute factory, control to `get JSON back` instead of PANDAS. \n",
    "#### `No saving to LAKE`"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "result, model_version_used = p.compute_factory.call_webservice(to_score,False,False) # (X_test, firstRowOnly=True,pandas_result=True, api_uri=None,api_key=\"auto from keyvault\")\r\n",
    "df_res = pd.read_json(result)\r\n",
    "to_score.join(df_res) # Need to join the FEATURES yourself, post webservice call (simulate no ESML dependancy in caller)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Alt 3 - Simulate client witn no ESML SDK, just using the \"scoring endpoint\". \n",
    "- Just JSON result (No ESML dependancy `get JSON back`)\n",
    "#### `No saving to LAKE` and `no JOIN` "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from baselayer_azure_ml import ComputeFactory\r\n",
    "import json\r\n",
    "keyvault = p.ws.get_default_keyvault() # Authentica to your Azure ML workspace (ws)\r\n",
    "api_uri = keyvault.get_secret(name='esml-dev-p02-m10-api') \r\n",
    "api_key = keyvault.get_secret(name='esml-dev-p02-m10-apisecret') # DEV + Titanic\r\n",
    "\r\n",
    "#api_uri = keyvault.get_secret(name='esml-test-p02-m10-api') # TEST + Titanic\r\n",
    "#api_key = keyvault.get_secret(name='esml-test-p02-m10-apisecret')\r\n",
    "\r\n",
    "result_json = ComputeFactory.call_webservice_static(to_score, api_uri,api_key,firstRowOnly=False) # Simulate \"REST call\" (no ESML dependancy, just a wrapper for a pytnon REST call)\r\n",
    "res_dict = json.loads(result_json.text) # json -> dictionary\r\n",
    "df_res = pd.read_json(res_dict) # dictionary -> pandas\r\n",
    "all_result = X_test.join(df_res) # features + result\r\n",
    "all_result"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# And....you need to save the data yourself to the lake, at this location\r\n",
    "to_score_folder, scored_folder, date_folder = p.get_gold_scored_unique_path()\r\n",
    "print(\"Save your data here, if you want to have ADF WriteBack function\")\r\n",
    "print()\r\n",
    "print(scored_folder)\r\n",
    "print()\r\n",
    "print(\"Note: Last folder, UUID folder, should represent a 'unique scoring' for a day, but can be injected. Example: if we want a customerGUID instead \")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# END"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# EXTRA - more about `AutoLake Paths`"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import repackage\r\n",
    "repackage.add(\"../azure-enterprise-scale-ml/esml/common/\")\r\n",
    "from esml import ESMLDataset, ESMLProject\r\n",
    "p = ESMLProject() \r\n",
    "p.ws = p.get_workspace_from_config() #2) Load DEV or TEST or PROD Azure ML Studio workspace"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "p.inference_mode = True # This flag will \"change the paths\", from TRAIN folder to INFERENCE folder-structure\r\n",
    "\r\n",
    "print(\"\")\r\n",
    "print(\"INFERENCE\")\r\n",
    "print(\"\")\r\n",
    "\r\n",
    "for d in p.Datasets:\r\n",
    "    print(d.Name)\r\n",
    "    print(\"IN\", d.InPath)\r\n",
    "    print(\"Bronze\", d.BronzePath)\r\n",
    "    print(\"Silver\", d.SilverPath)\r\n",
    "\r\n",
    "to_score_folder, scored_folder, date_folder = p.get_gold_scored_unique_path(p.date_scoring_folder)\r\n",
    "print(\"Gold\", to_score_folder, \"  ...uuid folder, is to be able to have multiple unique scorings, same datetime\")\r\n",
    "\r\n",
    "print(\"\")\r\n",
    "print(\"TRAIN\")\r\n",
    "print(\"\")\r\n",
    "\r\n",
    "p.inference_mode = False # This flag will \"change the paths\"\r\n",
    "\r\n",
    "for d in p.Datasets:\r\n",
    "    print(d.Name)\r\n",
    "    print(\"IN\", d.InPath)\r\n",
    "    print(\"Bronze\", d.BronzePath)\r\n",
    "    print(\"Silver\", d.SilverPath)\r\n",
    "\r\n",
    "print(\"Gold\", p.GoldPath)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.12 64-bit ('azure_automl': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  },
  "interpreter": {
   "hash": "3fec2c5a411dce07235ef28c8752b6cecf1f94423de7e7c24e62fc38b1bc47de"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}