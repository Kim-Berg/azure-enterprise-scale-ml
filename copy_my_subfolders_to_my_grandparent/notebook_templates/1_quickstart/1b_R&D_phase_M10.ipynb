{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# R&D phase: About this notebook - CLASSIFICATION\n",
    "- Purpose: TRAINS a model with Azure AutoML and with AZURE compute cluster and calculates test_set scoring, automatically compares if newly trained model is better.\n",
    "    - To iteratively try different ML-algorithms see what's best, change performance settings, train again.\n",
    "    - Also to try different apporoaches, classification or regression approach - which is better for the use case.\n",
    "\n",
    "- Q: `WHEN to move on form R&D phase to PRODUCTION phase notebook?`\n",
    "    - When you are happy with the MODEL (or if you have a big dataset that requires pipeline for training) - then go to the next notebook `2a_PRODUCTION_phase` to create PIPELINES: \n",
    "        - PRODUCTION PHASE & MLOps requires 1 `training pipeline`, and a `scoring pipeline` or `scoring online endpoint`, for inference \n",
    "- This notebook - Details:\n",
    "    - 1) Automaps data as Azure ML datasets. Based on your `lake_settings.json`\n",
    "    - 2) Splits the GOLD data into 3 buckets. \n",
    "        - NB this is done with local compute, not Azure, use \n",
    "             - Option 1: `2a_PRODUCTION_phase` training pipeline if data is too big for local RAM memory\n",
    "             - Option 2: Stay in this notebook & local split of data, but increase RAM memory of your/this Azure VM developer (DSVM) computer.\n",
    "             - Option 2: Stay in this notebook & local split of data, but reduce data size. Only use a sample .parquet (or .csv) file in the IN-folder.\n",
    "    - 3) Trains model\n",
    "    - 4) Registers model\n",
    "    - 5) Calculate test_set scoring\n",
    "    - 6) Deploys model - ONLINE endpoint to AKS\n",
    "    - 7) Inference: Smoke testing, using the ONLINE endpoint - get result back, saves the result to datalake also\n",
    "    - DONE.\n",
    "    \n",
    "- This notebook is called: `M10_v143_esml_classification_1_train_env_dev.ipynb` in the notebook_templates folder\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Login / Switch DEV_TEST_PROD environment (1-timer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"../azure-enterprise-scale-ml/esml/common/\")\n",
    "from azureml.core.authentication import InteractiveLoginAuthentication\n",
    "from esml import ESMLProject\n",
    "\n",
    "p = ESMLProject()\n",
    "p.dev_test_prod=\"dev\"\n",
    "\n",
    "print(p.tenant)\n",
    "print(p.workspace_name) # self.workspace_name,subscription_id = self.subscription_id,resource_group = self.resource_group\n",
    "print(p.subscription_id)\n",
    "print(p.resource_group)\n",
    "\n",
    "auth = InteractiveLoginAuthentication(tenant_id = p.tenant)\n",
    "#auth = InteractiveLoginAuthentication(force=True, tenant_id = p.tenant)\n",
    "ws, config_name = p.authenticate_workspace_and_write_config(auth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) ESML - TRAIN Classification, TITANIC model, and DEPLOY with predict_proba scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"../azure-enterprise-scale-ml/esml/common/\")\n",
    "from esml import ESMLProject\n",
    "import pandas as pd\n",
    "\n",
    "param_esml_env = \"dev\" \n",
    "param_inference_model_version = \"1\" # DATALAKE(my_model/inference/active) | settings/project_specific/active/active_scoring_in_folder.json\n",
    "param_scoring_folder_date = \"1000-01-01 00:00:01.243860\" # DATALAKE(my_model/inference/active) | settings/project_specific/active/active_scoring_in_folder.json\n",
    "param_train_in_folder_date = \"1000-01-01 00:00:01.243860\" # DATALAKE(my_model/train/active) | settings/project_specific/active/active_in_folder.json\n",
    "\n",
    "p = ESMLProject(param_esml_env,param_inference_model_version,param_scoring_folder_date,param_train_in_folder_date)\n",
    "#p = ESMLProject() # Alternatively use empty contructor, which takes parameters from settings\\project_specific\\model\\active\\active_in_folder.json\n",
    "\n",
    "p.active_model = 10\n",
    "p.inference_mode = False\n",
    "p.ws = p.get_workspace_from_config() #2) Load DEV or TEST or PROD Azure ML Studio workspace\n",
    "p.verbose_logging = False\n",
    "p.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unregister_all_datasets=False\n",
    "if(unregister_all_datasets):\n",
    "    p.unregister_all_datasets(p.ws) # For DEMO purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_feature_engieering():\n",
    "    # R&D purpose: Try some data wrangling here...we will later incorporate this in an Azure ML Pipeline, as \"steps\"\n",
    "    esml_dataset = p.DatasetByName(\"ds01_titanic\") \n",
    "    df_bronze = esml_dataset.Bronze.to_pandas_dataframe()\n",
    "    df_bronze.columns = df_bronze.columns.str.replace(\"[/]\", \"_\") # Rename werid column names\n",
    "\n",
    "    df_silver = p.save_silver(esml_dataset,df_bronze) #Bronze -> Silver\n",
    "\n",
    "    esml_dataset2 = p.DatasetByName(\"ds02_haircolor\")\n",
    "    esml_dataset3 = p.DatasetByName(\"ds03_housing\")\n",
    "    esml_dataset4 = p.DatasetByName(\"ds04_lightsaber\")\n",
    "\n",
    "    p.save_silver(esml_dataset2,esml_dataset2.Bronze.to_pandas_dataframe()) #Bronze -> Silver\n",
    "    p.save_silver(esml_dataset3,esml_dataset3.Bronze.to_pandas_dataframe()) #Bronze -> Silver\n",
    "    p.save_silver(esml_dataset4,esml_dataset4.Bronze.to_pandas_dataframe()) #Bronze -> Silver\n",
    "\n",
    "    gold = p.save_gold(esml_dataset.Silver.to_pandas_dataframe())  #Silver -> Gold STEP\n",
    "    return gold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datastore = None\n",
    "try:\n",
    "    datastore = p.connect_to_lake() # Connects to the correct ALDS GEN 2 storage account (DEV, TEST or PROD)\n",
    "    gold_train = p.GoldTrain\n",
    "    gold_train.name\n",
    "    print(\"Not 1st time. We have data mapped already...and splitted. Now connected to LAKE\")\n",
    "except: # If 1st time....no Gold exists, nor any mapping\n",
    "    print(\"1st time. Lets init, map what data we have in LAKE, as Azure ML Datasets\")\n",
    "    datastore = p.init() # 3) Automapping from datalake to Azure ML datasets\n",
    "    gold = test_feature_engieering()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.Gold.to_pandas_dataframe().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SUMMARY - step 1\n",
    "- ESML has now `Automap` and `Autoregister` Azure ML Datasets as: `IN, SILVER, BRONZE, GOLD`\n",
    "- ESML has read configuration for correct environment (DEV, TEST, PROD). \n",
    "    - Both small customers, and large Enterprise customers often wants:  DEV, TEST, PROD in `diffferent Azure ML workspaces` (and different subscriptions)\n",
    "- User has done feature engineering, and saved GOLD `p.save_gold`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"rows in GOLD {}\".format(p.Gold.to_pandas_dataframe().shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SPLIT option A) ESML default split logic, which you can override"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M10_GOLD_TRAIN, M10_GOLD_VALIDATE, M10_GOLD_TEST = p.split_gold_3(0.6,label=p.active_model[\"label\"],stratified=False) # Splits and Auto-registers as AZUREM ML Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SPLIT option B) Use YOUR split logic, override the default\n",
    "- You need to create your own class (ESMLSplitter is just an example class) such as MySplitter(IESMLSplitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"../azure-enterprise-scale-ml/\")\n",
    "\n",
    "from esmlrt.interfaces.iESMLSplitter import IESMLSplitter # Just for reference to see where the abstract class exists\n",
    "from esmlrt.runtime.ESMLSplitter import ESMLSplitter1 # Point at your own code/class here instead..that needst to implement the IESMLSplitter class\n",
    "\n",
    "my_IESMLSplitter = ESMLSplitter1()\n",
    "M10_GOLD_TRAIN, M10_GOLD_VALIDATE, M10_GOLD_TEST = p.split_gold_3(train_percentage=0.6,label=p.active_model[\"label\"],stratified=False,override_with_custom_iESMLSplitter=my_IESMLSplitter) # Splits and Auto-registers as AZUREM ML Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IN_2_GOLD\n",
    "- If just wanting to refine data to GOLD, for a Power BI report (No ML involved)\n",
    "- Scenario: You want to refine data from \"IN_2_GOLD\" with an easy way to READ/WRITE data (using the enterprise datalake via ESML AutoLake and ESML SDK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.GoldTrain.to_pandas_dataframe().head()  # Azure ML Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) `ESML` Train model in `5 codelines`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"We are in environment {}\".format(p.dev_test_prod))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets look at our AutoML performance settings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "automl_performance_config = p.get_automl_performance_config() # 1)Get config, for active environment (dev,test or prod)\n",
    "automl_performance_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets look at our label, and our machine learning task type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Label is: {}'.format(p.active_model[\"label\"]))\n",
    "print('ml_type / task is: {}'.format(p.active_model[\"ml_type\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets TRAIN with AutoML & Azure compute cluster (M11 demo takes ~ 10-15min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from esml import ESMLProject\n",
    "from baselayer_azure_ml import AutoMLFactory,azure_metric_regression,azure_metric_classification\n",
    "from azureml.train.automl import AutoMLConfig\n",
    "\n",
    "automl_performance_config = p.get_automl_performance_config() # 1)Get config, for active environment (dev,test or prod)\n",
    "aml_compute = p.get_training_aml_compute(p.ws) # 2)Get compute, for active environment\n",
    "\n",
    "automl_config = AutoMLConfig(task = p.active_model[\"ml_type\"], # 4) Override the ENV config, for model(that inhertits from enterprise DEV_TEST_PROD config baseline)\n",
    "                            primary_metric = p.active_model[\"ml_metric\"],# azure_metric_classification.AUC, #  Note: Regression[MAE, RMSE,R2,Spearman] Classification[AUC,Accuracy,Precision,Precision_avg,Recall]\n",
    "                            compute_target = aml_compute,\n",
    "                            training_data = p.GoldTrain, # is 'train_6' pandas dataframe, but as an Azure ML Dataset\n",
    "                            experiment_exit_score = p.active_model[\"ml_time_out_score\"], # DEMO purpose. remove experiment_exit_score if you want to have good accuracy (put a comment # on this row to remove it)\n",
    "                            label_column_name = p.active_model[\"label\"],\n",
    "                            **automl_performance_config\n",
    "                        )\n",
    "\n",
    "best_run, fitted_model, experiment = AutoMLFactory(p).train_as_run(automl_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Production purpose: \"once and only once\": Wrap code\n",
    "- 3 Callers: MLOps, AMLPipeline, and this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"../../2_A_aml_pipeline/4_inference/batch/M10/your_code/\")\n",
    "from your_custom_code import M01In2GoldProcessor\n",
    "\n",
    "#p.init()\n",
    "esml_dataset1 = p.DatasetByName(\"ds01_titanic\") # Get dataset 1\n",
    "df_bronze = esml_dataset1.Bronze.to_pandas_dataframe()\n",
    "silver1 = p.save_silver(esml_dataset1,df_bronze) #Bronze -> Silver\n",
    "\n",
    "esml_dataset2 = p.DatasetByName(\"ds02_haircolor\") # Get dataset 2\n",
    "df_bronze2 = esml_dataset2.Bronze.to_pandas_dataframe()\n",
    "silver2 = p.save_silver(esml_dataset2,df_bronze2) #Bronze -> Silver\n",
    "\n",
    "df1 = M01In2GoldProcessor().M01_ds01_process_in2silver(silver1.to_pandas_dataframe())  # You can then copy this statement in your pipeline-step \"in2silver_ds01...py\"\n",
    "df2 = M01In2GoldProcessor().M01_ds02_process_in2silver(silver2.to_pandas_dataframe())  # You can then copy this statement in your pipeline-step \"in2silver_ds02...py\"\n",
    "\n",
    "merged_gold = M01In2GoldProcessor().M01_merge_silvers(df1,df2) # # You can then copy this statement in your pipeline-step \"silver_merged_2_gold.py\"\n",
    "p.save_gold(merged_gold).to_pandas_dataframe().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2b) ESML Scoring Drift/Concept Drift: Compare with `1-codeline`: Promote model or not? If better, then `Register model`\n",
    "- `IF` newly trained model in `current` environment (`DEV`, `TEST` or `PROD`) scores BETTER than existing model in `target` environment, then `new model` can be registered and promoted.\n",
    "- Q: Do we have `SCORING DRIFT / CONCEPT DRIFT?`\n",
    "- Q: Is a model trained on NEW data better? IS the one in production degraded? (not fit for the data it scores - real world changed, other CONCEPT)\n",
    "- A: - Lets check. Instead of `DataDrift`, lets look at `actual SCORING` on new data (and/or new code, feature engineering) - See if we should PROMOTE newly trained model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"current AI Factory environment: '{}' - AML WS: '{}'\".format(p.dev_test_prod, p.ws.name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from baselayer_azure_ml_model import ESMLModelCompare\n",
    "\n",
    "current_env = p.dev_test_prod # dev\n",
    "target_env = \"dev\" # Does newly trained Model v3 in DEV, score better than Model v2 in TEST?\n",
    "print(\"promote model in DEV to TEST? (move to other Azure ML Studio Workspace)\")\n",
    "\n",
    "compare = ESMLModelCompare(p)\n",
    "promote,source_model_name,new_run_id,target_model_name, target_best_run_id,target_workspace,source_model = compare.compare_scoring_current_vs_new_model(target_env) # Compare DEV to TEST (or TEST to PROD)  (1min, 17sek VS 33sec)\n",
    "\n",
    "print(\"SCORING DRIFT: If new model scores better in DEV (new data, or new code), we can promote this to TEST & PROD \\n\")\n",
    "print(\"New Model: {} in environment {}\".format(target_model_name, p.dev_test_prod))\n",
    "print(\"Existing Model: {} in environment {}\".format(source_model_name,target_env))\n",
    "\n",
    "if (promote): # Can register=\"promote\" a model in same workspace (test->test), or also register in OTHER Azure ML workspace (test->prod)\n",
    "    if(p.dev_test_prod == target_env):\n",
    "        compare.register_active_model(target_env,source_model) # if SAME workspace this brings more \"metadata\" faster to the model registration\n",
    "    else:\n",
    "        compare.register_model_in_correct_ws(target_env) # if REMOTE target workspace we can get same metadata, BUT, just takes performancewise longer. More lookups to \"source Run\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST SET SCORING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test-set: Ensure we have a TEST_SET splitted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = p.active_model[\"label\"]\n",
    "try:\n",
    "    p.GoldTest.name\n",
    "except: \n",
    "    p.connect_to_lake() # p.init() + automap\n",
    "    train_6, validate_set_2, test_set_2 = p.split_gold_3(0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOW we can calcualate scoring on TEST_SET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from baselayer_azure_ml import ESMLTestScoringFactory\n",
    "\n",
    "auc,accuracy,f1, precision,recall,matrix,matthews,plt = ESMLTestScoringFactory(p).get_test_scoring_7_classification()\n",
    "\n",
    "print(\"AUC:\")\n",
    "print(auc)\n",
    "print()\n",
    "print(\"Accuracy:\")\n",
    "print(accuracy)\n",
    "print()\n",
    "print(\"F1 Score:\")\n",
    "print(f1)\n",
    "print()\n",
    "print(\"Precision:\")\n",
    "print(precision)\n",
    "print()\n",
    "print(\"Recall:\")\n",
    "print(recall)\n",
    "print()\n",
    "print(\"Mathews correlation:\")\n",
    "print(matthews)\n",
    "print()\n",
    "print(\"Confusion Matrix:\")\n",
    "print(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "labels = p.GoldTest.to_pandas_dataframe()[p.active_model[\"label\"]].unique()\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=matrix, display_labels=labels)\n",
    "p1 = disp.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When deploying classification - you might want to edit the auto-generated scoring-file\n",
    "- See below for example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(os.path.dirname(globals()['_dh'][0]))\n",
    "\n",
    "scoring_file = 'scoring_file_dev_M10_titanic.py'\n",
    "script_file_local = \"./settings/project_specific/model/dev_test_prod/train/automl/\"+scoring_file\n",
    "script_file_abs = os.path.abspath(script_file_local)\n",
    "\n",
    "inference_config_to_override_and_inject, model, best_run = p.get_active_model_inference_config(p.ws)\n",
    "inference_config_to_override_and_inject.entry_script = script_file_abs\n",
    "inference_config_to_override_and_inject.entry_script # Verify path"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.13 ('azure_automl_esml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "2f0f778a4495e689b30073b7a599e6a826d304e8985d11475b75364c935a444d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
