{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# R&D or PRODUCTION phase: This will generate a PIPELINE with 1-M Databricks steps\n",
    "- All Databricks steps, or mixed with Python steps\n",
    "- Purpose: Creates 1 of the 2 PIPELINES\n",
    "    - `2a) training pipeline:` TRAINS a model with Azure AutoML and with AZURE compute cluster and calculates test_set scoring, automatically compares if newly trained model is better. \n",
    "\n",
    "# Prerequisite - Databricks:\n",
    "- You need to have a `ESML Databricks template snaphshot folder` (M01,M11, ...) in your Databricks workspace\n",
    "- Run the notebooks in that folder first, interactively, so you know they work - then you come back to THIS notebooks, to generate an Azure ML pipeline, pointing at those Databricks notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO for you: CONFIGURATION\n",
    "- 1) Change `p.active_model=11` to correct model number `1` if your model has that number.\n",
    "    - See  [lake_settings.json](./settings/project_specific/model/lake_settings.json) to find YOUR model number.\n",
    "- 2) After you run the cell [2) AUTO-GENERATE code: a snapshot folder](#2_generate_snapshot_folder), you need to add YOUR feature engineering logic\n",
    "    -  This code you probably already have, from the R&D phase, in this CUSTOMIZE cell in the notebook: [1_R&D_phase_M10_M11.ipynb](./1_quickstart/1_R&D_phase_M10_M11.ipynb)\n",
    "        - 2a) You need to add this code to the `your_custom_code.py` after you have genereated the snapshot folder, for it to be reachable and uploaded at pipeline creation.\n",
    "            - Tip: You can CREATE A CLASS, and add static methods, e.g. `ds01_process_in2silver(dataframe1)`  in the `your_custom_code.py` \n",
    "        - 2b) You need to create the Databricks mapping here `ESMLPipelineStepMap.py`, define which steps are going to be handled by a Databricks notebooks and Databricks Sparl cluster.\n",
    "- 3) Now you have your code in the `your_custom_code.py`, then you need to reference that code from the auto-generated pipeline-steps files such as `in2silver_ds01_diabetes.py`\n",
    "    - Note: This snapshot folder will not exist, until you have run the first 2 cells in this notebook, or after this cell has run [2) AUTO-GENERATE code: a snapshot folder](#2_generate_snapshot_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Initiate ESMLPipelineFactory (Always run thic CELL below)\n",
    "- To attach ESML controlplane to your project\n",
    "- To point at `template-data` for the pipelinbe to know the schema of data\n",
    "- To init the ESMLPipelinefactory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using lake_settings.json with ESML version 1.4 - Models array support including LABEL\n",
      "\n",
      " ---- Q: WHICH files are generated as templates, for you to EDIT? ---- \n",
      "A: These files & locations:\n",
      "File to EDIT (step: IN_2_SILVER_1): ../../../01_pipelines/batch/M11/in2silver_ds01_diabetes.py\n",
      "File to EDIT (step: IN_2_SILVER_2): ../../../01_pipelines/batch/M11/in2silver_ds02_other.py\n",
      "File to EDIT (step: SILVER_MERGED_2_GOLD): ../../../01_pipelines/batch/M11/silver_merged_2_gold.py\n",
      "File to EDIT (step: SCORING_GOLD): ../../../01_pipelines/batch/M11/scoring_gold.py\n",
      "File to EDIT (step: TRAIN_SPLIT_AND_REGISTER): ../../../01_pipelines/batch/M11/train_split_and_register.py\n",
      "File to EDIT (step: TRAIN_MANUAL): ../../../01_pipelines/batch/M11/train_manual.py\n",
      "File to EDIT (step: TRAIN_AUTOML): ../../../01_pipelines/batch/M11/train_post_automl_step.py\n",
      "File to EDIT a lot (reference in step-scripts Custom code): ../../../01_pipelines/batch/M11/your_code/your_custom_code.py\n",
      "\n",
      " ---- WHAT model to SCORE with, & WHAT data 'date_folder'? ---- \n",
      "InferenceModelVersion (model version to score with): 0\n",
      "Date_scoring_folder (data to score) : 1000-01-01 10:35:01.243860\n",
      "ESML environment: dev\n",
      "Inference mode (self.batch_pipeline_parameters[4]): 0\n",
      "\n",
      " ---- ESML Datalake locations: ESML Datasets (IN-data) ---- \n",
      "Name (lake folder): ds01_diabetes and AzureName IN: M11_ds01_diabetes_train_IN\n",
      "IN projects/project002/11_diabetes_model_reg/train/ds01_diabetes/in/dev/1000/01/01/\n",
      "Bronze projects/project002/11_diabetes_model_reg/train/ds01_diabetes/out/bronze/dev/\n",
      "Silver projects/project002/11_diabetes_model_reg/train/ds01_diabetes/out/silver/dev/\n",
      "\n",
      "Name (lake folder): ds02_other and AzureName IN: M11_ds02_other_train_IN\n",
      "IN projects/project002/11_diabetes_model_reg/train/ds02_other/in/dev/1000/01/01/\n",
      "Bronze projects/project002/11_diabetes_model_reg/train/ds02_other/out/bronze/dev/\n",
      "Silver projects/project002/11_diabetes_model_reg/train/ds02_other/out/silver/dev/\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"../azure-enterprise-scale-ml/esml/common/\")\n",
    "from esml import ESMLProject\n",
    "from baselayer_azure_ml_pipeline import ESMLPipelineFactory, esml_pipeline_types\n",
    "\n",
    "p = ESMLProject() # Will search in ROOT for your copied SETTINGS folder '../settings/model/active/active_in_folder.json',\n",
    "p.inference_mode = False\n",
    "p.active_model = 11 # 10=titanic , 11=Diabetes\n",
    "p.ws = p.get_workspace_from_config()\n",
    "p_factory = ESMLPipelineFactory(p)\n",
    "\n",
    "training_datefolder = '1000-01-01 10:35:01.243860' # Will override active_in_folder.json\n",
    "p_factory.batch_pipeline_parameters[0].default_value = 0 # Will override active_in_folder.json.model.version = 0 meaning that ESML will find LATEST PROMOTED, and not use a specific Model.version. It will read data from .../inference/0/... folder\n",
    "p_factory.batch_pipeline_parameters[1].default_value = training_datefolder # overrides ESMLProject.date_scoring_folder.\n",
    "p_factory.describe()\n",
    "\n",
    "all_steps_databricks = False #Notebook parameter: Disabled CELL that includes all mapped steps as DatabricksSteps\n",
    "simple_mode_but_separate_compute = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The below cells for an IN_2_GOLD_TRAIN_AUTOML pipeline will:\n",
    "- 1) Generate code files\n",
    "- 2) Build pipeline, ESML autoguild this, and will upload the snapshot folder together with the Azure ML pipeline.\n",
    "- 3) Run the pipeline. Smoke testing, see that it works\n",
    "- 4) IF it works, Publish the pipeline, or else, edit the code files or configuration, retry step 2 and 3.\n",
    "- 5) Print the pipeline_id, that is essential to use from Azure Data factory "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) `AUTO-GENERATE code: a snapshot folder`\n",
    "<a id='2_generate_snapshot_folder'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did NOT overwrite script-files with template-files such as 'scoring_gold.py', since overwrite_if_exists=False\n"
     ]
    }
   ],
   "source": [
    "## Generate CODE - then edit it to get correct environments\n",
    "p_factory.create_dataset_scripts_from_template(overwrite_if_exists=False) # Do this once, then edit them manually. overwrite_if_exists=False is DEFAULT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative A) - Filter out, use some steps, a whitelist\n",
    "- Using a whitelist filter. 1-M of your mapped steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Azure ML Workspace:\n",
      "Attached Databricks db_compute_name:\n",
      "Compute target n1-p002-aml-91 already exists\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'step_name': 'in2silver_ds01_diabetes',\n",
       "  'code': '/Repos/jostrm@microsoft.com/esml-mlops/notebook_databricks/esml/dev/project/11_diabetes_model_reg/M11/10_in2silver_ds01_diabetes',\n",
       "  'compute_type': 'dbx',\n",
       "  'date_folder_or': None,\n",
       "  'dataset_folder_names': 'ds01_diabetes',\n",
       "  'dataset_filename_ending': '*.csv',\n",
       "  'compute_name': 'n1-p002-aml-91',\n",
       "  'cluster_id': '1201-224730-rn6t5arn'},\n",
       " {'step_name': 'in2silver_ds02_other',\n",
       "  'code': '/Repos/jostrm@microsoft.com/esml-mlops/notebook_databricks/esml/dev/project/11_diabetes_model_reg/M11/10_in2silver_ds02_other',\n",
       "  'compute_type': 'dbx',\n",
       "  'date_folder_or': None,\n",
       "  'dataset_folder_names': 'ds02_other',\n",
       "  'dataset_filename_ending': '*.csv',\n",
       "  'compute_name': 'n1-p002-aml-91',\n",
       "  'cluster_id': '1201-224730-rn6t5arn'},\n",
       " {'step_name': 'silver_merged_2_gold',\n",
       "  'code': '/Repos/jostrm@microsoft.com/esml-mlops/notebook_databricks/esml/dev/project/11_diabetes_model_reg/M11/20_merge_2_gold',\n",
       "  'compute_type': 'dbx',\n",
       "  'date_folder_or': None,\n",
       "  'dataset_folder_names': 'ds01_diabetes,ds02_other',\n",
       "  'dataset_filename_ending': '*.parquet',\n",
       "  'compute_name': 'n1-p002-aml-91',\n",
       "  'cluster_id': '1201-224730-rn6t5arn'}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"../azure-enterprise-scale-ml/\")\n",
    "from esmlrt.interfaces.iESMLPipelineStepMap import IESMLPipelineStepMap\n",
    "from esmlrt.interfaces.iESMLPipelineStepMap import esml_snapshot_step_names\n",
    "sys.path.insert(0, \"../01_pipelines/batch/M11/your_code/\")\n",
    "from ESMLPipelineStepMap import ESMLPipelineStepMap\n",
    "\n",
    "dataset_folder_names = p.active_model['dataset_folder_names']\n",
    "step1 = esml_snapshot_step_names.in2silver_template.format(dataset_folder_names[0])\n",
    "step2 = esml_snapshot_step_names.in2silver_template.format(dataset_folder_names[1])\n",
    "step3 = esml_snapshot_step_names.silver_merged_2_gold\n",
    "step4 = esml_snapshot_step_names.train_split_and_register\n",
    "step5 = esml_snapshot_step_names.train_manual\n",
    "\n",
    "step_filter_whitelist = [step1,step2,step3]\n",
    "\n",
    "my_map = ESMLPipelineStepMap(step_filter_whitelist) # TODO 4 YOU: You need to implement this class. See \"your_code\" folder \n",
    "#map = ESMLPipelineStepMap()\n",
    "p_factory.use_advanced_compute_settings(my_map)\n",
    "\n",
    "# Print the Mappning\n",
    "train_map = my_map.get_train_map(p.active_model['dataset_folder_names']) # Get the map\n",
    "train_map # prints it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative B) - Use all possible steps you defined in the ESMLPipleineStepMap\n",
    "- No whitelist filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This notebook CELL is disabled. Change 'all_steps_databricks=True' to enable it.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"../azure-enterprise-scale-ml/\")\n",
    "from esmlrt.interfaces.iESMLPipelineStepMap import IESMLPipelineStepMap\n",
    "sys.path.insert(0, \"../01_pipelines/batch/M11/your_code/\")\n",
    "from ESMLPipelineStepMap import ESMLPipelineStepMap\n",
    "\n",
    "if(all_steps_databricks):\n",
    "    mapping = ESMLPipelineStepMap() # TODO 4 YOU: You need to implement this class. See \"your_code\" folder \n",
    "    p_factory.use_advanced_compute_settings(mapping)\n",
    "\n",
    "    # Print the Mappning\n",
    "    train_map = mapping.get_train_map(p.active_model['dataset_folder_names']) # Get the map\n",
    "    train_map # prints it\n",
    "else:\n",
    "    print(\"This notebook CELL is disabled. Change 'all_steps_databricks=True' to enable it.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAINING (3a,4a,5a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3a) `BUILDS the TRANING pipeline`\n",
    "- esml_pipeline_types.IN_2_GOLD_TRAIN_AUTOML\n",
    "- Take note on the `esml_pipeline_types` below, of type: esml_pipeline_types.`IN_2_GOLD_TRAIN_AUTOML`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GEN2 as Datastore\n",
      "Environment ESML-AzureML-144-AutoML_126 exists\n",
      "Dataset: ds01_diabetes has advanced mapping - an Azure Databricks mapping\n",
      "Dataset: ds02_other has advanced mapping - an Azure Databricks mapping\n",
      "ESML advanced mode: with advanced compute mappings\n",
      " - Step: silver_merged_2_gold has advanced mapping - an Azure Databricks mapping\n",
      "Found attached Databricks compute cluster\n",
      "previous_step_is_databricks = 1\n",
      "create_gold_train_step: inference_mode=False\n",
      "par_date_utc: 1000-01-01 10:35:01.243860\n",
      "Created Databricks step in pipeline\n",
      "Initiated DEFAULT compute - for step train_split_and_register \n",
      "ESML will auto-create a compute...\n",
      "Note: OVERRIDING enterprise performance settings with project specifics. (to change, set flag in 'dev_test_prod_settings.json' -> override_enterprise_settings_with_model_specific=False)\n",
      "Using a model specific cluster, per configuration in project specific settings, (the integer of 'model_number' is the base for the name)\n",
      "Note: OVERRIDING enterprise performance settings with project specifics. (to change, set flag in 'dev_test_prod_settings.json' -> override_enterprise_settings_with_model_specific=False)\n",
      "Found existing cluster p02-m11weu-dev for project and environment, using it.\n",
      "Succeeded\n",
      "AmlCompute wait for completion finished\n",
      "\n",
      "Minimum number of nodes requested have been provisioned\n",
      "image_build_compute = p02-m11weu-dev\n",
      "create_gold_train_step: inference_mode=False\n",
      "ESML-train_path_out = projects/project002/11_diabetes_model_reg/train/gold/dev/Train/{run-id}/\n",
      "Adding train step, creating...\n",
      "Searching with Model list LAMBDA FILTER, on experiment_name in Model.tags called: 11_diabetes_model_reg . Meaning ESML checks for both Notebook run (AutoMLRun, Run) and PipelineRuns (AutoMLStep, PipelineRun)\n",
      "E.g. Even if Pipeline experiment is called '11_diabetes_model_reg_IN_2_GOLD_TRAIN' it will be included, since original model_folder_name in ESML is '11_diabetes_model_reg' as a notebook Run experiment name. Both is included in search\n",
      "Filter search, minutes: 6.061832610766093\n",
      "Current BEST model is: AutoMLd1093aff80 from Model registry with experiment_name-TAG 11_diabetes_model_reg, run_id-TAG dd4875e6-4f33-4f21-bd46-7819fca0c5dd  model_name-TAG AutoMLd1093aff80\n",
      "esml_time_updated: 2022-11-30 04:02:03.308372\n",
      "status_code : esml_promoted_2_dev\n",
      "model_name  : AutoMLd1093aff80\n",
      "trained_in_workspace   : msft-weu-DEV-eap-proj02_ai-amls\n",
      "current worksdpace p.ws  : msft-weu-DEV-eap-proj02_ai-amls\n",
      "train_folder_template_with_date_id: projects/project002/11_diabetes_model_reg/train/gold/dev/{id_folder}/\n",
      "Fetching all ESML environments\n",
      "Environments fetched\n",
      "PythonScriptStep TRAIN created\n"
     ]
    }
   ],
   "source": [
    "## BUILD (takes ~6-12minutes)\n",
    "if(simple_mode_but_separate_compute):\n",
    "    p_factory.use_advanced_compute_settings(None)\n",
    "    batch_pipeline = p_factory.create_batch_pipeline(esml_pipeline_types.IN_2_GOLD_TRAIN_MANUAL, same_compute_for_all=False, aml_compute=None, allow_reuse=True)\n",
    "else:\n",
    "    batch_pipeline = p_factory.create_batch_pipeline(esml_pipeline_types.IN_2_GOLD_TRAIN_MANUAL)\n",
    "    #batch_pipeline = p_factory.create_batch_pipeline(esml_pipeline_types.IN_2_GOLD_TRAIN_MANUAL, same_compute_for_all=True, aml_compute=None, allow_reuse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4a) `EXECUTES the pipeline`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NB! Run in v1 legacy mode\n",
    "- You need to have your Azure Machine Learning workspace set to `v1_legacy_mode=True`\n",
    "- HOW do I know if I run v1 or v2? \n",
    "  - If you see this error message in `executionlogs.txt in Azure machine learning studio Output+logs tab on pipeline rune`, containing the word in path `backendV2` when executing pipeline (cell below this), it is not in v1 legacy mode:\n",
    "     - <i>Failed to start the job for runid: 33ff1e3a-1ca7-4de0-bcee-b851cd2bb89d because of exception_type: ServiceInvocationException, error: Failure in StartSnapshotRun while calling service Execution; HttpMethod: POST; Response StatusCode: BadRequest; Exception type: Microsoft.RelInfra.Extensions.HttpRequestDetailException|-Microsoft.RelInfra.Common.Exceptions.ErrorResponseException, stack trace:    at Microsoft.Aether.EsCloud.Common.Client.ExecutionServiceClient.StartSnapshotRunAsync(String jobId, RunDefinition runDefinition, String runId, WorkspaceIdentity workspaceIdentity, String experimentName, CreatedBy createdBy) in D:\\a\\_work\\1\\s\\src\\aether\\platform\\\\`backendV2`\\\\Clouds\\ESCloud\\ESCloudCommon\\Client\\ExecutionServiceClient.cs:line 162\n",
    "   at Microsoft.Aether.EsCloud.Common.JobProcessor.StartRunAsync(EsCloudJobMetadata job) in D:\\a\\_work\\1\\s\\src\\aether\\platform\\backendV2\\Clouds\\ESCloud\\ESCloudCommon\\JobProcessor.cs:line 605\n",
    "   </i>\n",
    "- WHY? \n",
    "    - Azure ML SDK v2 does not yet (writing this 2022-10)support Spark jobs in pipeline, nor private endpoint.\n",
    "- TODO: To set the workspace in LEGACY v1 mode run this code 1 time, in a cell: `p.ws.update(v1_legacy_mode=True)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#p.ws.update(v1_legacy_mode=True) # If you happen to have a workspace in v2 mode, and want to change back to v1 legacy mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "label = 'Y'\n",
    "train_df = aml.to_pandas_dataframe()\n",
    "#y1 = train_df[label]\n",
    "X=train_df.drop(label, axis=1)\n",
    "y = train_df.pop(label).to_frame()\n",
    "\n",
    "#print(y1.head()) # no column\n",
    "#print(type(y1)) # series\n",
    "\n",
    "print(X.head())\n",
    "print(\"\")\n",
    "print(y.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'projects/project002/11_diabetes_model_reg/train/gold/dev/gold_dbx.parquet/*.parquet'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.GoldPathDatabricks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "projects/project002/11_diabetes_model_reg/train/gold/dev/Train/gold_train_dbx.parquet/*.parquet\n"
     ]
    }
   ],
   "source": [
    "train_path,validate_path,test_path = p.path_gold_train_splitted_template() # projects/project002/11_diabetes_model_reg/train/gold/dev/Train/{id_folder}}/\n",
    "train_path_out = train_path.format(id_folder=\"gold_train_dbx.parquet\") + '*.parquet' # gold_train_dbx.parquet\n",
    "print(train_path_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'azureml.pipeline.steps.databricks_step.DatabricksStep'>\n",
      "<class 'azureml.pipeline.steps.databricks_step.DatabricksStep'>\n",
      "<class 'azureml.pipeline.steps.databricks_step.DatabricksStep'>\n",
      "<class 'azureml.pipeline.steps.python_script_step.PythonScriptStep'>\n",
      "<class 'azureml.pipeline.steps.python_script_step.PythonScriptStep'>\n"
     ]
    }
   ],
   "source": [
    "for s in p_factory.pipeline_steps_array:\n",
    "    print(type(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execute_pipeline (scoring): Inference_mode: 0\n",
      "-Scoring data, default value 1000-01-01 10:35:01.243860\n",
      "Adding pipeline parameters\n",
      "Created step in2silver_ds01_diabetes [abd9c113][f7347261-2bfb-4895-81f4-670128a9b1ce], (This step will run and generate new outputs)Created step in2silver_ds02_other [2c845737][dfa6d432-2f6f-481e-b63c-7f4de9c8ee68], (This step will run and generate new outputs)\n",
      "\n",
      "Created step silver_merged_2_gold [20a1fa9e][a8dbabba-e4e5-472e-b02a-e1d1a9591280], (This step will run and generate new outputs)\n",
      "Created step SPLIT AND REGISTER datasets [cb93096b][5344bbe8-b056-4ea7-8d44-4ac5faa5e82c], (This step will run and generate new outputs)\n",
      "Created step TRAIN in  [dev], COMPARE & REGISTER model in [dev] & PROMOTE to [test] [8ef8174e][ce64b3e6-feaa-4ebc-be30-b14cfa3548dc], (This step will run and generate new outputs)\n",
      "Created data reference M11_ds01_diabetes_train_IN for StepId [86d25f21][a16432db-47a4-4091-8f49-ebf6f30d6944], (Consumers of this data will generate new runs.)Created data reference M11_ds02_other_train_IN for StepId [5b1b5512][aad27bf7-89ab-4568-b6e1-e6639170e715], (Consumers of this data will generate new runs.)\n",
      "\n",
      "Submitted PipelineRun 0e40977c-99a1-4b7c-a3af-3374530d07ce\n",
      "Link to Azure Machine Learning Portal: https://ml.azure.com/runs/0e40977c-99a1-4b7c-a3af-3374530d07ce?wsid=/subscriptions/ca0a8c40-b06a-4e4e-8434-63c03a1dee34/resourcegroups/MSFT-WEU-EAP_PROJECT02_AI-DEV-RG/workspaces/msft-weu-DEV-eap-proj02_ai-amls&tid=72f988bf-86f1-41af-91ab-2d7cd011db47\n",
      "Pipeline submitted for execution!\n",
      " ### \n",
      "PipelineRunId: 0e40977c-99a1-4b7c-a3af-3374530d07ce\n",
      "Link to Azure Machine Learning Portal: https://ml.azure.com/runs/0e40977c-99a1-4b7c-a3af-3374530d07ce?wsid=/subscriptions/ca0a8c40-b06a-4e4e-8434-63c03a1dee34/resourcegroups/MSFT-WEU-EAP_PROJECT02_AI-DEV-RG/workspaces/msft-weu-DEV-eap-proj02_ai-amls&tid=72f988bf-86f1-41af-91ab-2d7cd011db47\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Finished'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## RUN and it will train in BIG Data, since using 100% Azure compute for all steps, including SPLITTING data\n",
    "pipeline_run = p_factory.execute_pipeline(batch_pipeline) # If this give ERROR message, looking at executionlogs.txt in Azure machine learning studio Output+logs tab on pipeline rune\n",
    "pipeline_run.wait_for_completion(show_output=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5a) PUBLISH the TRAINING pipeline & PRINT its ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PUBLISH\n",
    "published_pipeline, endpoint = p_factory.publish_pipeline(batch_pipeline,\"_1\") # \"_1\" is optional    to create a NEW pipeline with 0 history, not ADD version to existing pipe & endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PRINT: Get info to use in Azure data factory\n",
    "- `published_pipeline.id` (if private Azure ML workspace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"2) Fetch scored data: Below needed for Azure Data factory PIPELINE activity (Pipeline OR Endpoint. Choose the latter\") \n",
    "print (\"- Endpoint ID\")\n",
    "print(\"Endpoint ID:  {}\".format(endpoint.id))\n",
    "print(\"Endpoint Name:  {}\".format(endpoint.name))\n",
    "print(\"Experiment name:  {}\".format(p_factory.experiment_name))\n",
    "\n",
    "print(\"In AZURE DATA FACTORY - This is the ID you need, if using PRIVATE LINK, private Azure ML workspace.\")\n",
    "print(\"-You need PIPELINE id, not pipeline ENDPOINT ID ( since cannot be chosen in Azure data factory if private Azure ML)\")\n",
    "published_pipeline.id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DONE! Next Step - Deploy model, serve your model for INFERENCING purpose:\n",
    "- For INFERENCE you may need either to DEPLOY the model \n",
    "    - a) ONLINE on AKS endpoint\n",
    "        - Notebook: \n",
    "    - b) BATCH SCORING on an Azure machine learning pipeline\n",
    "        - Notebook: [your_root]\\notebook_templates_quickstart\\\\`3a_PRODUCTION_phase_BATCH_INFERENCE_Pipeline.ipynb`\n",
    "    - c) STREAMING using Eventhubs and Azure Databricks structured streaming\n",
    "        - Notebook: TBA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Q: `Next step in PRODUCTION phaase after the 2a and 3a or 3b notebooks are done?`\n",
    " \n",
    "- 1) `DataOps+MLOps:` Go to your ESMLProjects `Azure data factory`, and use the `ESML DataOps templates` (Azure data factory templates) for `IN_2_GOLD_TRAIN` and `IN_2_GOLD_SCORING`\n",
    "    - azure-enterprise-scale-ml\\copy_my_subfolders_to_my_grandparent\\adf\\v1_3\\PROJECT000\\LakeOnly\\\\`STEP03_IN_2_GOLD_TRAIN_v1_3.zip`\n",
    "- 2) `MLOps CI/CD` Go to the next notebook `mlops` folder, to setup `CI/CD` in Azure Devops\n",
    "    - Import this in Azure devops\n",
    "        azure-enterprise-scale-ml\\copy_my_subfolders_to_my_grandparent\\mlops\\01_template_v14\\azure-devops-build-pipeline-to-import\\\\`ESML-v14-project002_M11-DevTest.json`\n",
    "    - Change the Azure Devops `VARIABLES` for service principle, tenant, etc.\n",
    "    - Change parameters in the `inlince Azure CLI script` to correct model you want to work with, and the correct data you want to train with, or score.\n",
    "        - Step `21-train_in_2_gold_train_pipeline`\n",
    "        - INLINE code calls the file: `21-train_in_2_gold_train_pipeline.py`\n",
    "        - INLINE parameters: `--esml_model_number 11 --esml_date_utc \"1000-01-01 10:35:01.243860\"`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# StepMap - how to print & look at it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_map = map.get_train_map(p.active_model['dataset_folder_names'])\n",
    "has_dbx,step_name,map_step = map.get_dbx_map_step(train_map,'ds01_diabetes')\n",
    "print(has_dbx)\n",
    "print(step_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_map = map.get_train_map(p.active_model['dataset_folder_names'])\n",
    "for d in p.Datasets:\n",
    "    print(d.Name)\n",
    "    has_dbx,step_name,map_step = map.get_dbx_map_step(train_map,d.Name)\n",
    "    print(\"has_dbx:\",has_dbx)\n",
    "    print(\"step_name\",step_name)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('azure_automl_esml_v144')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "a4a3f6f829c0fbf992fdd78de6ec4e694e293d154a9b96895f90a426de0ee97e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
